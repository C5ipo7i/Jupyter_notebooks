{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "Promixal Policy Optimization\n",
    "\n",
    "https://arxiv.org/pdf/1707.06347.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from openai/baseline\n",
    "# with minor edits\n",
    "# see https://github.com/openai/baselines/baselines/common/vec_env/subproc_vec_env.py\n",
    "# \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from multiprocessing import Process, Pipe\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class CloudpickleWrapper(object):\n",
    "    \"\"\"\n",
    "    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "    def __getstate__(self):\n",
    "        import cloudpickle\n",
    "        return cloudpickle.dumps(self.x)\n",
    "\n",
    "    def __setstate__(self, ob):\n",
    "        import pickle\n",
    "        self.x = pickle.loads(ob)\n",
    "\n",
    "class VecEnv(ABC):\n",
    "    \"\"\"\n",
    "    An abstract asynchronous, vectorized environment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_envs, observation_space, action_space):\n",
    "        self.num_envs = num_envs\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "    @abstractmethod\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset all the environments and return an array of\n",
    "        observations, or a dict of observation arrays.\n",
    "        If step_async is still doing work, that work will\n",
    "        be cancelled and step_wait() should not be called\n",
    "        until step_async() is invoked again.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_async(self, actions):\n",
    "        \"\"\"\n",
    "        Tell all the environments to start taking a step\n",
    "        with the given actions.\n",
    "        Call step_wait() to get the results of the step.\n",
    "        You should not call this if a step_async run is\n",
    "        already pending.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step_wait(self):\n",
    "        \"\"\"\n",
    "        Wait for the step taken with step_async().\n",
    "        Returns (obs, rews, dones, infos):\n",
    "         - obs: an array of observations, or a dict of\n",
    "                arrays of observations.\n",
    "         - rews: an array of rewards\n",
    "         - dones: an array of \"episode done\" booleans\n",
    "         - infos: a sequence of info objects\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Clean up the environments' resources.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Step the environments synchronously.\n",
    "        This is available for backwards compatibility.\n",
    "        \"\"\"\n",
    "        self.step_async(actions)\n",
    "        return self.step_wait()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        #logger.warn('Render not defined for %s' % self)\n",
    "        pass\n",
    "        \n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        if isinstance(self, VecEnvWrapper):\n",
    "            return self.venv.unwrapped\n",
    "        else:\n",
    "            return self\n",
    "\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper.x\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'reset_task':\n",
    "            ob = env.reset_task()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class parallelEnv(VecEnv):\n",
    "    def __init__(self, env_name='PongDeterministic-v4',\n",
    "                 n=4, seed=None,\n",
    "                 spaces=None):\n",
    "\n",
    "        env_fns = [ gym.make(env_name) for _ in range(n) ]\n",
    "\n",
    "        if seed is not None:\n",
    "            for i,e in enumerate(env_fns):\n",
    "                e.seed(i+seed)\n",
    "        \n",
    "        \"\"\"\n",
    "        envs: list of gym environments to run in subprocesses\n",
    "        adopted from openai baseline\n",
    "        \"\"\"\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        nenvs = len(env_fns)\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n",
    "        self.ps = [Process(target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn)))\n",
    "            for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "        for p in self.ps:\n",
    "            p.daemon = True # if the main process crashes, we should not cause things to hang\n",
    "            p.start()\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self.remotes[0].send(('get_spaces', None))\n",
    "        observation_space, action_space = self.remotes[0].recv()\n",
    "        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send(('step', action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def reset_task(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('reset_task', None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "            for remote in self.remotes:            \n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('close', None))\n",
    "        for p in self.ps:\n",
    "            p.join()\n",
    "        self.closed = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(32,32)):\n",
    "        super(Policy,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.nA = nA\n",
    "        self.nS = nS\n",
    "        self.size = hidden_dims[-1] * hidden_dims[-2]\n",
    "        \n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(1,len(self.hidden_dims)):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i-1],hidden_dims[i])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1],nA)\n",
    "        self.sig = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        # flatten the tensor\n",
    "#         x = x.view(-1,self.size)\n",
    "        return F.softmax(self.output_layer(x),dim=1)\n",
    "#         return self.sig(self.output_layer(x))\n",
    "    \n",
    "    # Return the action along with the probability of the action. For weighting the reward garnered by the action.\n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "#         print('action',action)\n",
    "        return action.item(),m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training single Env\n",
    "\n",
    "- Randomize starting position\n",
    "- Gather trajectories\n",
    "- Discount the rewards\n",
    "- Update policy to maximize rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_env(env):\n",
    "    # seed the environment with N step random actions prior\n",
    "    nrand_actions = int(np.random.random() * 3)\n",
    "    state = env.reset()\n",
    "    for act in range(nrand_actions):\n",
    "        state,reward,done,_ = env.step(env.action_space.sample())\n",
    "    return state,env\n",
    "\n",
    "# convert states to probability, passing through the policy\n",
    "def states_to_prob(policy, states,actions):\n",
    "    policy_input = torch.tensor(states,dtype=torch.float32)\n",
    "    return policy(policy_input)\n",
    "    \n",
    "\n",
    "def collect_trajectories(env,policy,tmax):\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    a_probs = []\n",
    "#     state,env = initialize_env(env)\n",
    "    state = env.reset()\n",
    "    for t in range(tmax):\n",
    "        probs = policy(state).cpu().detach().numpy()[0]\n",
    "        action = np.random.choice([0,1],p=probs)\n",
    "        state,reward,done,_ = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        a_probs.append(probs)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        if done:\n",
    "            break\n",
    "    return actions,np.vstack(a_probs),states,rewards,dones\n",
    "\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "#     print('rewards',rewards)\n",
    "#     print('old_probs',old_probs)\n",
    "#     print('actions',actions)\n",
    "    # discount and take future rewards\n",
    "    discounts = discount**np.arange(len(rewards))\n",
    "    future_r = [rewards[i:]*discounts[:-i] if i>0 else rewards*discounts for i in range(len(rewards))]\n",
    "    rewards_future = [sum(future_r[i]) for i in range(len(future_r))]\n",
    "#     print('rewards_future',rewards_future)\n",
    "    mean = np.mean(rewards_future)\n",
    "    std = np.std(rewards_future) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean)/std\n",
    "    \n",
    "    \n",
    "    \n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy,states,actions)\n",
    "    \n",
    "#     print('actions',actions)\n",
    "#     print('new_probs',new_probs)\n",
    "#     print('old_probs',old_probs)\n",
    "#     print('new_probs',new_probs.shape)\n",
    "#     print('old_probs',old_probs.shape)\n",
    "    # slice both according to the actions taken\n",
    "    index = np.arange(new_probs.shape[0])\n",
    "    new_probs = new_probs[index,np.array(actions)]\n",
    "    old_probs = old_probs[index,np.array(actions)]\n",
    "#     print('new_probs',new_probs.shape)\n",
    "#     print('old_probs',old_probs.shape)\n",
    "    \n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    rewards = torch.tensor(rewards_future, dtype=torch.float, device=device)\n",
    "    \n",
    "#     policy_input = torch.tensor(states,dtype=torch.float32)\n",
    "#     new_probs = policy(policy_input).squeeze(-1)\n",
    "#     new_probs = torch.where(actions == 0, new_probs[:,0], new_probs[:,1])\n",
    "#     print('new_probs',new_probs)\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    clip = torch.clamp(ratio,1-epsilon,1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "    \n",
    "    \n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "#     print('entropy',entropy)\n",
    "#     print('ratio',ratio)\n",
    "#     print('clip',clip)\n",
    "#     print('ratio*rewards',ratio*rewards)\n",
    "#     print('clip*rewards',clip*rewards)\n",
    "#     print('clipped_surrogate',clipped_surrogate)\n",
    "    return torch.mean(clipped_surrogate + entropy*beta)\n",
    "\n",
    "# Single agent\n",
    "def train(env,policy,optimizer,episodes,discount,epsilon,beta,tmax,SGD_epoch):\n",
    "    total_rewards = []\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        # get trajectories\n",
    "        actions,a_probs,states,rewards,dones = collect_trajectories(env,policy,tmax)\n",
    "        for _ in range(SGD_epoch):\n",
    "            # Surrogate\n",
    "            L = clipped_surrogate(policy,a_probs,states,actions,rewards,discount,epsilon,beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # for batch loss\n",
    "#             print('L',L)\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            del L\n",
    "        \n",
    "        # the clipping parameter reduces as time goes on\n",
    "        epsilon*=.999\n",
    "\n",
    "        # the regulation term also reduces\n",
    "        # this reduces exploration in later runs\n",
    "        beta*=.995\n",
    "\n",
    "        # get the average reward of the parallel environments\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # display some progress every 20 iterations\n",
    "        if i_episode % 50 == 0:\n",
    "            print(\"Episode: {0:d}, score: {1:f}\".format(i_episode,total_rewards[-1]))\n",
    "    \n",
    "def main():\n",
    "    seed = 1234\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.seed(seed)\n",
    "    nA = env.action_space.n\n",
    "#     nA = 1\n",
    "    nS = env.observation_space.shape[0]\n",
    "    policy = Policy(seed,nS,nA).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr = 1e-2)\n",
    "    \n",
    "    discount_rate = .995\n",
    "    epsilon = 0.1\n",
    "    beta = .01\n",
    "    tmax = 200\n",
    "    SGD_epoch = 4\n",
    "    episodes = 1000\n",
    "    \n",
    "    train(env,policy,optimizer,episodes,discount_rate,epsilon,beta,tmax,SGD_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, score: 25.000000\n",
      "Episode: 100, score: 56.000000\n",
      "Episode: 150, score: 21.000000\n",
      "Episode: 200, score: 22.000000\n",
      "Episode: 250, score: 37.000000\n",
      "Episode: 300, score: 28.000000\n",
      "Episode: 350, score: 32.000000\n",
      "Episode: 400, score: 26.000000\n",
      "Episode: 450, score: 29.000000\n",
      "Episode: 500, score: 49.000000\n",
      "Episode: 550, score: 39.000000\n",
      "Episode: 600, score: 61.000000\n",
      "Episode: 650, score: 36.000000\n",
      "Episode: 700, score: 30.000000\n",
      "Episode: 750, score: 25.000000\n",
      "Episode: 800, score: 39.000000\n",
      "Episode: 850, score: 68.000000\n",
      "Episode: 900, score: 34.000000\n",
      "Episode: 950, score: 29.000000\n",
      "Episode: 1000, score: 41.000000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIGHT = 1\n",
    "LEFT = 0\n",
    "def collect_multi_trajectories(env,policy,tmax):\n",
    "    n = 8\n",
    "    nrand = 2\n",
    "    # number of parallel instances\n",
    "    n=len(envs.ps)\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list=[]\n",
    "    reward_list=[]\n",
    "    prob_list=[]\n",
    "    action_list=[]\n",
    "\n",
    "    envs.reset()\n",
    "    \n",
    "    # start all parallel agents\n",
    "    envs.step([1]*n)\n",
    "    \n",
    "    # perform nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "\n",
    "        # prepare the input\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (n, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = torch.tensor([fr1,fr2],dtype=torch.float32)\n",
    "        \n",
    "        # probs will only be used as the pi_old\n",
    "        # no gradient propagation is needed\n",
    "        # so we move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        ###\n",
    "        print('probs',probs.shape)\n",
    "        print('np.random.rand(n)',np.random.rand(n).shape)\n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0-probs)\n",
    "        \n",
    "        \n",
    "        # advance the game (0=no action)\n",
    "        # we take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "\n",
    "    # return pi_theta, states, actions, rewards, probability\n",
    "    return prob_list, state_list, \\\n",
    "        action_list, reward_list\n",
    "\n",
    "def clipped_surrogate_multi(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    clipped_surrogate = torch.min(ratio*rewards, clip*rewards)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "    \n",
    "    print('entropy',entropy)\n",
    "    print('clipped_surrogate',clipped_surrogate)\n",
    "    # this returns an average of all the entries of the tensor\n",
    "    # effective computing L_sur^clip / T\n",
    "    # averaged over time-step and number of trajectories\n",
    "    # this is desirable because we have normalized our rewards\n",
    "    return torch.mean(clipped_surrogate + beta*entropy)\n",
    "\n",
    "# Single agent\n",
    "def train_multi(env,policy,optimizer,episodes,discount,epsilon,beta,tmax,SGD_epoch):\n",
    "    mean_rewards = []\n",
    "    for i_episode in range(1,episodes+1):\n",
    "        # get trajectories\n",
    "        actions,a_probs,states,rewards,dones = collect_multi_trajectories(env,policy,tmax)\n",
    "        for _ in range(SGD_epoch):\n",
    "            # Surrogate\n",
    "            L = -clipped_surrogate_multi(policy,a_probs,states,actions,rewards,discount,epsilon,beta)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # for batch loss\n",
    "            print('L',L)\n",
    "            L.backward()\n",
    "            optimizer.step()\n",
    "            del L\n",
    "        \n",
    "        # the clipping parameter reduces as time goes on\n",
    "        epsilon*=.999\n",
    "\n",
    "        # the regulation term also reduces\n",
    "        # this reduces exploration in later runs\n",
    "        beta*=.995\n",
    "\n",
    "        # get the average reward of the parallel environments\n",
    "        total_rewards.append(sum(rewards))\n",
    "\n",
    "        # display some progress every 20 iterations\n",
    "        if i_episode % 20 == 0:\n",
    "            print(\"Episode: {0:d}, score: {1:f}\".format(i_episode,total_rewards[-1]))\n",
    "    \n",
    "def main_multi():\n",
    "    seed = 1234\n",
    "    envs = parallelEnv('CartPole-v0', n=8, seed=1234)\n",
    "    env = gym.make('CartPole-v0')\n",
    "    nA = env.action_space.n\n",
    "#     nA = 1\n",
    "    nS = env.observation_space.shape[0]\n",
    "    del env\n",
    "    policy = Policy(seed,nS,nA).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr = 1e-2)\n",
    "    \n",
    "    discount_rate = .995\n",
    "    epsilon = 0.1\n",
    "    beta = .01\n",
    "    tmax = 200\n",
    "    SGD_epoch = 4\n",
    "    episodes = 500\n",
    "    \n",
    "    train_multi(envs,policy,optimizer,episodes,discount_rate,epsilon,beta,tmax,SGD_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs (2, 8, 2)\n",
      "np.random.rand(n) (8,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (8,) (2,8,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-362-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-361-2215f3e3af31>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscount_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSGD_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-361-2215f3e3af31>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, policy, optimizer, episodes, discount, epsilon, beta, tmax, SGD_epoch)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_episode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# get trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_multi_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSGD_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;31m# Surrogate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-361-2215f3e3af31>\u001b[0m in \u001b[0;36mcollect_multi_trajectories\u001b[0;34m(env, policy, tmax)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'probs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np.random.rand(n)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEFT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (8,) (2,8,2) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-54:\n",
      "Process Process-53:\n",
      "Process Process-51:\n",
      "Process Process-55:\n",
      "Process Process-56:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Process Process-50:\n",
      "Process Process-49:\n",
      "Traceback (most recent call last):\n",
      "Process Process-52:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"<ipython-input-349-d097a606b668>\", line 104, in worker\n",
      "    cmd, data = remote.recv()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/anaconda3/envs/torch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "main_multi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
