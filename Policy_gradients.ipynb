{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import gym\n",
    "\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space 4\n",
      "action space 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "print('state space',nS)\n",
    "print('action space',nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(scores):\n",
    "    # Rolling mean plot\n",
    "    interval = 5\n",
    "    rolling_mean = [np.mean(scores[(slice_*interval):(slice_+1)*interval]) for slice_ in range(math.ceil(len(scores)/interval))]\n",
    "    x_axis = np.arange(len(rolling_mean)) * interval\n",
    "    plt.plot(x_axis, rolling_mean)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "A suite of methods where we optimize for policy directly.\n",
    "First we define our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,nS,nA,hidden_dims=(32,32)):\n",
    "        super(Network,self).__init__()\n",
    "        self.activation_fc = F.relu\n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.actions = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        action_probabilities = F.softmax(self.actions(x), dim=1)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy algorithms with np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 4.51309609413147 in seconds\n",
      "average num episodes before solve 12.695\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def random_policy_search(env):\n",
    "    scores = []\n",
    "    for _ in range(1000):\n",
    "        best_params = None\n",
    "        best_reward = 0\n",
    "        for i in range(100):\n",
    "            parameters = np.random.rand(4) * 2 -1\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "        scores.append(i)\n",
    "    return scores\n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores = random_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "# plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing\n",
    "\n",
    "Steepest ascent\n",
    "With 5 param modifications per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, s_size=4, a_size=2):\n",
    "        self.w = 1e-4*np.random.rand(s_size, a_size)  # weights for simple linear policy: state_space x action_space\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = np.dot(state, self.w)\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def act(self, state):\n",
    "        probs = self.forward(state)\n",
    "        #action = np.random.choice(2, p=probs) # option 1: stochastic policy\n",
    "        action = np.argmax(probs)              # option 2: deterministic policy\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "\n",
    "def hill_climbing(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100, noise_scale=1e-2):\n",
    "    \"\"\"Implementation of hill climbing with adaptive noise scaling.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        noise_scale (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_R = -np.Inf\n",
    "    best_w = policy.w\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action = policy.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "\n",
    "        if R >= best_R: # found better weights\n",
    "            best_R = R\n",
    "            best_w = policy.w\n",
    "            noise_scale = max(1e-3, noise_scale / 2)\n",
    "            policy.w += noise_scale * np.random.rand(*policy.w.shape) \n",
    "        else: # did not find better weights\n",
    "            noise_scale = min(2, noise_scale * 2)\n",
    "            policy.w = best_w + noise_scale * np.random.rand(*policy.w.shape)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            policy.w = best_w\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "            \n",
    "scores = hill_climbing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 180.11616897583008 in seconds\n",
      "average num episodes before solve 422.016\n",
      "updates 1109\n",
      "no_improve 104645\n"
     ]
    }
   ],
   "source": [
    "def hill_policy_search(env,num_agents):\n",
    "    scores = []\n",
    "    updates = 0\n",
    "    no_improve = 0\n",
    "    for _ in range(250):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                updates += 1\n",
    "                best_agent = np.argmax(rewards)\n",
    "                best_params = params[best_agent]\n",
    "                best_reward = rewards[best_agent]\n",
    "                if reward == 200:\n",
    "                    break\n",
    "            else:\n",
    "                no_improve += 1\n",
    "        scores.append(i)\n",
    "    return scores,updates,no_improve\n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 5\n",
    "scores,updates,no_improve = hill_policy_search(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('updates',updates)\n",
    "print('no_improve',no_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 9.671694040298462 in seconds\n",
      "average num episodes before solve 12.0\n",
      "anneals 19905\n",
      "resets 2906\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def annealed_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,noise - 0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = annealed_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Adaptive noise scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 8.949865818023682 in seconds\n",
      "average num episodes before solve 2.0\n",
      "anneals 21111\n",
      "resets 2999\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def adaptive_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = start_noise\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = adaptive_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Average return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 36.22808504104614 in seconds\n",
      "average num episodes before solve 32.0\n",
      "anneals 21123\n",
      "resets 4490\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def average_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    last_move = None\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        start_noise = 0.1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(250):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = 0\n",
    "            for run in range(3):\n",
    "                reward += run_episode(parameters,env)\n",
    "            reward /= 3\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                if last_move == 'anneal':\n",
    "                    noise = min(start_noise,noise-0.04)\n",
    "                else:\n",
    "                    noise -= 0.04\n",
    "                last_move = 'reset'\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "                last_move = 'anneal'\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = average_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 192.90656518936157 in seconds\n",
      "average num episodes before solve 98.0\n",
      "anneals 156825\n",
      "resets 3397\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def agents_hill_policy_search(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        start_noise = 0.1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                best_agent = np.argmax(rewards)\n",
    "                best_params = params[best_agent]\n",
    "                best_reward = rewards[best_agent]\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = agents_hill_policy_search(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEM\n",
    "\n",
    "Cross Entropy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 55.59503626823425 in seconds\n",
      "average num episodes before solve 0.0\n",
      "anneals 43458\n",
      "resets 1311\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def CEM(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(150):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                indexes = [i for i,reward in enumerate(rewards) if reward > best_reward]\n",
    "#                 print('indexes',indexes)\n",
    "                avg_params = np.sum(np.array(params)[indexes],axis=0) / len(indexes)\n",
    "                best_params = avg_params\n",
    "                best_reward = sum(np.array(rewards)[indexes]) / len(indexes)\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,noise-0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = CEM(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Method\n",
    "\n",
    "Like CEM but with a weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 61.40742611885071 in seconds\n",
      "average num episodes before solve 499.0\n",
      "anneals 45293\n",
      "resets 1171\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def EM(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(150):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                indexes = [i for i,reward in enumerate(rewards) if reward > best_reward]\n",
    "                rewards = np.array(rewards)\n",
    "                params = np.array(params)\n",
    "                init_sum = np.sum(rewards[indexes])\n",
    "                weighted_sum = np.divide(rewards[indexes],init_sum).reshape(len(indexes),1)\n",
    "#                 print('params[indexes]',params[indexes])\n",
    "#                 print('weighted_sum',weighted_sum)\n",
    "                weighted_params = np.multiply(params[indexes],weighted_sum)\n",
    "#                 print('weighted_params',weighted_params)\n",
    "                best_params = np.sum(weighted_params,axis=0)\n",
    "#                 print('best_params',best_params)\n",
    "                best_reward = sum(rewards[indexes]) / len(indexes)\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = start_noise\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = EM(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same algorithms but with torch NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,env,hidden_dims=(32,32),gate = F.relu):\n",
    "        super(Agent,self).__init__()\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.h1 = hidden_dims[0]\n",
    "        self.h2 = hidden_dims[1]\n",
    "        self.env = env\n",
    "        \n",
    "        # Layers\n",
    "        self.gate = gate\n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.gate(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.gate(hidden_layer(x))\n",
    "        x = F.softmax(self.output_layer(x))\n",
    "        return x\n",
    "        \n",
    "    def set_weights(self,weights):\n",
    "        hidden_dims = self.hidden_dims\n",
    "        \n",
    "        nS = self.nS\n",
    "        h1 = hidden_dims[0]\n",
    "        h2 = hidden_dims[1]\n",
    "        nA = self.nA\n",
    "        \n",
    "        fc1_end = (nS * h1) + h1\n",
    "        fc2_end = fc1_end + ((h1*h2)+h2)\n",
    "        \n",
    "        # separate the weights for each layer\n",
    "        fc1_W = torch.from_numpy(weights[:nS * h1])\n",
    "        fc1_b = torch.from_numpy(weights[nS * h1: fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end: fc1_end + (h1*h2)])\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end + (h1*h2): fc1_end + (h1*h2) + h2])\n",
    "        fc3_W = torch.from_numpy(weights[fc2_end: fc2_end + (h2*nA)])\n",
    "        fc3_b = torch.from_numpy(weights[fc2_end + (h2*nA): fc2_end + (h2*nA) + nA])\n",
    "        \n",
    "#         s_size = self.s_size\n",
    "#         h_size = self.h_size\n",
    "#         a_size = self.a_size\n",
    "#         # separate the weights for each layer\n",
    "#         fc1_end = (s_size*h_size)+h_size\n",
    "#         fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))\n",
    "#         fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])\n",
    "#         fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))\n",
    "#         fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])\n",
    "#         # set the weights for each layer\n",
    "#         self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "#         self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "#         self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "#         self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "\n",
    "        # set the weights for each layer\n",
    "        self.input_layer.weight.data.copy_(fc1_W.view_as(self.input_layer.weight.data))\n",
    "        self.input_layer.bias.data.copy_(fc1_b.view_as(self.input_layer.bias.data))\n",
    "        self.hidden_layers[0].weight.data.copy_(fc2_W.view_as(self.hidden_layers[0].weight.data))\n",
    "        self.hidden_layers[0].bias.data.copy_(fc2_b.view_as(self.hidden_layers[0].bias.data))\n",
    "        self.output_layer.weight.data.copy_(fc3_W.view_as(self.output_layer.weight.data))\n",
    "        self.output_layer.bias.data.copy_(fc3_b.view_as(self.output_layer.bias.data))\n",
    "        \n",
    "    def get_weights_dim(self):\n",
    "        return (self.nS+1)*self.h1 + (self.h1+1)*self.h2 + (self.h2+1) * self.nA\n",
    "    \n",
    "    def evaluate(self,weights,gamma=1.0,max_t=1000):\n",
    "        self.set_weights(weights)\n",
    "        total_reward = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            action_probs = self.forward(state)[0]\n",
    "            action = np.random.choice(np.arange(self.nA),p=action_probs.detach().numpy())\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            total_reward += reward# * math.pow(gamma,t)\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing\n",
    "\n",
    "With Torch NNs instead of np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs [0.48339996 0.5166001 ]\n",
      "action 1\n",
      "action_probs [0.48646936 0.5135306 ]\n",
      "action 1\n",
      "action_probs [0.4899933 0.5100067]\n",
      "action 1\n",
      "action_probs [0.4912174 0.5087825]\n",
      "action 1\n",
      "action_probs [0.49158344 0.5084166 ]\n",
      "action 0\n",
      "action_probs [0.49127054 0.5087295 ]\n",
      "action 0\n",
      "action_probs [0.49050406 0.5094959 ]\n",
      "action 1\n",
      "action_probs [0.49137488 0.5086251 ]\n",
      "action 1\n",
      "action_probs [0.49074233 0.5092576 ]\n",
      "action 1\n",
      "action_probs [0.48709518 0.51290476]\n",
      "action 1\n",
      "action_probs [0.48264414 0.5173558 ]\n",
      "action 0\n",
      "action_probs [0.4856125 0.5143875]\n",
      "action 0\n",
      "action_probs [0.48834038 0.5116596 ]\n",
      "action 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hill_climb(nA,agent,env,n_episodes=1800, max_t=200):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    index = 0\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        state = env.reset()\n",
    "        previous_score = 0\n",
    "        score = 0\n",
    "        best_agent = agent\n",
    "        for t in range(max_t):\n",
    "            action_probs = agent(state)[0]\n",
    "            print('action_probs',action_probs.detach().numpy())\n",
    "            action = np.random.choice(np.arange(nA),p=action_probs.detach().numpy())\n",
    "            print('action',action)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "#             agent.step(state,action,reward,next_state,done,index)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            index += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        if previous_score > score:\n",
    "            for parameter in agent.parameters():\n",
    "                parameter.data.copy_(previous_weights.data)\n",
    "        else:\n",
    "            for parameter in agent.parameters():\n",
    "                parameter.data.copy_(parameter.data + np.random.rand()/100)\n",
    "        break\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "agent = Agent(env)\n",
    "scores = hill_climb(nA,agent,env)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Method (CEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 44.80\n",
      "rewards [ 10.  79.  40.  24.  10.   9.  16.   9.  10.  95.  28.  30.  26. 132.\n",
      "  13.  97.  10.  24.  24.  11.  16.  11.  13.  15.  16.  10.  10.  53.\n",
      "  35.  13.  35.   9.  39.  22.  47.  42.   9.  20.   9.  75.  12. 113.\n",
      "  21.   9.  12.  35.   9.  10.   8.  59.]\n",
      "Episode 20\tAverage Score: 91.75\n",
      "rewards [200.  24.  58. 152. 164. 143. 103. 117. 200.  69.  25.  11. 164.  10.\n",
      " 108.  12. 119.  18. 106.  27. 124.  68.  79.  90.  53. 161. 166. 137.\n",
      "  11. 200.  40.  58.  32. 109.  33.  44. 200. 150. 200.  45.  88.  35.\n",
      " 122. 130. 111.  26.  63.  79.  58. 107.]\n",
      "Episode 30\tAverage Score: 127.83\n",
      "rewards [130.  26.  39. 119. 200.  21. 186. 168. 200. 179.  19. 200. 200.  94.\n",
      "  64.  29. 200.  35. 200. 200. 174.  21. 200.   8. 102. 200. 200. 200.\n",
      " 200. 119. 200. 200. 200. 200. 193.  32.  94. 200. 169.  23. 173.  23.\n",
      "  17. 200. 200.  46. 200. 180. 200. 200.]\n",
      "Episode 40\tAverage Score: 145.88\n",
      "rewards [200. 183. 105. 200. 200.  74. 200. 200. 123.  64.  17. 200. 200.  98.\n",
      " 185. 146. 166. 161.  21.  68. 200. 200. 158. 200. 200.  79. 200. 200.\n",
      "  14.  96. 200. 200. 152. 200. 200. 200.  61. 200.  21. 200. 200.  10.\n",
      " 200. 189. 193.  38. 192. 200. 200. 200.]\n",
      "Episode 50\tAverage Score: 156.70\n",
      "rewards [200. 200.  11. 200.  11. 165.  18. 132. 200. 200. 200.   9. 200. 200.\n",
      " 200. 129. 132.   9.  76.  79. 178. 200. 200. 200. 200. 200. 200. 200.\n",
      "  15. 200. 117. 200.  10. 164. 200.  87. 120. 200.  86.  12. 200. 200.\n",
      " 200.  85. 147.  90.   8.  17. 200. 200.]\n",
      "Episode 60\tAverage Score: 163.92\n",
      "rewards [  9.  20. 144. 200.  12. 170. 200. 176. 200.  15. 142.  61. 101. 157.\n",
      "  30. 200. 200. 200. 200. 200. 200. 200.  10.  28.  59. 200.  49.  10.\n",
      " 200. 200. 162.  20.  18.   9. 200. 155. 200. 200. 200. 200.  10. 153.\n",
      "  26. 200. 200. 187. 200.  23. 150. 200.]\n",
      "Episode 70\tAverage Score: 169.07\n",
      "rewards [200. 200.  10. 170. 200. 200. 190. 200. 200. 200. 200. 183. 200. 200.\n",
      "  18. 200. 200. 200. 200. 111. 200. 200. 200.  85. 200. 200. 200. 112.\n",
      " 200. 200. 200. 200. 200. 196. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200.]\n",
      "Episode 80\tAverage Score: 172.94\n",
      "rewards [200. 200. 200. 200. 200.  10. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200.   9.  65. 192. 200. 200. 200. 200. 200. 151. 158. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 170. 200. 200. 200. 200.\n",
      " 200.  11. 200. 200. 200. 109. 200. 200.]\n",
      "Episode 90\tAverage Score: 175.94\n",
      "rewards [200. 200. 200. 200. 200. 200. 200. 200. 200.  83. 183. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 187.   9. 200.\n",
      "  17. 175. 200. 200. 200. 200. 200. 200. 200. 131. 200. 200. 200. 200.\n",
      " 146. 200. 200. 200. 200. 129. 200.  14.]\n",
      "Episode 100\tAverage Score: 178.35\n",
      "rewards [200.  10. 200. 200. 200. 200. 200.   9. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200.  27. 172. 164. 200. 200. 200. 200.\n",
      " 200.  28. 200. 200. 200. 165. 110.  15. 200. 200.  83. 200. 200.  10.\n",
      " 200. 200. 200. 200. 200. 200. 200. 155.]\n",
      "Episode 110\tAverage Score: 193.87\n",
      "rewards [200.  10. 175. 169. 200. 200. 145. 191. 177. 149. 127. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 174. 200. 186.\n",
      " 200. 200.  10. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 149.   8.]\n",
      "\n",
      "Environment solved in 11 iterations!\tAverage Score: 195.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def agent_cem(agent,n_iterations=200,print_every=10,max_t=500,gamma=0.99,population=50,elite_frac=0.2,sigma=0.5):\n",
    "    n_elite = int(population*elite_frac)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    \n",
    "    for i_iteration in range(1,n_iterations+1):\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(population)]\n",
    "        rewards = np.array([agent.evaluate(weights,gamma,max_t) for weights in weights_pop])\n",
    "        \n",
    "        elite_idx = rewards.argsort()\n",
    "#         print('elite_idx',elite_idx)\n",
    "#         print('np.arange(population-n_elite,population)',np.arange(population-n_elite,population))\n",
    "        elite_select = elite_idx[np.arange(population-n_elite,population)]\n",
    "#         print('elite_select',elite_select)\n",
    "        elite_weights = [weights_pop[i] for i in elite_select]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "        \n",
    "        reward = agent.evaluate(best_weight,gamma)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        torch.save(agent.state_dict(),'cem_checkpoint.pth')\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "            print('rewards',rewards)\n",
    "            \n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "print(env.action_space.n)\n",
    "agent = Agent(env)\n",
    "scores = agent_cem(agent)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Policy_m(nn.Module):\n",
    "    def __init__(self,nS,nA,hidden_dims=(16,16)):\n",
    "        super(Policy_m,self).__init__()\n",
    "        print('nS,nA',nS,nA)\n",
    "        self.fc1 = nn.Linear(nS,hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0],hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(),m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 24.08\n",
      "Episode 200\tAverage Score: 37.82\n",
      "Episode 300\tAverage Score: 47.01\n",
      "Episode 400\tAverage Score: 49.64\n",
      "Episode 500\tAverage Score: 55.60\n",
      "Episode 600\tAverage Score: 53.09\n",
      "Episode 700\tAverage Score: 41.99\n",
      "Episode 800\tAverage Score: 55.57\n",
      "Episode 900\tAverage Score: 61.78\n",
      "Episode 1000\tAverage Score: 68.50\n"
     ]
    }
   ],
   "source": [
    "# policy_m = Policy_m(nS,nA)\n",
    "# policy = Policy_m(nS,nA).to(device)\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 1e-2)\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "def reinforce_2(env,policy,optimizer,n_episodes=1000, max_t=1000, gamma=0.99, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action,log_prob = policy.act(state)\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards))]\n",
    "        R = sum([a*b for a,b in zip(discounts,rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = reinforce_2(env,policy,optimizer)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 32.46\n",
      "Episode 200\tAverage Score: 48.34\n",
      "Episode 300\tAverage Score: 76.95\n",
      "Episode 400\tAverage Score: 135.24\n",
      "Episode 500\tAverage Score: 92.40\n",
      "Episode 600\tAverage Score: 139.46\n",
      "Episode 700\tAverage Score: 150.47\n",
      "Environment solved in 670 episodes!\tAverage Score: 195.23\n"
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(env,policy,n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "Categorical cross entropy:\n",
    "The final layer is a softmax, and then cross entropy\n",
    "\n",
    "−(ylog(p)+(1−y)log(1−p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CrossEntropy(yhat,y):\n",
    "    if y == 1:\n",
    "        return -log(yhat)\n",
    "    else:\n",
    "        return -log(1 - yhat)\n",
    "    \n",
    "# short form\n",
    "def CE(yhat,y):\n",
    "    return -log((-1+y) + yhat)\n",
    "\n",
    "# multiclass\n",
    "def MCE(yhat,y):\n",
    "    # if they are vectors\n",
    "    return -log(np.add(yhat,np.add(y,-1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reinforce again\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,nS,nA,seed,hidden_dims=(32,32)):\n",
    "        super(Policy,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear()\n",
    "        self.fc2 = nn.Linear()\n",
    "        self.fc = nn.ModuleList()\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def act(self):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reward normalization\n",
    "\n",
    "def norm_train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "            \n",
    "        mean_r = np.mean(rewards)\n",
    "        N = len(rewards)\n",
    "        sigma_r = np.sqrt(sum((rewards-mean_r)**2)/N)\n",
    "        norm_r = rewards - mean_r / simga_r\n",
    "        \n",
    "        \n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Future rewards\n",
    "\n",
    "def future_train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "        future_rewards = [sum(rewards[i:]) for i in range(rewards)]\n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(future_rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "Proximal Policy Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
