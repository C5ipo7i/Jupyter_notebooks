{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(180000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 180 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import gym\n",
    "\n",
    "%autosave 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state space 4\n",
      "action space 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(seed)\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "print('state space',nS)\n",
    "print('action space',nA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(scores):\n",
    "    # Rolling mean plot\n",
    "    interval = 5\n",
    "    rolling_mean = [np.mean(scores[(slice_*interval):(slice_+1)*interval]) for slice_ in range(math.ceil(len(scores)/interval))]\n",
    "    x_axis = np.arange(len(rolling_mean)) * interval\n",
    "    plt.plot(x_axis, rolling_mean)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "A suite of methods where we optimize for policy directly.\n",
    "First we define our network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,nS,nA,hidden_dims=(32,32)):\n",
    "        super(Network,self).__init__()\n",
    "        self.activation_fc = F.relu\n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.actions = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        action_probabilities = F.softmax(self.actions(x), dim=1)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy algorithms with np arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 4.51309609413147 in seconds\n",
      "average num episodes before solve 12.695\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def random_policy_search(env):\n",
    "    scores = []\n",
    "    for _ in range(1000):\n",
    "        best_params = None\n",
    "        best_reward = 0\n",
    "        for i in range(100):\n",
    "            parameters = np.random.rand(4) * 2 -1\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "        scores.append(i)\n",
    "    return scores\n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores = random_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "# plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hill Climbing\n",
    "\n",
    "Steepest ascent\n",
    "With 5 param modifications per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy():\n",
    "    def __init__(self, s_size=4, a_size=2):\n",
    "        self.w = 1e-4*np.random.rand(s_size, a_size)  # weights for simple linear policy: state_space x action_space\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = np.dot(state, self.w)\n",
    "        return np.exp(x)/sum(np.exp(x))\n",
    "    \n",
    "    def act(self, state):\n",
    "        probs = self.forward(state)\n",
    "        #action = np.random.choice(2, p=probs) # option 1: stochastic policy\n",
    "        action = np.argmax(probs)              # option 2: deterministic policy\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "\n",
    "def hill_climbing(n_episodes=1000, max_t=1000, gamma=1.0, print_every=100, noise_scale=1e-2):\n",
    "    \"\"\"Implementation of hill climbing with adaptive noise scaling.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        gamma (float): discount rate\n",
    "        print_every (int): how often to print average score (over last 100 episodes)\n",
    "        noise_scale (float): standard deviation of additive noise\n",
    "    \"\"\"\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_R = -np.Inf\n",
    "    best_w = policy.w\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action = policy.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "\n",
    "        if R >= best_R: # found better weights\n",
    "            best_R = R\n",
    "            best_w = policy.w\n",
    "            noise_scale = max(1e-3, noise_scale / 2)\n",
    "            policy.w += noise_scale * np.random.rand(*policy.w.shape) \n",
    "        else: # did not find better weights\n",
    "            noise_scale = min(2, noise_scale * 2)\n",
    "            policy.w = best_w + noise_scale * np.random.rand(*policy.w.shape)\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            policy.w = best_w\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "            \n",
    "scores = hill_climbing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 180.11616897583008 in seconds\n",
      "average num episodes before solve 422.016\n",
      "updates 1109\n",
      "no_improve 104645\n"
     ]
    }
   ],
   "source": [
    "def hill_policy_search(env,num_agents):\n",
    "    scores = []\n",
    "    updates = 0\n",
    "    no_improve = 0\n",
    "    for _ in range(250):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                updates += 1\n",
    "                best_agent = np.argmax(rewards)\n",
    "                best_params = params[best_agent]\n",
    "                best_reward = rewards[best_agent]\n",
    "                if reward == 200:\n",
    "                    break\n",
    "            else:\n",
    "                no_improve += 1\n",
    "        scores.append(i)\n",
    "    return scores,updates,no_improve\n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 5\n",
    "scores,updates,no_improve = hill_policy_search(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('updates',updates)\n",
    "print('no_improve',no_improve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 9.671694040298462 in seconds\n",
      "average num episodes before solve 12.0\n",
      "anneals 19905\n",
      "resets 2906\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def annealed_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,noise - 0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = annealed_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Adaptive noise scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 8.949865818023682 in seconds\n",
      "average num episodes before solve 2.0\n",
      "anneals 21111\n",
      "resets 2999\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def adaptive_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = run_episode(parameters,env)\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = start_noise\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = adaptive_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Average return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 36.22808504104614 in seconds\n",
      "average num episodes before solve 32.0\n",
      "anneals 21123\n",
      "resets 4490\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def average_hill_policy_search(env):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    last_move = None\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        start_noise = 0.1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(250):\n",
    "            parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "            reward = 0\n",
    "            for run in range(3):\n",
    "                reward += run_episode(parameters,env)\n",
    "            reward /= 3\n",
    "            if reward > best_reward:\n",
    "                best_params = parameters\n",
    "                best_reward = reward\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                if last_move == 'anneal':\n",
    "                    noise = min(start_noise,noise-0.04)\n",
    "                else:\n",
    "                    noise -= 0.04\n",
    "                last_move = 'reset'\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "                last_move = 'anneal'\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "scores,anneals,resets = average_hill_policy_search(env)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 192.90656518936157 in seconds\n",
      "average num episodes before solve 98.0\n",
      "anneals 156825\n",
      "resets 3397\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def agents_hill_policy_search(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    for _ in range(500):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        start_noise = 0.1\n",
    "        noise = 0.1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                best_agent = np.argmax(rewards)\n",
    "                best_params = params[best_agent]\n",
    "                best_reward = rewards[best_agent]\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = agents_hill_policy_search(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEM\n",
    "\n",
    "Cross Entropy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 55.59503626823425 in seconds\n",
      "average num episodes before solve 0.0\n",
      "anneals 43458\n",
      "resets 1311\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def CEM(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(150):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                indexes = [i for i,reward in enumerate(rewards) if reward > best_reward]\n",
    "#                 print('indexes',indexes)\n",
    "                avg_params = np.sum(np.array(params)[indexes],axis=0) / len(indexes)\n",
    "                best_params = avg_params\n",
    "                best_reward = sum(np.array(rewards)[indexes]) / len(indexes)\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = min(start_noise,noise-0.04)\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = CEM(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Method\n",
    "\n",
    "Like CEM but with a weighted sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time of computation 61.40742611885071 in seconds\n",
      "average num episodes before solve 499.0\n",
      "anneals 45293\n",
      "resets 1171\n"
     ]
    }
   ],
   "source": [
    "# ultra fast random search\n",
    "def EM(env,num_agents):\n",
    "    scores = []\n",
    "    anneals = 0\n",
    "    resets = 0\n",
    "    start_noise = 0.1\n",
    "    noise = 0.1\n",
    "    for _ in range(150):\n",
    "        best_params = np.random.rand(4) * 2 -1\n",
    "        best_reward = 0\n",
    "        for i in range(500):\n",
    "            rewards = []\n",
    "            params = []\n",
    "            for k in range(num_agents):\n",
    "                parameters = best_params + (np.random.rand(4) * 2 -1)*noise\n",
    "                reward = run_episode(parameters,env)\n",
    "                rewards.append(reward)\n",
    "                params.append(parameters)\n",
    "            if max(rewards) > best_reward:\n",
    "                indexes = [i for i,reward in enumerate(rewards) if reward > best_reward]\n",
    "                rewards = np.array(rewards)\n",
    "                params = np.array(params)\n",
    "                init_sum = np.sum(rewards[indexes])\n",
    "                weighted_sum = np.divide(rewards[indexes],init_sum).reshape(len(indexes),1)\n",
    "#                 print('params[indexes]',params[indexes])\n",
    "#                 print('weighted_sum',weighted_sum)\n",
    "                weighted_params = np.multiply(params[indexes],weighted_sum)\n",
    "#                 print('weighted_params',weighted_params)\n",
    "                best_params = np.sum(weighted_params,axis=0)\n",
    "#                 print('best_params',best_params)\n",
    "                best_reward = sum(rewards[indexes]) / len(indexes)\n",
    "                if reward == 200:\n",
    "                    break\n",
    "                resets += 1\n",
    "                noise = start_noise\n",
    "            else:\n",
    "                anneals += 1\n",
    "                noise += 0.04\n",
    "        scores.append(i)\n",
    "    return i,anneals,resets\n",
    "    \n",
    "        \n",
    "def run_episode(params,env):\n",
    "    state = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        action = 0 if np.matmul(params,state) < 0 else 1\n",
    "        state,reward,done,_ = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "    \n",
    "tic = time.time()\n",
    "num_agents = 3\n",
    "scores,anneals,resets = EM(env,num_agents)\n",
    "toc = time.time()\n",
    "print('time of computation {} in seconds'.format(str(toc-tic)))\n",
    "print('average num episodes before solve',np.mean(scores))\n",
    "print('anneals',anneals)\n",
    "print('resets',resets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same algorithms but with torch NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,env,hidden_dims=(32,32),gate = F.relu):\n",
    "        super(Agent,self).__init__()\n",
    "        self.nS = env.observation_space.shape[0]\n",
    "        self.nA = env.action_space.n\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.h1 = hidden_dims[0]\n",
    "        self.h2 = hidden_dims[1]\n",
    "        self.env = env\n",
    "        \n",
    "        # Layers\n",
    "        self.gate = gate\n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.gate(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.gate(hidden_layer(x))\n",
    "        x = F.softmax(self.output_layer(x))\n",
    "        return x\n",
    "        \n",
    "    def set_weights(self,weights):\n",
    "        hidden_dims = self.hidden_dims\n",
    "        \n",
    "        nS = self.nS\n",
    "        h1 = hidden_dims[0]\n",
    "        h2 = hidden_dims[1]\n",
    "        nA = self.nA\n",
    "        \n",
    "        fc1_end = (nS * h1) + h1\n",
    "        fc2_end = fc1_end + ((h1*h2)+h2)\n",
    "        \n",
    "        # separate the weights for each layer\n",
    "        fc1_W = torch.from_numpy(weights[:nS * h1])\n",
    "        fc1_b = torch.from_numpy(weights[nS * h1: fc1_end])\n",
    "        fc2_W = torch.from_numpy(weights[fc1_end: fc1_end + (h1*h2)])\n",
    "        fc2_b = torch.from_numpy(weights[fc1_end + (h1*h2): fc1_end + (h1*h2) + h2])\n",
    "        fc3_W = torch.from_numpy(weights[fc2_end: fc2_end + (h2*nA)])\n",
    "        fc3_b = torch.from_numpy(weights[fc2_end + (h2*nA): fc2_end + (h2*nA) + nA])\n",
    "        \n",
    "#         s_size = self.s_size\n",
    "#         h_size = self.h_size\n",
    "#         a_size = self.a_size\n",
    "#         # separate the weights for each layer\n",
    "#         fc1_end = (s_size*h_size)+h_size\n",
    "#         fc1_W = torch.from_numpy(weights[:s_size*h_size].reshape(s_size, h_size))\n",
    "#         fc1_b = torch.from_numpy(weights[s_size*h_size:fc1_end])\n",
    "#         fc2_W = torch.from_numpy(weights[fc1_end:fc1_end+(h_size*a_size)].reshape(h_size, a_size))\n",
    "#         fc2_b = torch.from_numpy(weights[fc1_end+(h_size*a_size):])\n",
    "#         # set the weights for each layer\n",
    "#         self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "#         self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "#         self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "#         self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "\n",
    "        # set the weights for each layer\n",
    "        self.input_layer.weight.data.copy_(fc1_W.view_as(self.input_layer.weight.data))\n",
    "        self.input_layer.bias.data.copy_(fc1_b.view_as(self.input_layer.bias.data))\n",
    "        self.hidden_layers[0].weight.data.copy_(fc2_W.view_as(self.hidden_layers[0].weight.data))\n",
    "        self.hidden_layers[0].bias.data.copy_(fc2_b.view_as(self.hidden_layers[0].bias.data))\n",
    "        self.output_layer.weight.data.copy_(fc3_W.view_as(self.output_layer.weight.data))\n",
    "        self.output_layer.bias.data.copy_(fc3_b.view_as(self.output_layer.bias.data))\n",
    "        \n",
    "    def get_weights_dim(self):\n",
    "        return (self.nS+1)*self.h1 + (self.h1+1)*self.h2 + (self.h2+1) * self.nA\n",
    "    \n",
    "    def evaluate(self,weights,gamma=1.0,max_t=1000):\n",
    "        self.set_weights(weights)\n",
    "        total_reward = 0.0\n",
    "        state = self.env.reset()\n",
    "        for t in range(max_t):\n",
    "            action_probs = self.forward(state)[0]\n",
    "            action = np.random.choice(np.arange(self.nA),p=action_probs.detach().numpy())\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            total_reward += reward# * math.pow(gamma,t)\n",
    "            if done:\n",
    "                break\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing\n",
    "\n",
    "With Torch NNs instead of np arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_probs [0.48339996 0.5166001 ]\n",
      "action 1\n",
      "action_probs [0.48646936 0.5135306 ]\n",
      "action 1\n",
      "action_probs [0.4899933 0.5100067]\n",
      "action 1\n",
      "action_probs [0.4912174 0.5087825]\n",
      "action 1\n",
      "action_probs [0.49158344 0.5084166 ]\n",
      "action 0\n",
      "action_probs [0.49127054 0.5087295 ]\n",
      "action 0\n",
      "action_probs [0.49050406 0.5094959 ]\n",
      "action 1\n",
      "action_probs [0.49137488 0.5086251 ]\n",
      "action 1\n",
      "action_probs [0.49074233 0.5092576 ]\n",
      "action 1\n",
      "action_probs [0.48709518 0.51290476]\n",
      "action 1\n",
      "action_probs [0.48264414 0.5173558 ]\n",
      "action 0\n",
      "action_probs [0.4856125 0.5143875]\n",
      "action 0\n",
      "action_probs [0.48834038 0.5116596 ]\n",
      "action 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def hill_climb(nA,agent,env,n_episodes=1800, max_t=200):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    index = 0\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        state = env.reset()\n",
    "        previous_score = 0\n",
    "        score = 0\n",
    "        best_agent = agent\n",
    "        for t in range(max_t):\n",
    "            action_probs = agent(state)[0]\n",
    "            print('action_probs',action_probs.detach().numpy())\n",
    "            action = np.random.choice(np.arange(nA),p=action_probs.detach().numpy())\n",
    "            print('action',action)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "#             agent.step(state,action,reward,next_state,done,index)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            index += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        if previous_score > score:\n",
    "            for parameter in agent.parameters():\n",
    "                parameter.data.copy_(previous_weights.data)\n",
    "        else:\n",
    "            for parameter in agent.parameters():\n",
    "                parameter.data.copy_(parameter.data + np.random.rand()/100)\n",
    "        break\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "    \n",
    "    \n",
    "agent = Agent(env)\n",
    "scores = hill_climb(nA,agent,env)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Method (CEM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Score: 44.80\n",
      "rewards [ 10.  79.  40.  24.  10.   9.  16.   9.  10.  95.  28.  30.  26. 132.\n",
      "  13.  97.  10.  24.  24.  11.  16.  11.  13.  15.  16.  10.  10.  53.\n",
      "  35.  13.  35.   9.  39.  22.  47.  42.   9.  20.   9.  75.  12. 113.\n",
      "  21.   9.  12.  35.   9.  10.   8.  59.]\n",
      "Episode 20\tAverage Score: 91.75\n",
      "rewards [200.  24.  58. 152. 164. 143. 103. 117. 200.  69.  25.  11. 164.  10.\n",
      " 108.  12. 119.  18. 106.  27. 124.  68.  79.  90.  53. 161. 166. 137.\n",
      "  11. 200.  40.  58.  32. 109.  33.  44. 200. 150. 200.  45.  88.  35.\n",
      " 122. 130. 111.  26.  63.  79.  58. 107.]\n",
      "Episode 30\tAverage Score: 127.83\n",
      "rewards [130.  26.  39. 119. 200.  21. 186. 168. 200. 179.  19. 200. 200.  94.\n",
      "  64.  29. 200.  35. 200. 200. 174.  21. 200.   8. 102. 200. 200. 200.\n",
      " 200. 119. 200. 200. 200. 200. 193.  32.  94. 200. 169.  23. 173.  23.\n",
      "  17. 200. 200.  46. 200. 180. 200. 200.]\n",
      "Episode 40\tAverage Score: 145.88\n",
      "rewards [200. 183. 105. 200. 200.  74. 200. 200. 123.  64.  17. 200. 200.  98.\n",
      " 185. 146. 166. 161.  21.  68. 200. 200. 158. 200. 200.  79. 200. 200.\n",
      "  14.  96. 200. 200. 152. 200. 200. 200.  61. 200.  21. 200. 200.  10.\n",
      " 200. 189. 193.  38. 192. 200. 200. 200.]\n",
      "Episode 50\tAverage Score: 156.70\n",
      "rewards [200. 200.  11. 200.  11. 165.  18. 132. 200. 200. 200.   9. 200. 200.\n",
      " 200. 129. 132.   9.  76.  79. 178. 200. 200. 200. 200. 200. 200. 200.\n",
      "  15. 200. 117. 200.  10. 164. 200.  87. 120. 200.  86.  12. 200. 200.\n",
      " 200.  85. 147.  90.   8.  17. 200. 200.]\n",
      "Episode 60\tAverage Score: 163.92\n",
      "rewards [  9.  20. 144. 200.  12. 170. 200. 176. 200.  15. 142.  61. 101. 157.\n",
      "  30. 200. 200. 200. 200. 200. 200. 200.  10.  28.  59. 200.  49.  10.\n",
      " 200. 200. 162.  20.  18.   9. 200. 155. 200. 200. 200. 200.  10. 153.\n",
      "  26. 200. 200. 187. 200.  23. 150. 200.]\n",
      "Episode 70\tAverage Score: 169.07\n",
      "rewards [200. 200.  10. 170. 200. 200. 190. 200. 200. 200. 200. 183. 200. 200.\n",
      "  18. 200. 200. 200. 200. 111. 200. 200. 200.  85. 200. 200. 200. 112.\n",
      " 200. 200. 200. 200. 200. 196. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200.]\n",
      "Episode 80\tAverage Score: 172.94\n",
      "rewards [200. 200. 200. 200. 200.  10. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200.   9.  65. 192. 200. 200. 200. 200. 200. 151. 158. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 170. 200. 200. 200. 200.\n",
      " 200.  11. 200. 200. 200. 109. 200. 200.]\n",
      "Episode 90\tAverage Score: 175.94\n",
      "rewards [200. 200. 200. 200. 200. 200. 200. 200. 200.  83. 183. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 187.   9. 200.\n",
      "  17. 175. 200. 200. 200. 200. 200. 200. 200. 131. 200. 200. 200. 200.\n",
      " 146. 200. 200. 200. 200. 129. 200.  14.]\n",
      "Episode 100\tAverage Score: 178.35\n",
      "rewards [200.  10. 200. 200. 200. 200. 200.   9. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200.  27. 172. 164. 200. 200. 200. 200.\n",
      " 200.  28. 200. 200. 200. 165. 110.  15. 200. 200.  83. 200. 200.  10.\n",
      " 200. 200. 200. 200. 200. 200. 200. 155.]\n",
      "Episode 110\tAverage Score: 193.87\n",
      "rewards [200.  10. 175. 169. 200. 200. 145. 191. 177. 149. 127. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 174. 200. 186.\n",
      " 200. 200.  10. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200. 200.\n",
      " 200. 200. 200. 200. 200. 200. 149.   8.]\n",
      "\n",
      "Environment solved in 11 iterations!\tAverage Score: 195.13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def agent_cem(agent,n_iterations=200,print_every=10,max_t=500,gamma=0.99,population=50,elite_frac=0.2,sigma=0.5):\n",
    "    n_elite = int(population*elite_frac)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_weight = sigma*np.random.randn(agent.get_weights_dim())\n",
    "    \n",
    "    for i_iteration in range(1,n_iterations+1):\n",
    "        weights_pop = [best_weight + (sigma*np.random.randn(agent.get_weights_dim())) for i in range(population)]\n",
    "        rewards = np.array([agent.evaluate(weights,gamma,max_t) for weights in weights_pop])\n",
    "        \n",
    "        elite_idx = rewards.argsort()\n",
    "#         print('elite_idx',elite_idx)\n",
    "#         print('np.arange(population-n_elite,population)',np.arange(population-n_elite,population))\n",
    "        elite_select = elite_idx[np.arange(population-n_elite,population)]\n",
    "#         print('elite_select',elite_select)\n",
    "        elite_weights = [weights_pop[i] for i in elite_select]\n",
    "        best_weight = np.array(elite_weights).mean(axis=0)\n",
    "        \n",
    "        reward = agent.evaluate(best_weight,gamma)\n",
    "        scores_deque.append(reward)\n",
    "        scores.append(reward)\n",
    "        \n",
    "        torch.save(agent.state_dict(),'cem_checkpoint.pth')\n",
    "        \n",
    "        if i_iteration % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_iteration, np.mean(scores_deque)))\n",
    "            print('rewards',rewards)\n",
    "            \n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('\\nEnvironment solved in {:d} iterations!\\tAverage Score: {:.2f}'.format(i_iteration-100, np.mean(scores_deque)))\n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "print(env.action_space.n)\n",
    "agent = Agent(env)\n",
    "scores = agent_cem(agent)\n",
    "plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy_m(nn.Module):\n",
    "    def __init__(self,nS,nA,hidden_dims=(16,16)):\n",
    "        super(Policy_m,self).__init__()\n",
    "        print('nS,nA',nS,nA)\n",
    "        self.fc1 = nn.Linear(nS,hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0],hidden_dims[1])\n",
    "        self.fc3 = nn.Linear(hidden_dims[-1],nA)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(),m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 19.77\n",
      "Episode 200\tAverage Score: 28.92\n",
      "Episode 300\tAverage Score: 18.66\n",
      "Episode 400\tAverage Score: 54.40\n",
      "Episode 500\tAverage Score: 35.91\n",
      "Episode 600\tAverage Score: 56.11\n",
      "Episode 700\tAverage Score: 109.37\n",
      "Episode 800\tAverage Score: 47.93\n",
      "Episode 900\tAverage Score: 89.94\n",
      "Episode 1000\tAverage Score: 130.53\n"
     ]
    }
   ],
   "source": [
    "# policy_m = Policy_m(nS,nA)\n",
    "# policy = Policy_m(nS,nA).to(device)\n",
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr = 1e-2)\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "def reinforce_2(env,policy,optimizer,n_episodes=1000, max_t=1000, gamma=1.00, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action,log_prob = policy.act(state)\n",
    "            state,reward,done,_ = env.step(action)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "#         discounts = [gamma**i for i in range(len(rewards))]\n",
    "#         R = sum([a*b for a,b in zip(discounts,rewards)])\n",
    "        discounts = gamma**np.arange(len(rewards))\n",
    "        future_r = [rewards[i:]*discounts[:-i] if i>0 else rewards*discounts for i in range(len(rewards))]\n",
    "        R = sum([sum(future_r[i]) for i in range(len(future_r))])\n",
    "#         print('R',R)\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "\n",
    "scores = reinforce_2(env,policy,optimizer)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_loss tensor(781.4344, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(97094.8594, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(308.6120, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1453.4479, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4883.8188, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3248.0078, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(311.4699, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(14892.9951, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(482.9884, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2516.9170, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2074.6787, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2043.8636, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3769.8420, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(8547.3613, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2847.8083, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(705.9960, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2913.3965, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(23372.4961, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(663.8660, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4800.0889, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3407.2627, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3719.5308, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2767.5442, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4215.8364, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2930.7883, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(88528.4844, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(9539.2920, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1472.0809, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(641.6562, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12327.9346, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4487.0845, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(60818.7266, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(11528.2002, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1228.6036, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1225.9509, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(13604.0703, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(20759.4805, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(830.8467, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(485.0138, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3721.8354, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(10318.9326, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4833.2363, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16146.8223, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(294388.8750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1762.5757, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3990.7717, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1209.4082, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(617.4130, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2791.5151, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12368.3965, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(785.0694, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(44063.6211, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(958.0059, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(764.8237, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7920.3574, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1418.3312, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(40751.9922, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2382.4392, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1174.0081, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(9279.5664, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4823.8418, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1429.5450, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1194.1891, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4241.1484, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7602.8525, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(967.7641, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(758.9808, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(76005.3125, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3234.7612, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7460.2036, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(794.7091, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(602.4135, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(56047.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(14691.5117, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2458.9373, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(66555.6797, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6726.6235, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1187.7997, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16023.2227, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(17660.0059, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(21569.5566, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(607.0061, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1454.2870, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(21824.2227, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6707.5195, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3223.9780, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(10158.3027, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4880.1250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3173.2859, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(668.1889, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(14663.4561, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3898.3237, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1002.4476, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(33294.5586, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(109897.0156, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(20697.9805, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(24764.0664, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(20001.9883, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4181.6689, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12035.0967, grad_fn=<SumBackward0>)\n",
      "Episode 100\tAverage Score: 26.58\n",
      "policy_loss tensor(11518.8564, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(64013.6172, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1064.4518, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(18864.3027, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(15250.0098, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(11643.6484, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(317061.9688, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(11133.0342, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(9754.6006, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(17703.9746, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(46109.1953, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(93850.9766, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(13602.3418, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(686.0543, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(8003.5498, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6183.4180, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(9260.3438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(844.5444, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(512.4508, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1472.6096, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16876.5293, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2089.8096, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(409268.7188, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(5936.0381, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(150698.4219, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7759.3208, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7544.8892, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(73243.5391, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(43690.6797, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(11832.5781, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(50467.2305, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6082.7612, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16613.6426, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(221715.6719, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7073.8330, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(173764.9062, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12521.0625, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(25929.7988, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7725.9116, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(15695.4463, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(350202.9375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(21745.2422, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(70127.4062, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(288691.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(72027.5625, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(32500.8008, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(127431.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(23724.5801, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(426712.1562, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(63207.3750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4706.0879, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_loss tensor(62114.5859, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(208792.5156, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(291854.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(8495.1621, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(8240.2910, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(39748.6719, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(22944.9492, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(78289.9297, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(586966.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6258.0049, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1847.3353, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(415990., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(45198.1758, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(221336.8750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(268693.3438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(107318.8047, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(672744.4375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2794.9541, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7061.3496, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(146317.2656, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(361913.4688, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(329827.8438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(292155.7188, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(206741.3438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12537.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(323346.1562, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(23562.5723, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7079.8906, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7981.0601, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(14685.7266, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2231.1226, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(156378.3281, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(53957.2930, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2674.6189, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(37152.2891, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(22846.9004, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(40546.4453, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(10749.0488, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(22002.5156, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12031.5029, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(62755.7031, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16341.3789, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(309570.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6010.9717, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(19904.3477, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(622410.8750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(22947.7090, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(15604.8018, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2810.9429, grad_fn=<SumBackward0>)\n",
      "Episode 200\tAverage Score: 51.80\n",
      "policy_loss tensor(95852.6094, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(21479.6582, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(26991.7559, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(91442.4922, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2006249.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(17988.5723, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(87582.2266, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2503152.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(171970.5312, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(100127.4688, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(158857.3750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(43233.1719, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(252570.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(4187.4780, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(492038.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(61877.3164, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(121939.3750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(54780.0781, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(43815.1055, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16787.5996, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(26619.4805, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(17174.8398, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(27929.4805, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(161070.2812, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(55457.3984, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(131154.8594, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(14745.6680, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(26863.6211, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(18642.2852, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(18180.3848, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(8357.5732, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(3024.5173, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(69699.5938, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(10331.1797, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(134315., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(56094.4883, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(24273.0859, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(15489.8076, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(23145.6367, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(24012.9355, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(9573.0938, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(22439.7852, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(50797.9609, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(85984.2578, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(25755.4648, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(32632.0469, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(201757.3594, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(19303.2520, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(29327.0488, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(159935.0156, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(27669.8125, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(45733.6016, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(36454.4219, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(510410.3750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(135801.6875, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(85398.4141, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(393208.5938, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2474673.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(28665.9141, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(723518.3125, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(798910.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(378481., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(990428.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(124407.6172, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(161333.3281, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(13763.8965, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(144466.2969, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(563341.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1012294.0625, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(40791.1562, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(867787.3125, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1189545.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(860748.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(860075.9375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(142060.8750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(981633.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2305416., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2400051.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(190740.8125, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1699163.3750, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(27776.1250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(119478.0625, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(53041.6641, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(276625.8438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(43592.4648, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(132037.9844, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(79412.2344, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(244023.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(102378.4219, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(19963.8691, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(65469.8203, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(30139.7539, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(90262.0703, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(26167.6328, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(45321.0078, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(27203.3906, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(38652.5586, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(5107.9507, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(31290.0508, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(20728.9219, grad_fn=<SumBackward0>)\n",
      "Episode 300\tAverage Score: 74.98\n",
      "policy_loss tensor(27577.7676, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(26555.9688, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6485.0786, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(137497.4062, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(24506.0918, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(13812.1270, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_loss tensor(28059.0059, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(66512.2109, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(24378.2520, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(16744.7598, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(7893.8315, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(79980.6719, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(105275.4297, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(60165.9688, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(32282.8984, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(49777.1406, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(42457.3984, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(38616.4961, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(151556.7969, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(36683.0547, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(6450.5259, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(87189.1172, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(222860.9375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12493.1924, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(113681.3438, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(81937.4062, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(49597.8281, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(122202.4844, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(114925.0625, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(57835.5820, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(65407.3789, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(10989.3340, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(165949.5312, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(129746.3047, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(110868.4062, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(104277.6094, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(56889.4609, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(144201.7969, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(116196.0781, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(108084.4766, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(74548.2109, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(12410.6553, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(174038.2344, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(450079.1250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(99905.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(33520.2812, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(310087.0938, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(75887.0469, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(417422.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(63058.8164, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(383372.2812, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(504927.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(315129.0938, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2386370.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1411137., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2406755.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(155907.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2458357.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(198147.6094, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2304077., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1266342.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1341451.6250, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(721697.9375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2448388., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2432591.7500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2382989., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(947865.9375, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2343085.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(599125., grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2385469.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2398930.2500, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2357140.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(1604116.5000, grad_fn=<SumBackward0>)\n",
      "policy_loss tensor(2252545.5000, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a34b51026364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-a34b51026364>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(env, policy, n_episodes, max_t, gamma, print_every)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "def reinforce(env,policy,n_episodes=1000, max_t=1000, gamma=1.0, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "        \n",
    "#         discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "#         R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        discounts = gamma**np.arange(len(rewards))\n",
    "        future_r = [rewards[i:]*discounts[:-i] if i>0 else rewards*discounts for i in range(len(rewards))]\n",
    "        R = sum([sum(future_r[i]) for i in range(len(future_r))])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        print('policy_loss',policy_loss)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque)>=195.0:\n",
    "            print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_deque)))\n",
    "            break\n",
    "        \n",
    "    return scores\n",
    "    \n",
    "scores = reinforce(env,policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "Categorical cross entropy:\n",
    "The final layer is a softmax, and then cross entropy\n",
    "\n",
    "−(ylog(p)+(1−y)log(1−p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CrossEntropy(yhat,y):\n",
    "    if y == 1:\n",
    "        return -log(yhat)\n",
    "    else:\n",
    "        return -log(1 - yhat)\n",
    "    \n",
    "# short form\n",
    "def CE(yhat,y):\n",
    "    return -log((-1+y) + yhat)\n",
    "\n",
    "# multiclass\n",
    "def MCE(yhat,y):\n",
    "    # if they are vectors\n",
    "    return -log(np.add(yhat,np.add(y,-1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reinforce again\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,nS,nA,seed,hidden_dims=(32,32)):\n",
    "        super(Policy,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear()\n",
    "        self.fc2 = nn.Linear()\n",
    "        self.fc = nn.ModuleList()\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def act(self):\n",
    "        pass\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reward normalization\n",
    "\n",
    "def norm_train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "            \n",
    "        mean_r = np.mean(rewards)\n",
    "        N = len(rewards)\n",
    "        sigma_r = np.sqrt(sum((rewards-mean_r)**2)/N)\n",
    "        norm_r = rewards - mean_r / simga_r\n",
    "        \n",
    "        \n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Future rewards\n",
    "\n",
    "def future_train(agent,env,i_episode=2,gamma=1.0):\n",
    "    trajectories = []\n",
    "    rewards = []\n",
    "    scores = []\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        logs = []\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            action,log_probs = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            logs.append(log_probs)\n",
    "            trajectories.append((state,action,reward,next_state))\n",
    "            state = next_state\n",
    "        future_rewards = [sum(rewards[i:]) for i in range(rewards)]\n",
    "        discounted_rewards = [reward*(gamma**i) for i,reward in enumerate(future_rewards)]\n",
    "        gradient = sum([d_reward * log for d_reward,log in zip(discounted_rewards,logs)]) \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        learning_rate * gradient\n",
    "        optimizer.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
