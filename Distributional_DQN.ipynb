{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The output shape looks like (N_atoms,N_actions)\n",
    "Softmax is applied independantly for each action dimension of the output, \n",
    "    to ensure each action dimension is appropriately normalized.\n",
    "    \n",
    "Z is a vector with N_atoms. Defined by Z_i = v_min + (i-1) v_max - v_min / N_atoms - 1\n",
    "With Probability mass P_i-theta (S_t,A_t) on each atom i such that d_t = (Z,P_theta(S_t,A_T))\n",
    "The goal is to update theta such that this distrubtion matches the actual distribution of returns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class Dueling_QNetwork(nn.Module):\n",
    "    def __init__(self,atoms,state_space,action_space,seed,hidden_dims=(32,32),activation_fc=F.relu):\n",
    "        super(Dueling_QNetwork,self).__init__()\n",
    "        self.activation_fc = activation_fc\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.atoms = atoms\n",
    "        \n",
    "        print('hidden_dims',hidden_dims)\n",
    "        self.input_layer = nn.Linear(state_space,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.value_output = nn.Linear(hidden_dims[-1],atoms)\n",
    "        self.advantage_output = nn.Linear(hidden_dims[-1],atoms*action_space)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        a = self.advantage_output(x)\n",
    "        # Reshape for distributional output\n",
    "        a = a.view(self.action_space,self.atoms)\n",
    "        v = self.value_output(x)\n",
    "        v = v.expand_as(a)\n",
    "        q = v + a - a.mean(1,keepdim=True).expand_as(a)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Tree.\n",
    "3 tiered tree structure containing\n",
    "Root node (Object. sum of all lower values)\n",
    "Intermediate Node (Object. Root as parent, sums a given slice of the priority array)\n",
    "Priority Array (Array of priorities, length buffer_size)\n",
    "\n",
    "The number of Intermediate nodes is calculated by the buffer_size / batch_size.\n",
    "\n",
    "I_episode: current episode of training\n",
    "\n",
    "Index: is calculated by i_episode % buffer_size. This loops the index after exceeding the buffer_size.\n",
    "\n",
    "Indicies: (List) of memory/priority entries\n",
    "\n",
    "intermediate_dict: maps index to intermediate node. Since each Intermediate node is responsible \n",
    "for a given slice of the priority array, given a particular index, it will return the Intermediate node\n",
    "'responsible' for that index.\n",
    "\n",
    "## Functions:\n",
    "\n",
    "Add:\n",
    "Calculates the priority of each TD error -> (abs(TD_error)+epsilon)**alpha\n",
    "Stores the priority in the Priority_array.\n",
    "Updates the sum_tree with the new priority\n",
    "\n",
    "Update_Priorities:\n",
    "Updates the index with the latest priority of that sample. As priorities can change over training\n",
    "for a particular experience\n",
    "\n",
    "Sample:\n",
    "Splits the current priority_array based on the number of entries, by the batch_size.\n",
    "Returns the indicies of those samples and the priorities.\n",
    "\n",
    "Propogate:\n",
    "Propogates the new priority value up through the tree\n",
    "\"\"\"\n",
    "\n",
    "class PriorityTree(object):\n",
    "    def __init__(self,buffer_size,batch_size,alpha,epsilon):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_intermediate_nodes = round(buffer_size / batch_size)\n",
    "        self.current_intermediate_node = 0\n",
    "        self.root = Node(None)\n",
    "        self.intermediate_nodes = [Intermediate(self.root,batch_size*x,batch_size*(x+1)) for x in range(self.num_intermediate_nodes)]\n",
    "        self.priority_array = np.zeros(buffer_size)\n",
    "        self.intermediate_dict = {}\n",
    "        for index,node in enumerate(self.intermediate_nodes):\n",
    "            for key in range((batch_size*(index+1))-batch_size,batch_size*(index+1)):\n",
    "                self.intermediate_dict[key] = node\n",
    "        print('Priority Tree: Batch Size {} Buffer size {} Number of intermediate Nodes {}'.format(batch_size,buffer_size,self.num_intermediate_nodes))\n",
    "        \n",
    "    def add(self,TD_error,index):\n",
    "        priority = (abs(TD_error)+self.epsilon)**self.alpha\n",
    "        self.priority_array[index] = priority\n",
    "        # Update sum\n",
    "        propogate(self.intermediate_dict[index],self.priority_array)\n",
    "    \n",
    "    def sample(self,index):\n",
    "        # Sample one experience uniformly from each slice of the priorities\n",
    "        if index >= self.buffer_size:\n",
    "            indicies = [random.sample(list(range(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes)),1)[0] for sample in range(self.batch_size)]\n",
    "        else:\n",
    "            interval = int(index / self.batch_size)\n",
    "            indicies = [random.sample(list(range(sample*interval,(sample+1)*interval)),1)[0] for sample in range(self.batch_size)]\n",
    "#         print('indicies',indicies)\n",
    "        priorities = self.priority_array[indicies]\n",
    "        return priorities,indicies\n",
    "    \n",
    "    def update_priorities(self,TD_errors,indicies):\n",
    "#         print('TD_errors',TD_errors)\n",
    "#         print('TD_errors shape',TD_errors.shape)\n",
    "        priorities = (abs(TD_errors)+self.epsilon)**self.alpha\n",
    "#         print('priorities shape',priorities.shape)\n",
    "#         print('indicies shape',len(indicies))\n",
    "#         print('self.priority_array shape',self.priority_array.shape)\n",
    "        self.priority_array[indicies] = priorities\n",
    "        # Update sum\n",
    "        nodes = [self.intermediate_dict[index] for index in indicies] \n",
    "        intermediate_nodes = set(nodes)\n",
    "        [propogate(node,self.priority_array) for node in intermediate_nodes]\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self,parent):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "            \n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def sum_children(self):\n",
    "        return sum([child.value for child in self.children])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "class Intermediate(Node):\n",
    "    def __init__(self,parent,start,end):\n",
    "        self.parent = parent\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.value = 0\n",
    "        parent.add_child(self)\n",
    "    \n",
    "    def sum_leafs(self,arr):\n",
    "        return np.sum(arr[self.start:self.end])\n",
    "\n",
    "def propogate(node,arr):\n",
    "    if node.parent != None:\n",
    "        node.value = node.sum_leafs(arr)\n",
    "        propogate(node.parent,arr)\n",
    "    else:\n",
    "        node.value = node.sum_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Buffer HyperParameters\n",
    "alpha(priority or w) dictates how biased the sampling should be towards the TD error. 0 < a < 1\n",
    "beta(IS) informs the importance of the sample update\n",
    "\n",
    "The paper uses a sum tree to calculate the priority sum in O(log n) time. As such, i've implemented my own version\n",
    "of the sum_tree which i call priority tree.\n",
    "\n",
    "We're increasing beta(IS) from 0.5 to 1 over time\n",
    "alpha(priority) we're holding constant at 0.5\n",
    "\"\"\"\n",
    "\n",
    "class PriorityReplayBuffer(object):\n",
    "    def __init__(self,action_size,buffer_size,batch_size,seed,alpha=0.5,beta=0.5,beta_end=1,beta_duration=1e+5,epsilon=7e-5):\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_duration = beta_duration\n",
    "        self.beta_increment = (beta_end - beta) / beta_duration\n",
    "        self.max_w = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.TD_sum = 0\n",
    "\n",
    "        self.experience = namedtuple('experience',field_names=['state','action','reward','next_state','done','i_episode'])\n",
    "        self.sum_tree = PriorityTree(buffer_size,batch_size,alpha,epsilon)\n",
    "        self.memory = {}\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done,TD_error,i_episode):\n",
    "        e = self.experience(state,action,reward,next_state,done,i_episode)\n",
    "        index = i_episode % self.buffer_size\n",
    "        # add memory to memory and add corresponding priority to the priority tree\n",
    "        self.memory[index] = e\n",
    "        self.sum_tree.add(TD_error,index)\n",
    "\n",
    "    def sample(self,index):\n",
    "        # We times the error by these weights for the updates\n",
    "        # Super inefficient to sum everytime. We could implement the tree sum structure. \n",
    "        # Or we could sum once on the first sample and then keep track of what we add and lose from the buffer.\n",
    "        # priority^a over the sum of the priorities^a = likelyhood of the given choice\n",
    "        # Anneal beta\n",
    "        self.update_beta()\n",
    "        # Get the samples and indicies\n",
    "        priorities,indicies = self.sum_tree.sample(index)\n",
    "        # Normalize with the sum\n",
    "        norm_priorities = priorities / self.sum_tree.root.value\n",
    "        samples = [self.memory[index] for index in indicies]\n",
    "#         samples = list(operator.itemgetter(*self.memory)(indicies))\n",
    "#         samples = self.memory[indicies]\n",
    "        # Importance weights\n",
    "#         print('self.beta',self.beta)\n",
    "#         print('self.beta',self.buffer_size)\n",
    "        importances = [(priority * self.buffer_size)**-self.beta for priority in norm_priorities]\n",
    "        self.max_w = max(self.max_w,max(importances))\n",
    "        # Normalize importance weights\n",
    "#         print('importances',importances)\n",
    "#         print('self.max_w',self.max_w)\n",
    "        norm_importances = [importance / self.max_w for importance in importances]\n",
    "#         print('norm_importances',norm_importances)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in samples if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in samples if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in samples if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in samples if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in samples if e is not None])).float().to(device)\n",
    "        \n",
    "        if index % 4900 == 0:\n",
    "            print('beta',self.beta)\n",
    "            print('self.max_w',self.max_w)\n",
    "            print('len mem',len(self.memory))\n",
    "            print('tree sum',self.sum_tree.root.value)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones),indicies,norm_importances\n",
    "\n",
    "    def update_beta(self):\n",
    "#         print('update_beta')\n",
    "#         print('self.beta_end',self.beta_end)\n",
    "#         print('self.beta_increment',self.beta_increment)\n",
    "        self.beta += self.beta_increment\n",
    "        self.beta = min(self.beta,self.beta_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DQN with Priority Replay, DDQN, Dueling DQN and Distributional DQN.\n",
    "\"\"\"\n",
    "class Dist_DQN(object):\n",
    "    def __init__(self,state_space,action_space,seed,update_every,batch_size,buffer_size,min_buffer_size,learning_rate,GAMMA,tau,clip_norm,alpha,N_atoms,v_max,v_min):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.min_buffer_size = min_buffer_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_every = update_every\n",
    "        self.GAMMA = GAMMA\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # For polyak averaging\n",
    "        self.tau = tau\n",
    "        self.clip_norm = clip_norm\n",
    "        \n",
    "        # For distributional\n",
    "        self.N_atoms = N_atoms # 51 for C51\n",
    "        self.v_max = v_max # Max possible score\n",
    "        self.v_min = v_min # Min possible score\n",
    "        self.delta_z = (self.v_max - self.v_min) / float(self.N_atoms - 1)\n",
    "        self.atoms = np.linspace(v_min,v_max,N_atoms)\n",
    "        \n",
    "        # Deep Q Networks\n",
    "        self.qnetwork_local = Dueling_QNetwork(N_atoms,state_space,action_space,seed)\n",
    "        self.qnetwork_target = Dueling_QNetwork(N_atoms,state_space,action_space,seed)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=learning_rate)\n",
    "        # Initialize replaybuffer\n",
    "        \n",
    "        self.memory = PriorityReplayBuffer(action_space,buffer_size,batch_size,seed,alpha)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self,state,action,reward,next_state,done,index):\n",
    "        # Calculate TD error\n",
    "        # Target np.argmax(np.max(output.detach().numpy(),axis=1))\n",
    "        current_network_action = np.argmax(np.max(self.qnetwork_local(next_state).detach().numpy(),axis=1))\n",
    "        print('current_network_action',current_network_action)\n",
    "        # initial state comes in as (1,4), squeeze to get (4)\n",
    "        target = reward + self.GAMMA*(self.qnetwork_target(next_state)[current_network_action])\n",
    "        # Local. same rational for squeezing\n",
    "        local = self.qnetwork_local(state).squeeze(0)[action]\n",
    "        TD_error = reward + target - local\n",
    "        # Save the experience\n",
    "        self.memory.add(state,action,reward,next_state,done,TD_error,index)\n",
    "        \n",
    "        # learn from the experience\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            if len(self.memory) > self.min_buffer_size:\n",
    "                experiences,indicies,weights = self.memory.sample(index)\n",
    "                self.learn(experiences,indicies,weights)\n",
    "        \n",
    "    def act(self,state,eps=0.):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        if random.random() > eps:\n",
    "            z_concat = np.vstack(z.detach().squeeze(0).numpy())\n",
    "            q = np.sum(np.multiply(z_concat, self.atoms), axis=1) \n",
    "            action_idx = np.argmax(q)\n",
    "            return action_idx\n",
    "        else:\n",
    "            return np.random.choice(np.arange(self.action_space))\n",
    "        \n",
    "    def learn(self,experiences,indicies,weights):\n",
    "        \n",
    "        states,actions,rewards,next_states,dones = experiences\n",
    "        # Local max action\n",
    "        local_next_state_actions = self.qnetwork_local(next_states).max(2).unsqueeze(1)\n",
    "        # Target\n",
    "        target_values = self.qnetwork_target(next_states).detach()\n",
    "        max_target = target_values.gather(1,local_next_state_actions)\n",
    "#         print('max_target size',max_target.size())\n",
    "        max_target *= (1-dones) \n",
    "        targets = rewards + (self.GAMMA*max_target)\n",
    "#         print('targets',targets.size())\n",
    "        # Probability mass\n",
    "        m_prob = [np.zeros((BATCH_SIZE, N_atoms)) for i in range(nA)]\n",
    "#         targets = rewards + self.GAMMA*(target_values.gather(1,local_next_state_actions))\n",
    "        # Local\n",
    "        local = self.qnetwork_local(states).gather(1,actions)\n",
    "        TD_error = local - targets\n",
    "#         loss = ((torch.tensor(weights) * TD_error)**2*0.5).mean()\n",
    "        \n",
    "        z = DNetwork(next_states).detach().numpy() # Return a list [32x51, 32x51, 32x51]\n",
    "        # Get Optimal Actions for the next states (from distribution z)\n",
    "        z_concat = np.vstack(z)\n",
    "        local_values = self.z_to_q(z_concat)\n",
    "        local_action_idxs = np.argmax(local_values, axis=1)\n",
    "        print('local_action_idxs',local_action_idxs.shape)\n",
    "        \n",
    "        z_ = target_DNetwork(next_states).detach().numpy() # Return a list [32x51, 32x51, 32x51]\n",
    "        z_prime_concate = np.vstack(z_)\n",
    "        target_values = self.z_to_q(z_prime_concate)\n",
    "        # Select actions by local, evaluate with target\n",
    "        max_target = target_values.gather(1,local_action_idxs)\n",
    "        # Mask with dones\n",
    "        max_target *= (1-dones) \n",
    "        targets = rewards + (self.GAMMA*max_target)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        a_next = tf.argmax(z_, 1, output_type=tf.int32)\n",
    "        batch_dim = tf.shape(rew_t_ph)[0]\n",
    "        ThTz, debug = build_categorical_alg(p_tp1, rew_t_ph, a_next, gamma, batch_dim, done_mask_ph, dist_params)\n",
    "\n",
    "        # compute the error (potentially clipped)\n",
    "        cat_idx = tf.transpose(tf.reshape(tf.concat([tf.range(batch_dim), act_t_ph], axis=0), [2, batch_dim]))\n",
    "        p_t_next = tf.gather_nd(p_t, cat_idx)\n",
    "\n",
    "        cross_entropy = -1 * ThTz * tf.log(p_t_next)\n",
    "        errors = tf.reduce_sum(cross_entropy, axis=-1)\n",
    "\n",
    "        mean_error = tf.reduce_mean(errors)\n",
    "        \n",
    "        \n",
    "        \n",
    "        max_target = z_prime_concate[optimal_action_idxs]\n",
    "        max_target *= (1-dones)\n",
    "        targets = rewards + (self.GAMMA*max_target)\n",
    "        local = self.qnetwork_local(states).gather(1,actions)\n",
    "        TD_error = local - targets\n",
    "        \n",
    "        # Project Next State Value Distribution (of optimal action) to Current State\n",
    "        for i in range(num_samples):\n",
    "            if done[i]: # Terminal State\n",
    "                # Distribution collapses to a single point\n",
    "                Tz = min(self.v_max, max(self.v_min, rewards[i]))\n",
    "                bj = (Tz - self.v_min) / self.delta_z \n",
    "                m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                m_prob[action[i]][i][int(m_l)] += (m_u - bj)\n",
    "                m_prob[action[i]][i][int(m_u)] += (bj - m_l)\n",
    "            else:\n",
    "                for j in range(self.num_atoms):\n",
    "                    Tz = min(self.v_max, max(self.v_min, reward[i] + self.gamma * self.z[j]))\n",
    "                    bj = (Tz - self.v_min) / self.delta_z \n",
    "                    m_l, m_u = math.floor(bj), math.ceil(bj)\n",
    "                    m_prob[action[i]][i][int(m_l)] += z_[optimal_action_idxs[i]][i][j] * (m_u - bj)\n",
    "                    m_prob[action[i]][i][int(m_u)] += z_[optimal_action_idxs[i]][i][j] * (bj - m_l)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(local, targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.qnetwork_local.parameters(),self.clip_norm)\n",
    "        self.optimizer.step()\n",
    "        # Update the priorities\n",
    "        TD_errors = np.abs(TD_error.squeeze(1).detach().cpu().numpy())\n",
    "        self.memory.sum_tree.update_priorities(TD_errors,indicies)\n",
    "        self.update_target()\n",
    "        \n",
    "    def z_to_q(self,z_concat):\n",
    "        q_values = np.sum(np.multiply(z_concat, self.Z), axis=1) # length (num_atoms x num_actions)\n",
    "        return q.reshape((BATCH_SIZE, nA), order='F')\n",
    "    \n",
    "    def pick_actions(self,z):\n",
    "        q_values = z_to_q(z)\n",
    "        optimal_action_idxs = np.argmax(q_values, axis=1)\n",
    "        return optimal_action_idxs\n",
    "    \n",
    "    def build_categorical(self):\n",
    "        cat_idx = tf.transpose(tf.reshape(tf.concat([tf.range(batch_dim), a_next], axis=0), [2, batch_dim]))\n",
    "        p_best = tf.gather_nd(p_ph, cat_idx)\n",
    "\n",
    "        big_z = tf.reshape(tf.tile(z, [batch_dim]), [batch_dim, nb_atoms])\n",
    "        big_r = tf.transpose(tf.reshape(tf.tile(r_ph, [nb_atoms]), [nb_atoms, batch_dim]))\n",
    "\n",
    "        Tz = tf.clip_by_value(big_r + gamma * tf.einsum('ij,i->ij', big_z, 1.-done_mask), Vmin, Vmax)\n",
    "\n",
    "        big_Tz = tf.reshape(tf.tile(Tz, [1, nb_atoms]), [-1, nb_atoms, nb_atoms])\n",
    "        big_big_z = tf.reshape(tf.tile(big_z, [1, nb_atoms]), [-1, nb_atoms, nb_atoms])\n",
    "\n",
    "        Tzz = tf.abs(big_Tz - tf.transpose(big_big_z, [0, 2, 1])) / dz\n",
    "        Thz = tf.clip_by_value(1 - Tzz, 0, 1)\n",
    "\n",
    "        ThTz = tf.einsum('ijk,ik->ij', Thz, p_best)\n",
    "\n",
    "    return ThTz, {'p_best': p_best}\n",
    "\n",
    "    # def update_target(self,tau):\n",
    "    #     for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "    #         target_param.data.copy_(local_param.data)\n",
    "        \n",
    "    # Polyak averaging  \n",
    "    def update_target(self):\n",
    "        for local_param,target_param in zip(self.qnetwork_local.parameters(),self.qnetwork_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1-self.tau)*target_param.data)\n",
    "#         self.qnetwork_local.parameters() = TAU*self.qnetwork_local.parameters() + (1-TAU)*self.qnetwork_target.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,n_episodes=1800, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        Instead of updating target every (int) steps, using Polyak updating of .1 to gradually merge the networks\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    eps = eps_start\n",
    "    index = 0\n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done,index)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            index += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        eps = max(eps*eps_decay,eps_end)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) >= 200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "MIN_BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 50\n",
    "ALPHA = 0.6 # 0.7 or 0.6\n",
    "START_BETA = 0.5 # from 0.5-1\n",
    "END_BETA = 1\n",
    "LR = 0.00025\n",
    "EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.01\n",
    "UPDATE_EVERY = 4\n",
    "CLIP_NORM = 10\n",
    "\n",
    "# Distributional Hyperparameters\n",
    "V_max = 200\n",
    "V_min = -200\n",
    "N_atoms = 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space 8, Action Space 4\n",
      "hidden_dims (32, 32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[31;1mint\u001b[0m)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[31;1mint\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-576abfc69898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Observation Space {}, Action Space {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPriority_DQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUPDATE_EVERY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMIN_BUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCLIP_NORM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mALPHA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV_max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9288c94ce36d>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_space, action_space, seed, update_every, batch_size, buffer_size, min_buffer_size, learning_rate, GAMMA, tau, clip_norm, alpha, N_atoms, v_max, v_min)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Deep Q Networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDueling_QNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDueling_QNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqnetwork_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-819e19c60335>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, atoms, state_space, action_space, seed, hidden_dims, activation_fc)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvantage_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (tuple, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[31;1mint\u001b[0m)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtuple\u001b[0m, \u001b[31;1mint\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.shape[0]\n",
    "print('Observation Space {}, Action Space {}'.format(nS,nA))\n",
    "seed = 7\n",
    "agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA,N_atoms,V_max,V_min)\n",
    "\n",
    "scores = train(agent,env)\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Rolling mean plot\n",
    "interval = 25\n",
    "rolling_mean = [np.mean(scores[(slice_*interval):(slice_+1)*interval]) for slice_ in range(math.ceil(len(scores)/interval))]\n",
    "x_axis = np.arange(len(rolling_mean)) * interval\n",
    "plt.plot(x_axis, rolling_mean)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "plt.savefig('test'+ str(j)+'.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the bot in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # load the weights from file\n",
    "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "except:  \n",
    "    # Else reinstantiate the environment and other variables\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    env.seed(0)\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.shape[0]\n",
    "    seed = 7\n",
    "    agent = Priority_DQN(nS,nA,seed,UPDATE_EVERY,BATCH_SIZE,BUFFER_SIZE,MIN_BUFFER_SIZE,LR,GAMMA,TAU,CLIP_NORM,ALPHA)\n",
    "    agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "    \n",
    "\n",
    "for i in range(1):\n",
    "    state = env.reset()\n",
    "    img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    for j in range(500):\n",
    "        action = agent.act(state)\n",
    "        img.set_data(env.render(mode='rgb_array')) \n",
    "        plt.axis('off')\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # save the image\n",
    "#         plt.savefig('test'+ str(j)+'.png',bbox_inches='tight')\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
