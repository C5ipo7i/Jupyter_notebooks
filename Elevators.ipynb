{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions.distribution as dist\n",
    "import torch.distributions.categorical as Categorical\n",
    "\n",
    "import math\n",
    "from functools import reduce\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Elevator problem\n",
    "\n",
    "The goal of elevators is to deliver people to their destination in an efficient manner.\n",
    "\n",
    "\n",
    "## The Goal\n",
    "\n",
    "Design a system that solves the elevator problem by minimizing the amount of energy required for each elevator, the wait times for all passengers. \n",
    "\n",
    "**Two Types of Elevator systems**\n",
    "1. up/down button outside, exact floor inside\n",
    "2. input destination outside\n",
    "\n",
    "Constraints:\n",
    "- Time taken within the elevator is valued less than time waiting at a floor\n",
    "- Predictive analytics for where people are going to be. (Elevators sent to the most popular floor if no route)\n",
    "- Maximum allowable wait period\n",
    "- Energy used by each elevator\n",
    "- Elevator capacity\n",
    "\n",
    "could make each floor an object, that contains people (if any).\n",
    "could make each person an object that contains current floor and destination\n",
    "could make each person a tuple that is contained within the elevator class\n",
    "Separate people into inside the elevator and waiting for elevator. and done.\n",
    "Doesn't matter if there are multiple people on each floor. Its effectively 1 person, except for when it reaches capacity. \n",
    "\n",
    "\n",
    "## The training structure\n",
    "\n",
    "the training algorithm will go by steps. each step will add the current amount of time spent by people inside and outside the elevators to a collective time spent. That will be the reward signal. \n",
    "\n",
    "1. **Rewards**: Collective time spent waiting (can be weighted to bias for time inside or outside the elevator)\n",
    "2. **Done**: True when all passengers have been delivered\n",
    "3. **Penalty**: for exceeding maximum wait time. If this is true, then the agent must know the start time for each passenger.\n",
    "4. **Step**: increments elevators towards their current destinations. increments cumulative time waited.\n",
    "5. **Elevators**: Only pick people up when they reach a destination floor, and or drop people off.\n",
    "\n",
    "**Initially solve for the simpliest example of one person and one elevator**\n",
    "\n",
    "### Input\n",
    "\n",
    "The remaining people on each floor, their destinations and time they pressed the button. Locations of elevators and their destinations. The notion of people is a little strange as its purely about how many different destinations (if any) per floor\n",
    "\n",
    "1 hot representation of floors. each floor has a 1 hot representation of the destinations. where 0 or more can be turned on. \n",
    "1 hot representation of elevator locations,and elevator destinations, and the destinations of the people inside the elevators.\n",
    "\n",
    "- N_planes = N_people + N_destinations + N_elevators + N_elevator_targets + N_elevator_destinations\n",
    "- Input shape (N_planes,N_floors)\n",
    "\n",
    "### Actions\n",
    "\n",
    "- consist of destinations for each elevator.\n",
    "- action shape (N_elevators,N_floors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hotel(object):\n",
    "    def __init__(self,N_floors,N_elevators,N_people,capacity=None):\n",
    "        \"\"\"\n",
    "        N_floors : Int, Number of floors for the building\n",
    "        N_elevators : Int, Number of elevators\n",
    "        N_people : Int, Number of people\n",
    "        L_destinations : list, \n",
    "        Time_weights : list of weights, [0] weight for outside elevator, [1] weight for inside elevator\n",
    "        \"\"\"\n",
    "        self.N_floors = N_floors\n",
    "        self.N_elevators = N_elevators\n",
    "        self.N_people = N_people\n",
    "        \n",
    "        # Constraints\n",
    "        self.capacity = capacity\n",
    "        self.time_weights = [1,1]\n",
    "        self.reset()\n",
    "        \n",
    "    def step(self,action,printing=True):\n",
    "        \"\"\"\n",
    "        count all time spent\n",
    "        check if any elevators picked up or dropped people off\n",
    "        return new state\n",
    "        \"\"\"\n",
    "        self.move_elevators(action)\n",
    "        self.check_elevators()\n",
    "        reward = self.count_time()\n",
    "        state = self.return_state()\n",
    "        if printing == True:\n",
    "            print(str(self))\n",
    "        return state,reward,self.isdone\n",
    "        \n",
    "    def count_time(self):\n",
    "        outside_mask = np.where(self.floors==1)[0]\n",
    "        num_o_waiting = outside_mask.shape[0]\n",
    "        num_o_waiting *= self.time_weights[0]\n",
    "        # Inside elevators\n",
    "        inside_mask = np.where(self.elevator_destinations == 1)[0]\n",
    "        num_e_waiting = inside_mask.shape[0]\n",
    "        num_e_waiting *= self.time_weights[1]\n",
    "        self.cumulative_time += num_e_waiting + num_o_waiting\n",
    "        return num_e_waiting + num_o_waiting\n",
    "    \n",
    "    def move_elevators(self,action):\n",
    "        self.elevator_targets = action\n",
    "        # Move elevators 1 in the direction of the target\n",
    "        # Get the vertical distance\n",
    "        loc_mask = np.where(self.elevator_locations==1)[1]\n",
    "        tar_mask = np.where(self.elevator_targets==1)[1]\n",
    "        vert_distance = tar_mask - loc_mask\n",
    "        elevators = np.arange(self.N_elevators)\n",
    "        move = np.clip(vert_distance,-1,1)\n",
    "        # print('move elevators',move,loc_mask)\n",
    "        move_mask = loc_mask + move\n",
    "        self.elevator_locations[elevators,loc_mask] = 0 \n",
    "        self.elevator_locations[elevators,move_mask] = 1\n",
    "        \n",
    "    def check_elevators(self):\n",
    "        loc_mask = np.where(self.elevator_locations==1)[1]\n",
    "        tar_mask = np.where(self.elevator_targets==1)[1]\n",
    "        vert_distance = tar_mask - loc_mask\n",
    "        # If any target == the location, drop/pickup\n",
    "        if 0 in vert_distance:\n",
    "            # print('at location')\n",
    "            stopped_mask = np.where(vert_distance == 0)[0]\n",
    "            # print(self.elevator_destinations[stopped_mask])\n",
    "            # print(self.elevator_locations[stopped_mask])\n",
    "            # Check elevator destinations\n",
    "            if 1 in self.elevator_destinations[stopped_mask]:\n",
    "                for el in stopped_mask:\n",
    "                    loc_mask = np.where(self.elevator_locations[el] == 1)\n",
    "                    dest_mask = np.where(self.elevator_destinations[el] == 1)[0]\n",
    "                    if loc_mask in dest_mask:\n",
    "                        # Drop off\n",
    "                        # print('passenger dropped off')\n",
    "                        self.elevator_destinations[el,dest_mask] = 0\n",
    "                        \n",
    "                    \n",
    "            # Check pickups\n",
    "            if 1 in self.floors:\n",
    "                # print('check pickups')\n",
    "                for el in stopped_mask:\n",
    "                    loc_mask = np.where(self.elevator_locations[el] == 1)[0] # elevator floor index\n",
    "                    which_person = np.where(self.floors[:,loc_mask] == 1)[0] # Which person is at that floor (if any)\n",
    "                    if which_person.size > 0:\n",
    "                        # print('passenger picked up')\n",
    "                        self.floors[which_person,:] = 0\n",
    "                        self.elevator_destinations[[el]] += self.destinations[which_person]\n",
    "                        self.destinations[which_person,:] = 0\n",
    "                \n",
    "    def return_state(self):\n",
    "        # Concat together all floor and elevator states\n",
    "        # print('elevator_locations.shape',self.elevator_locations.shape)\n",
    "        # print('elevator_targets',self.elevator_targets.shape)\n",
    "        # print('elevator_destinations',self.elevator_destinations.shape)\n",
    "        machine_input = np.concatenate([self.floors,self.destinations,self.elevator_locations,self.elevator_targets,self.elevator_destinations],axis=0)\n",
    "        # print('return state final',machine_input.shape)\n",
    "        return np.expand_dims(machine_input, axis=0)\n",
    "    \n",
    "    def reset(self,top=True):\n",
    "        # Floors\n",
    "        self.floors = np.zeros((self.N_people,self.N_floors))\n",
    "        self.destinations = np.zeros((self.N_people,self.N_floors))\n",
    "        # Elevators\n",
    "        self.elevator_locations = np.zeros((self.N_elevators,self.N_floors))\n",
    "        self.elevator_targets = np.zeros((self.N_elevators,self.N_floors))\n",
    "        self.elevator_destinations = np.zeros((self.N_elevators,self.N_floors))\n",
    "        self.cumulative_time = 0\n",
    "        # Generate floor destinations\n",
    "        low = self.N_floors-1 if top == True else 0\n",
    "        high = self.N_floors\n",
    "        people_locations = np.random.choice(np.arange(self.N_floors),self.N_people,replace=False)\n",
    "        people_destinations = np.random.randint(low,high,self.N_people)\n",
    "        people = np.arange(self.N_people)\n",
    "        self.floors[people,people_locations] = 1\n",
    "        self.destinations[people,people_destinations] = 1\n",
    "        # Generate elevator locations\n",
    "        low_e = 0\n",
    "        high_e = 1\n",
    "        elevators = np.arange(self.N_elevators)\n",
    "        elevator_locations = np.random.randint(low_e,high_e,self.N_elevators)\n",
    "        self.elevator_locations[elevators,elevator_locations] = 1\n",
    "        return self.return_state()\n",
    "            \n",
    "    @property\n",
    "    def isdone(self):\n",
    "        # Check if finish condition is met`\n",
    "        outside_mask = np.where(self.floors==1)[0]\n",
    "        num_o_waiting = outside_mask.shape[0]\n",
    "        inside_mask = np.where(self.elevator_destinations == 1)[0]\n",
    "        num_e_waiting = inside_mask.shape[0]\n",
    "        if num_e_waiting + num_o_waiting == 0:\n",
    "            finished = 1\n",
    "        else:\n",
    "            finished = 0\n",
    "        return finished\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return (self.N_elevators,self.N_floors)\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return np.concatenate([self.floors,self.destinations,self.elevator_locations,self.elevator_targets,self.elevator_destinations],axis=0).shape\n",
    "    \n",
    "    # def reset(self):\n",
    "    #     # Floors\n",
    "    #     self.floors = np.zeros((self.N_people,self.N_floors))\n",
    "    #     self.destinations = np.zeros((self.N_people,self.N_floors))\n",
    "    #     # Elevators\n",
    "    #     self.elevator_locations = np.zeros((self.N_elevators,self.N_floors))\n",
    "    #     self.elevator_targets = np.zeros((self.N_elevators,self.N_floors))\n",
    "    #     self.elevator_destinations = np.zeros((self.N_elevators,self.N_floors))\n",
    "    #     self.cumulative_time = 0\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '%s(%r)' % (self.__class__,self.__dict__)\n",
    "    \n",
    "    def __str__(self):\n",
    "        elevator_locations = np.where(hotel.elevator_locations==1)[1]\n",
    "        elevator_destinations = np.where(hotel.elevator_destinations==1)[1]\n",
    "        people_locations = np.where(hotel.floors==1)[1]\n",
    "        destinations = np.where(hotel.destinations==1)[1]\n",
    "        return \"elevator_locations {}, elevator_destinations {}, people_locations {}, destinations {}\"\\\n",
    "    .format(elevator_locations,elevator_destinations,people_locations,destinations)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_floors = 5\n",
    "N_elevators = 1\n",
    "N_people = 1\n",
    "\n",
    "hotel = Hotel(N_floors,N_elevators,N_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.Hotel'>({'N_floors': 5, 'N_elevators': 1, 'N_people': 1, 'capacity': None, 'time_weights': [1, 1], 'floors': array([[0., 0., 0., 0., 1.]]), 'destinations': array([[0., 0., 0., 0., 1.]]), 'elevator_locations': array([[1., 0., 0., 0., 0.]]), 'elevator_targets': array([[0., 0., 0., 0., 0.]]), 'elevator_destinations': array([[0., 0., 0., 0., 0.]]), 'cumulative_time': 0})\n",
      "elevator_locations [0], elevator_destinations [], people_locations [4], destinations [4]\n"
     ]
    }
   ],
   "source": [
    "hotel.reset()\n",
    "print(repr(hotel))\n",
    "print(hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 5)"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel.return_state().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move elevators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action [[0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_action = np.zeros((N_elevators,N_floors))\n",
    "test_action[0,4] = 1 # Move up\n",
    "# test_action[0,0] = 1 # Move down\n",
    "# 2 elevators\n",
    "# test_action = np.zeros((N_elevators,N_floors))\n",
    "# test_action[0,4] = 1 # Move up\n",
    "# # test_action[1,2] = 1 # Move up\n",
    "# test_action[0,4] = 1 # Move down\n",
    "# test_action[1,4] = 1 # Move down\n",
    "print('action',test_action)\n",
    "hotel.move_elevators(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'elevator_locations [1], elevator_destinations [], people_locations [1], destinations [4]'"
      ]
     },
     "execution_count": 1058,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check elevators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel.check_elevators()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 1064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotel.reset()\n",
    "str(hotel)\n",
    "hotel.return_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "check_elevators() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1065-38398134f1d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_elevators\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_floors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Move up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhotel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1061-0ed2a549bb3d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, printing)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \"\"\"\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_elevators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_elevators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprinting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: check_elevators() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "test_action = np.zeros((N_elevators,N_floors))\n",
    "test_action[0,4] = 1 # Move up\n",
    "hotel.step(test_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self,agent): \n",
    "        if agent == 'PPO':\n",
    "            self.name = agent\n",
    "            self.gae_lambda=0.95\n",
    "            self.num_agents=1\n",
    "            self.batch_size=32\n",
    "            self.gradient_clip=10\n",
    "            self.SGD_epoch=10\n",
    "            self.epsilon=0.2\n",
    "            self.beta=0.01\n",
    "            self.gamma=0.99\n",
    "            self.lr = 1e-4\n",
    "            self.L2 = 0.01\n",
    "            self.checkpoint_path = 'model_weights/PPO.ckpt'\n",
    "        elif agent == \"ddpg\":\n",
    "            self.seed = 99\n",
    "            self.name = agent\n",
    "            self.num_agents = 2\n",
    "            self.QLR = 0.001\n",
    "            self.ALR = 0.0001\n",
    "            self.gamma = 0.99\n",
    "            self.L2 = 0 # 0.1\n",
    "            self.tau=0.01 # 0.001\n",
    "            self.noise_decay=0.99995\n",
    "            self.gae_lambda = 0.97\n",
    "            self.clip_norm = 10\n",
    "            # Buffer\n",
    "            self.buffer_size = int(1e4)\n",
    "            self.min_buffer_size = int(1e3)\n",
    "            self.batch_size = 256\n",
    "            # Priority Replay\n",
    "            self.ALPHA = 0.6 # 0.7 or 0.6\n",
    "            self.START_BETA = 0.5 # from 0.5-1\n",
    "            self.END_BETA = 1\n",
    "            # distributional\n",
    "            self.N_atoms = 51\n",
    "            self.v_min = -2\n",
    "            self.v_max = 2\n",
    "            self.delta_z = (self.v_min - self.v_max) / (self.N_atoms - 1)\n",
    "            # Tennis\n",
    "            self.action_low=-1.0 \n",
    "            self.action_high=1.0\n",
    "            self.winning_condition = 10\n",
    "            # Training\n",
    "            self.episodes = 100\n",
    "            self.tmax = 2000\n",
    "            self.print_every = 4\n",
    "            self.SGD_epoch = 1\n",
    "            self.checkpoint_path = 'model_weights/ddpg.ckpt'\n",
    "        else:\n",
    "            raise ValueError('Agent not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PyTorch implementation of Actor Critic class for PPO. \n",
    "Combined torso with dual output \n",
    "\n",
    "2 Options:\n",
    "use categorical for each slice\n",
    "use softmax and torch.log for whole\n",
    "\n",
    "Inputs:\n",
    "Active routes\n",
    "Historical routes (for novelty)\n",
    "Current distance (minus stripped routes)\n",
    "Can use the mask in the foward pass to auto restrict which techniques to suggest.\n",
    "\"\"\"\n",
    "\n",
    "class PPO_net(nn.Module):\n",
    "    def __init__(self,nS,nA,seed,indicies,hidden_dims=(256,128)):\n",
    "        super(PPO_net,self).__init__()\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.indicies = indicies\n",
    "        self.hidden_dims = hidden_dims\n",
    "        # TODO implement own batchnorm function\n",
    "        # self.batch = manual_batchnorm()\n",
    "        \n",
    "        # Layers\n",
    "        self.input_layer = nn.Linear(self.nS,hidden_dims[0])\n",
    "        # self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        # self.hidden_batches = nn.ModuleList()\n",
    "        for i in range(1,len(hidden_dims)):\n",
    "            # hidden_batch = nn.BatchNorm1d(hidden_dims[i-1])\n",
    "            hidden_layer = nn.Linear(hidden_dims[i-1],hidden_dims[i])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "            # self.hidden_batches.append(hidden_batch)\n",
    "\n",
    "        \n",
    "        # Action outputs, we softmax over the various classes for 1 per class (can change this for multi class)\n",
    "        self.actor_output = nn.Linear(hidden_dims[-1],nA)\n",
    "        self.critic_output = nn.Linear(hidden_dims[-1],1)\n",
    "\n",
    "    def forward(self,state):\n",
    "        \"\"\"\n",
    "        Expects state to be a torch tensor\n",
    "\n",
    "        Outputs Action,log_prob, entropy and (state,action) value\n",
    "        \"\"\"\n",
    "        assert isinstance(state,torch.Tensor)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        for i,hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = F.relu(hidden_layer(x))\n",
    "            \n",
    "        # state -> action\n",
    "        action = self.actor_output(x)\n",
    "        a_prob = F.softmax(action,dim=0)\n",
    "        log_prob = torch.log(action)\n",
    "        # Critic state value\n",
    "        v = self.critic_output(x)\n",
    "        return a_prob, log_prob, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(object):\n",
    "    def __init__(self,nS,nA,config):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.seed = config.seed\n",
    "        self.lr = config.lr\n",
    "\n",
    "        self.gradient_clip = config.gradient_clip\n",
    "        self.gamma = config.gamma\n",
    "        self.gae_lambda = config.gae_lambda\n",
    "        self.start_epsilon = self.epsilon = config.epsilon\n",
    "        self.start_beta = self.beta = config.beta\n",
    "        self.SGD_epoch = config.SGD_epoch\n",
    "        self.batch_size = config.batch_size\n",
    "\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        self.policy = PPO_net(nS,nA,self.seed).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.policy.parameters(),lr=1e-4,weight_decay=config.L2)\n",
    "\n",
    "    def load_weights(self,path):\n",
    "        self.policy.load_state_dict(torch.load(path))\n",
    "        self.policy.eval()\n",
    "\n",
    "    def save_weights(self,path):\n",
    "        directory = os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        torch.save(self.policy.state_dict(), path)\n",
    "\n",
    "    def reset_hyperparams(self):\n",
    "        self.discount = self.start_discount\n",
    "        self.epsilon = self.start_epsilon\n",
    "        self.beta = self.start_beta\n",
    "\n",
    "    def step_hyperparams(self):\n",
    "        self.epsilon *= 0.999\n",
    "        self.beta *= 0.995\n",
    "\n",
    "    def step(self,trajectory):\n",
    "        N = len(trajectory)\n",
    "        # decay beta,epsilon\n",
    "        self.step_hyperparams()\n",
    "        \n",
    "        states,actions,values,log_probs,rewards,next_states = self.unwrap(trajectory)\n",
    "        # Normalize the rewards\n",
    "        # rewards = (rewards - np.mean(rewards)) / np.std(rewards)\n",
    "        last_value = self.policy(self.tensor(next_states[-1]))[-1].unsqueeze(1).cpu().detach().numpy()\n",
    "        values = np.vstack(values + [last_value])\n",
    "        advs,TD_errors = self.gae(values,rewards)\n",
    "        returns = self.n_step_returns(rewards)\n",
    "        \n",
    "        states,actions,log_probs,returns,advs,TD_errors,rewards = self.bulk_tensor(states,actions,log_probs,returns,advs,TD_errors,rewards)\n",
    "        for indicies in self.minibatch(N):\n",
    "            states_b = states[indicies]\n",
    "            actions_b = actions[indicies]\n",
    "            log_probs_b = log_probs[indicies]\n",
    "            advs_b = advs[indicies]\n",
    "            returns_b = returns[indicies]\n",
    "            TD_errors_b = TD_errors[indicies]\n",
    "            rewards_b = rewards[indicies]\n",
    "            \n",
    "            self.learn(states_b,actions_b,log_probs_b,advs_b,returns_b,TD_errors_b,rewards_b)\n",
    "        \n",
    "    def bulk_tensor(self,states,actions,log_probs,returns,advs,TD_errors,rewards):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        log_probs = torch.from_numpy(log_probs).float().to(self.device)\n",
    "        advs = torch.from_numpy(advs).float().to(self.device)\n",
    "        TD_errors = torch.from_numpy(TD_errors).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        # TO fix negative stride\n",
    "        returns = torch.flip(torch.from_numpy(np.flip(returns,axis=0).copy()),dims=(0,)).float().to(self.device)\n",
    "        return states,actions,log_probs,returns,advs,TD_errors,rewards\n",
    "\n",
    "    def unwrap(self,trajectory):\n",
    "        # states = torch.from_numpy(np.vstack([e.state for e in trajectory])).float().to(self.device)\n",
    "        # values = torch.from_numpy(np.vstack([e.value for e in trajectory])).float().to(self.device)\n",
    "        # log_probs = torch.from_numpy(np.vstack([e.log_prob for e in trajectory])).float().to(self.device)\n",
    "        # rewards = torch.from_numpy(np.vstack([e.reward for e in trajectory])).float().to(self.device)\n",
    "        # next_states = torch.from_numpy(np.vstack([e.next_state for e in trajectory])).float().to(self.device)\n",
    "        \n",
    "        states = np.vstack([e.state for e in trajectory])\n",
    "        values = [e.value for e in trajectory]\n",
    "        actions = np.vstack([e.action for e in trajectory])\n",
    "        log_probs = np.vstack([e.log_prob for e in trajectory])\n",
    "        rewards = np.vstack([e.reward for e in trajectory])\n",
    "        next_states = np.vstack([e.next_state for e in trajectory])\n",
    "        \n",
    "        return states,actions,values,log_probs,rewards,next_states\n",
    "\n",
    "    def gae(self,values,rewards):\n",
    "        \"\"\"\n",
    "        Generalized Advantage Estimate\n",
    "\n",
    "        1d arrays\n",
    "        \"\"\"\n",
    "        N = rewards.shape[0]\n",
    "        combined = self.gamma*self.gae_lambda\n",
    "        vs = values[:-1]\n",
    "        next_vs = values[1:]\n",
    "        TD_errors = rewards + next_vs - vs\n",
    "        advs = np.zeros(rewards.shape)\n",
    "        for index in reversed(range(len(TD_errors))):\n",
    "            discounts = combined**np.arange(0,N-index)\n",
    "            advs[index] = np.sum(TD_errors[index:] * discounts)\n",
    "        return advs,TD_errors\n",
    "\n",
    "    def n_step_returns(self,rewards):\n",
    "        N = rewards.shape[0]\n",
    "        discounts = self.gamma**np.arange(N)\n",
    "        discounted_returns = rewards * discounts.reshape(N,1)\n",
    "        returns = discounted_returns.cumsum()[::-1].reshape(N,1)\n",
    "        return returns\n",
    "\n",
    "    def learn(self,states,actions,log_probs,advs,returns,TD_errors,rewards):\n",
    "        \"\"\"\n",
    "        Learn on batches from trajectory\n",
    "        \"\"\"\n",
    "        \n",
    "        new_actions,new_log_probs,new_values = self.policy(states)\n",
    "        \n",
    "        # ratio = (new_actions - actions)**2\n",
    "        # ratio = new_actions / actions\n",
    "        ratio = (new_log_probs - log_probs)**2\n",
    "\n",
    "        # ratio = new_log_probs / log_probs\n",
    "        clip = torch.clamp(ratio,1-self.epsilon,1+self.epsilon)\n",
    "        clipped_surrogate = torch.min(clip*advs,ratio*advs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        actor_loss = clipped_surrogate.mean()\n",
    "        critic_loss = F.smooth_l1_loss(rewards,new_values)\n",
    "        loss = (actor_loss + critic_loss)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(),self.gradient_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self,state):\n",
    "        state = self.tensor(state)\n",
    "        route,log_prob,value = self.policy(state)\n",
    "        return route.detach().cpu().numpy(),log_prob.detach().cpu().numpy(),value.detach().cpu().numpy()\n",
    "\n",
    "    def minibatch(self,N):\n",
    "        index = np.arange(N)\n",
    "        for _ in range(self.SGD_epoch):\n",
    "            indicies = np.random.choice(index,self.batch_size)\n",
    "            yield indicies\n",
    "\n",
    "    def tensor(self,state):\n",
    "        return torch.tensor(state).float().to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PER - Priority Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Tree.\n",
    "3 tiered tree structure containing\n",
    "Root node (Object. sum of all lower values)\n",
    "Intermediate Node (Object. Root as parent, sums a given slice of the priority array)\n",
    "Priority Array (Array of priorities, length buffer_size)\n",
    "\n",
    "The number of Intermediate nodes is calculated by the buffer_size / batch_size.\n",
    "\n",
    "I_episode: current episode of training\n",
    "\n",
    "Index: is calculated by i_episode % buffer_size. This loops the index after exceeding the buffer_size.\n",
    "\n",
    "Indicies: (List) of memory/priority entries\n",
    "\n",
    "intermediate_dict: maps index to intermediate node. Since each Intermediate node is responsible \n",
    "for a given slice of the priority array, given a particular index, it will return the Intermediate node\n",
    "'responsible' for that index.\n",
    "\n",
    "## Functions:\n",
    "\n",
    "Add:\n",
    "Calculates the priority of each TD error -> (abs(TD_error)+epsilon)**alpha\n",
    "Stores the priority in the Priority_array.\n",
    "Updates the sum_tree with the new priority\n",
    "\n",
    "Update_Priorities:\n",
    "Updates the index with the latest priority of that sample. As priorities can change over training\n",
    "for a particular experience\n",
    "\n",
    "Sample:\n",
    "Splits the current priority_array based on the number of entries, by the batch_size.\n",
    "Returns the indicies of those samples and the priorities.\n",
    "\n",
    "Propogate:\n",
    "Propogates the new priority value up through the tree\n",
    "\"\"\"\n",
    "\n",
    "class PriorityTree(object):\n",
    "    def __init__(self,buffer_size,batch_size,alpha,epsilon):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_indicies = np.arange(0,self.batch_size)\n",
    "\n",
    "        self.num_intermediate_nodes = math.ceil(buffer_size / batch_size)\n",
    "        self.current_intermediate_node = 0\n",
    "        self.root = Node(None)\n",
    "        self.intermediate_nodes = [Intermediate(self.root,batch_size*x,batch_size*(x+1)) for x in range(self.num_intermediate_nodes)]\n",
    "        self.priority_array = np.zeros(buffer_size)\n",
    "        self.intermediate_dict = {}\n",
    "        for index,node in enumerate(self.intermediate_nodes):\n",
    "            for key in range((batch_size*(index+1))-batch_size,batch_size*(index+1)):\n",
    "                self.intermediate_dict[key] = node\n",
    "        print('Priority Tree: Batch Size {} Buffer size {} Number of intermediate Nodes {}'.format(batch_size,buffer_size,self.num_intermediate_nodes))\n",
    "        \n",
    "    def add(self,TD_error,index):\n",
    "        priority = (abs(TD_error)+self.epsilon)**self.alpha\n",
    "        self.priority_array[index] = priority\n",
    "        # Update sum\n",
    "        propogate(self.intermediate_dict[index],self.priority_array)\n",
    "    \n",
    "    def sample(self,index,limit):\n",
    "        # Sample one experience uniformly from each slice of the priorities\n",
    "        # if index >= self.buffer_size:\n",
    "        #     indicies = [random.sample(list(range(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes)),1)[0] for sample in range(self.batch_size)]\n",
    "        #     # indicies = np.random.sample(np.arange(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes))\n",
    "        # else:\n",
    "        spacing = np.linspace(0,limit-self.batch_size,self.batch_size,dtype=np.int)\n",
    "        random_indicies = np.random.choice(self.batch_indicies,size=self.batch_size)\n",
    "        indicies = random_indicies + spacing\n",
    "\n",
    "\n",
    "        # interval = int(index / self.batch_size)\n",
    "        # indicies = [random.sample(list(range(sample*interval,(sample+1)*interval)),1)[0] for sample in range(self.batch_size)]\n",
    "#         print('indicies',indicies)\n",
    "        priorities = self.priority_array[indicies]\n",
    "        return priorities,indicies\n",
    "    \n",
    "    def update_priorities(self,TD_errors,indicies):\n",
    "#         print('TD_errors',TD_errors)\n",
    "#         print('TD_errors shape',TD_errors.shape)\n",
    "        priorities = (np.abs(TD_errors)+self.epsilon)**self.alpha\n",
    "#         print('priorities shape',priorities.shape)\n",
    "#         print('indicies shape',len(indicies))\n",
    "#         print('self.priority_array shape',self.priority_array.shape)\n",
    "        self.priority_array[indicies] = priorities\n",
    "        # Update sum\n",
    "        nodes = [self.intermediate_dict[index] for index in indicies] \n",
    "        intermediate_nodes = set(nodes)\n",
    "        [propogate(node,self.priority_array) for node in intermediate_nodes]\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self,parent):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "            \n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def sum_children(self):\n",
    "        return sum([child.value for child in self.children])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "class Intermediate(Node):\n",
    "    def __init__(self,parent,start,end):\n",
    "        self.parent = parent\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.value = 0\n",
    "        parent.add_child(self)\n",
    "    \n",
    "    def sum_leafs(self,arr):\n",
    "        return np.sum(arr[self.start:self.end])\n",
    "\n",
    "def propogate(node,arr):\n",
    "    if node.parent != None:\n",
    "        node.value = node.sum_leafs(arr)\n",
    "        propogate(node.parent,arr)\n",
    "    else:\n",
    "        node.value = node.sum_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Buffer HyperParameters\n",
    "alpha(priority or w) dictates how biased the sampling should be towards the TD error. 0 < a < 1\n",
    "beta(IS) informs the importance of the sample update\n",
    "\n",
    "The paper uses a sum tree to calculate the priority sum in O(log n) time. As such, i've implemented my own version\n",
    "of the sum_tree which i call priority tree.\n",
    "\n",
    "We're increasing beta(IS) from 0.5 to 1 over time\n",
    "alpha(priority) we're holding constant at 0.5\n",
    "\n",
    "Lets reimagine the buffer so that you can add a sample and then when you sample from the replay, \n",
    "you THEN calculate the TD error the priorities and importances. Then update the priorities based on the new distance.\n",
    "\"\"\"\n",
    "\n",
    "class PriorityReplayBuffer(object):\n",
    "    def __init__(self,buffer_size,batch_size,seed,alpha=0.5,beta=0.5,beta_end=1,beta_duration=1e+5,epsilon=7e-5,device=None):\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_duration = beta_duration\n",
    "        self.beta_increment = (beta_end - beta) / beta_duration\n",
    "        self.max_w = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.TD_sum = 0\n",
    "        self.index = 0\n",
    "        if device == None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.experience = namedtuple('experience',field_names=['state','action','reward','next_state','done'])\n",
    "        self.sum_tree = PriorityTree(buffer_size,batch_size,alpha,epsilon)\n",
    "        self.memory = {}\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done,TD_error):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        # add memory to memory and add corresponding priority to the priority tree\n",
    "        self.memory[self.index] = e\n",
    "        self.sum_tree.add(TD_error,self.index)\n",
    "        self.index = (self.index + 1) % self.buffer_size \n",
    "\n",
    "    def sample(self):\n",
    "        # We times the error by these weights for the updates\n",
    "        # Super inefficient to sum everytime. We could implement the tree sum structure. \n",
    "        # Or we could sum once on the first sample and then keep track of what we add and lose from the buffer.\n",
    "        # priority^a over the sum of the priorities^a = likelyhood of the given choice\n",
    "        # Anneal beta\n",
    "        self.update_beta()\n",
    "        # Get the samples and indicies\n",
    "        priorities,indicies = self.sum_tree.sample(self.index,len(self))\n",
    "        # Normalize with the sum\n",
    "        norm_priorities = priorities / self.sum_tree.root.value\n",
    "        samples = [self.memory[index] for index in indicies]\n",
    "        # Importance weights\n",
    "        importances = [(priority * self.buffer_size)**-self.beta for priority in norm_priorities]\n",
    "        self.max_w = max(self.max_w,max(importances))\n",
    "        # Normalize importance weights\n",
    "#         print('importances',importances)\n",
    "#         print('self.max_w',self.max_w)\n",
    "        norm_importances = [importance / self.max_w for importance in importances]\n",
    "#         print('norm_importances',norm_importances)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        states = torch.stack(states).float().to(self.device)\n",
    "        actions = torch.stack(actions).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
    "        next_states = torch.stack(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack(dones)).float().to(self.device)\n",
    "\n",
    "        # states = torch.from_numpy(np.vstack([e.state for e in samples if e is not None])).float().to(self.device)\n",
    "        # actions = torch.from_numpy(np.vstack([e.action for e in samples if e is not None])).float().to(self.device)\n",
    "        # rewards = torch.from_numpy(np.vstack([e.reward for e in samples if e is not None])).float().to(self.device)\n",
    "        # next_states = torch.from_numpy(np.vstack([e.next_state for e in samples if e is not None])).float().to(self.device)\n",
    "        # dones = torch.from_numpy(np.vstack([e.done for e in samples if e is not None]).astype(int)).float().to(self.device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones),indicies,norm_importances\n",
    "\n",
    "    # TODO\n",
    "    def update_priorities(self):\n",
    "        pass\n",
    "    \n",
    "    def update_beta(self):\n",
    "        self.beta += self.beta_increment\n",
    "        self.beta = min(self.beta,self.beta_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUnoise(object):\n",
    "    def __init__(self,size,seed,mu=0,theta=0.15,sigma=0.2):\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x+dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def hard_update(source,target):\n",
    "    for target_param,param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(256,128)):\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_space = nA\n",
    "        self.state_space = nS\n",
    "        self.nS = [1]\n",
    "        self.nA = reduce((lambda x,y: x*y),nA)\n",
    "        [self.nS.append(x) for x in nS]\n",
    "        self.nS = tuple(self.nS)\n",
    "#         print('conv critic',hidden_dims[0],nA,nS)\n",
    "        self.input_layer = nn.Conv1d(nS[0],hidden_dims[0],kernel_size=1)\n",
    "        self.action_input = nn.Conv1d(nA[0],hidden_dims[0],kernel_size=1)\n",
    "#         self.input_layer = nn.Linear(64,hidden_dims[0])\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(nS[1]+nA[1],hidden_dims[0]))\n",
    "        for i in range(0,len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        # self.fc1 = nn.Linear(hidden_dims[0]+nA,hidden_dims[1])\n",
    "        # self.fc1_bn = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1]*hidden_dims[-2],1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.input_layer.weight.data.uniform_(*hidden_init(self.input_layer))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.weight.data.uniform_(*hidden_init(hidden_layer))\n",
    "        self.output_layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,obs,action):\n",
    "        N = obs.size()[0]\n",
    "        # With batchnorm\n",
    "#         obs = obs.view(-1)\n",
    "#         action = action.view(-1)\n",
    "        # xs = self.input_bn(F.relu(self.input_layer(state)))\n",
    "        # x = torch.cat((xs,action),dim=1)\n",
    "        # x = self.fc1_bn(F.relu(self.fc1(x)))\n",
    "        # transpose obs and action\n",
    "        action = F.relu(self.action_input(action))\n",
    "        xs = F.relu(self.input_layer(obs))\n",
    "        x = torch.cat((xs, action), dim=-1)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        return self.output_layer(x.view(N,-1))\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(256,128)):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.action_space = nA\n",
    "        self.state_space = nS\n",
    "        self.nS = [1]\n",
    "        self.nA = reduce((lambda x,y: x*y),nA)\n",
    "        [self.nS.append(x) for x in nS]\n",
    "        self.nS = tuple(self.nS)\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.input_layer = nn.Conv1d(nS[0],hidden_dims[0],kernel_size=1)\n",
    "#         self.input_layer = nn.Linear(64,hidden_dims[0])\n",
    "        self.fc1 = nn.Linear(nS[0],hidden_dims[0])\n",
    "        self.fc2 = nn.Linear(hidden_dims[0],hidden_dims[1])\n",
    "        self.output_layer = nn.Linear(hidden_dims[0]*hidden_dims[1],self.nA)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.input_layer.weight.data.uniform_(*hidden_init(self.input_layer))\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.output_layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        x = state\n",
    "        if not isinstance(state,torch.Tensor):\n",
    "            x = torch.tensor(x,dtype=torch.float32,device = self.device) #device = self.device,\n",
    "            x = x.unsqueeze(0)\n",
    "#         x = x.view(-1)\n",
    "#         x = F.relu(self.conv1(x))\n",
    "        N = state.size()[0]\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = x.view(N,-1)\n",
    "        x = self.output_layer(x).view(N,self.action_space[0],self.action_space[-1])\n",
    "        return F.softmax(x,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, nS, nA, actor, critic, config):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.action_low = config.action_low\n",
    "        self.action_high = config.action_high\n",
    "        self.seed = config.seed\n",
    "\n",
    "        self.clip_norm = config.clip_norm\n",
    "        self.tau = config.tau\n",
    "        self.gamma = config.gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.L2 = config.L2\n",
    "        self.SGD_epoch = config.SGD_epoch\n",
    "        # noise\n",
    "        self.noise = OUnoise(nA,config.seed)\n",
    "        self.noise_scale = 1.0\n",
    "        self.noise_decay = config.noise_decay\n",
    "\n",
    "        # Priority Replay Buffer\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer_size = config.buffer_size\n",
    "        self.alpha = config.ALPHA\n",
    "        self.beta = self.start_beta = config.START_BETA\n",
    "        self.end_beta = config.END_BETA\n",
    "\n",
    "        # actors networks\n",
    "        self.actor = actor(self.seed,nS, nA).to(self.device)\n",
    "        self.actor_target = actor(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # Param noise\n",
    "        # self.param_noise = AdaptiveParamNoise()\n",
    "        # self.actor_perturbed = actor(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # critic networks\n",
    "        self.critic = critic(self.seed,nS, nA).to(self.device)\n",
    "        self.critic_target = critic(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # Copy the weights from local to target\n",
    "        hard_update(self.critic,self.critic_target)\n",
    "        hard_update(self.actor,self.actor_target)\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=1e-4, weight_decay=self.L2)\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3, weight_decay=self.L2)\n",
    "\n",
    "        # replay buffer\n",
    "        self.PER = PriorityReplayBuffer(self.buffer_size, self.batch_size,self.seed,alpha=self.alpha,device=self.device)\n",
    "\n",
    "        # reset agent for training\n",
    "        self.reset_episode()\n",
    "        self.it = 0\n",
    "\n",
    "    def save_weights(self,path):\n",
    "        params = {}\n",
    "        params['actor'] = self.actor.state_dict()\n",
    "        params['critic'] = self.critic.state_dict()\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def load_weights(self,path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.actor_target.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic'])\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def ddpg_distance_metric(actions1,actions2):\n",
    "        \"\"\"\n",
    "        Computes distance between actions taken by two different policies\n",
    "        Expects numpy arrays\n",
    "        \"\"\"\n",
    "        diff = actions1-actions2\n",
    "        mean_diff = np.mean(np.square(diff),axis=0)\n",
    "        dist = sqrt(np.mean(mean_diff))\n",
    "        return dist\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(self.tensor(state)).cpu().numpy()\n",
    "        action += self.noise.sample() * self.noise_scale\n",
    "        # renorm\n",
    "        if np.min(action) < 0:\n",
    "            action += np.absolute(np.min(action)) * 2\n",
    "        action  = action / np.sum(action)\n",
    "        self.noise_scale = max(self.noise_scale * self.noise_decay, 0.01)\n",
    "        self.actor.train()\n",
    "        return action\n",
    "\n",
    "    def act_perturbed(self,state):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_perturbed(self.tensor(state)).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def perturbed_update(self):\n",
    "        hard_update(self.actor,self.actor_perturbed)\n",
    "        params = self.actor_perturbed.state_dict()\n",
    "        for name in params:\n",
    "            if 'ln' in name:\n",
    "                pass\n",
    "            param = params[name]\n",
    "            random = torch.randn(param.shape).to(self.device)\n",
    "            param += random * self.param_noise.current_stddev\n",
    "            \n",
    "\n",
    "    def evaluate(self,state):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(self.tensor(state)).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def step(self, obs, actions, rewards, next_obs, dones):\n",
    "        # cast as torch tensors\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(self.device)\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        # Calc TD error\n",
    "        next_action = self.actor(next_obs)\n",
    "        next_value = self.critic_target(next_obs,next_action)\n",
    "        target = rewards + self.gamma * next_value * dones\n",
    "        local = self.critic(obs,actions)\n",
    "        TD_error = (target - local).squeeze(0)\n",
    "        self.PER.add(obs, actions, rewards, next_obs, dones, TD_error)\n",
    "        for _ in range(self.SGD_epoch):\n",
    "            samples,indicies,importances = self.PER.sample()\n",
    "            self.learn(samples,indicies,importances)\n",
    "\n",
    "    def add_replay_warmup(self,obs,actions,rewards,next_obs,dones):\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(self.device)\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        # Calculate TD_error\n",
    "        next_action = self.actor(next_obs)\n",
    "        next_value = self.critic_target(next_obs,next_action)\n",
    "        target = rewards + self.gamma * next_value * dones\n",
    "        local = self.critic(obs,actions)\n",
    "        TD_error = (target - local).squeeze(0)\n",
    "        self.PER.add(obs,actions,np.max(rewards),next_obs,np.max(dones),TD_error)\n",
    "\n",
    "    def learn(self,samples,indicies,importances):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = samples\n",
    "        next_states = next_states.squeeze(1)\n",
    "        states = states.squeeze(1)\n",
    "        actions = actions.squeeze(1)\n",
    "        # print('actions shape',actions.shape)\n",
    "        # print('next_states shape',next_states.shape)\n",
    "        with torch.no_grad():\n",
    "              target_actions = self.actor_target(next_states)\n",
    "        # print('target_actions shape',target_actions.shape)\n",
    "        next_values = self.critic_target(next_states,target_actions)\n",
    "        y_target = rewards + self.gamma * next_values * (1-dones)\n",
    "        y_current = self.critic(states, actions)\n",
    "        TD_error = y_current - y_target\n",
    "        # update critic\n",
    "        critic_loss = ((torch.tensor(importances).to(self.device)*TD_error)**2).mean()\n",
    "        self.critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(),self.clip_norm)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # update actor\n",
    "        local_actions = self.actor(states)\n",
    "        actor_loss = -self.critic(states, local_actions).mean()\n",
    "        self.actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(),self.clip_norm)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # Update PER\n",
    "        TD_errors = TD_error.squeeze(1).detach().cpu().numpy()\n",
    "        self.PER.sum_tree.update_priorities(TD_errors,indicies)\n",
    "\n",
    "        # soft update networks\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target network\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "\n",
    "    def tensor(self, x):\n",
    "        return torch.from_numpy(x).float().to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DDPG agent on elevators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Actions shape: (N,E,F) Where N = Batch size, E = num elevators, F = num floors\n",
    "\"\"\"\n",
    "\n",
    "def into_action(probs):\n",
    "    probs = probs.squeeze(0)\n",
    "#     print(probs.shape)\n",
    "    actions = np.zeros(probs.shape)\n",
    "    choices = np.arange(probs.shape[-1])\n",
    "#     print('choices',choices)\n",
    "    for elevator in range(actions.shape[0]):\n",
    "        action_mask = np.random.choice(choices,p=probs[elevator,:])\n",
    "#         print('action_mask',action_mask)\n",
    "        actions[elevator,action_mask] = 1\n",
    "#         print('actions',actions)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_replay_buffer(env, agent, min_buffer_size):\n",
    "    printing = False\n",
    "    obs = env.reset()\n",
    "    while len(agent.PER) < min_buffer_size:\n",
    "        # Random actions between 1 and -1\n",
    "        probs = agent.act(obs)\n",
    "#         print('agent probs',actions)\n",
    "        actions = into_action(probs)\n",
    "#         print('agent action',actions)\n",
    "        \n",
    "        next_obs,rewards,dones = env.step(actions,printing)\n",
    "        # reshape\n",
    "        agent.add_replay_warmup(obs,probs,rewards,next_obs,dones)\n",
    "        # Store experience\n",
    "        if dones:\n",
    "            obs = env.reset()\n",
    "        obs = next_obs\n",
    "    print('finished replay warm up')\n",
    "\n",
    "def train_ddpg(env, agent, config):\n",
    "    printing = False\n",
    "    episodes,tmax = config.episodes,config.tmax\n",
    "    tic = time.time()\n",
    "    means = []\n",
    "    mins = []\n",
    "    maxes = []\n",
    "    stds = []\n",
    "    mean_steps = []\n",
    "    steps = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    for e in range(1,episodes):\n",
    "        agent.reset_episode()\n",
    "        episode_scores = []\n",
    "        obs = env.reset()\n",
    "        for t in range(tmax):\n",
    "            probs = agent.act(obs)\n",
    "            actions = into_action(probs)\n",
    "            next_obs,rewards,dones = env.step(actions,printing)\n",
    "            # Step agent with reshaped observations\n",
    "            agent.step(obs, probs, rewards, next_obs, dones)\n",
    "            # Score tracking\n",
    "            episode_scores.append(rewards)\n",
    "            obs = next_obs\n",
    "            if dones:\n",
    "#                 print('done',t)\n",
    "                steps.append(int(t))\n",
    "                break\n",
    "            \n",
    "        scores_window.append(np.sum(episode_scores))\n",
    "        means.append(np.mean(scores_window))\n",
    "        mins.append(min(scores_window))\n",
    "        maxes.append(max(scores_window))\n",
    "        mean_steps.append(np.mean(steps))\n",
    "        stds.append(np.std(scores_window))\n",
    "        if e % 3 == 0:\n",
    "            toc = time.time()\n",
    "            r_mean = np.mean(scores_window)\n",
    "            r_max = max(scores_window)\n",
    "            r_min = min(scores_window)\n",
    "            r_std = np.std(scores_window)\n",
    "#             plot(means,maxes,mins,mean_steps,num_agents=2,name=config.name,game='Tennis')\n",
    "            print(\"\\rEpisode: {} out of {}, Steps {}, Mean steps {:.2f}, Noise {:.2f}, Rewards: mean {:.2f}, min {:.2f}, max {:.2f}, std {:.2f}, Elapsed {:.2f}\".format(e,episodes,np.sum(steps),np.mean(steps),agent.noise_scale,r_mean,r_min,r_max,r_std,(toc-tic)/60))\n",
    "#         if np.mean(scores_window) > config.winning_condition:\n",
    "#             print('Env solved!')\n",
    "            # save scores\n",
    "#             pickle.dump([means,maxes,mins,mean_steps], open(str(config.name)+'_scores.p', 'wb'))\n",
    "            # save policy\n",
    "#             agent.save_weights(config.checkpoint_path)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "def plot(name,means,stds):\n",
    "     \n",
    "    length = len(means)\n",
    "    means = np.array(means)\n",
    "    stds = np.array(stds)\n",
    "\n",
    "    mins = means-stds\n",
    "    maxes = means+stds\n",
    "\n",
    "    xline = np.linspace(0,length,length*10)\n",
    "    xfit = np.arange(length)\n",
    "    \n",
    "    spl = make_interp_spline(xfit,means,k=3)\n",
    "    spl2 = make_interp_spline(xfit,mins,k=3)\n",
    "    spl3 = make_interp_spline(xfit,maxes,k=3)\n",
    "\n",
    "    means_smooth = spl(xline)\n",
    "    mins_smooth = spl2(xline)\n",
    "    maxes_smooth = spl3(xline)\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "\n",
    "    title = str(name)+\" performance on Elevators\"\n",
    "    x_label = \"Number of Episodes\"\n",
    "    y_label = \"Score\"\n",
    "\n",
    "    ax.plot(xline, means_smooth, lw=1, color= '#539caf', alpha = 1, label= 'mean')\n",
    "    ax.fill_between(xline,mins_smooth,maxes_smooth,color='orange',alpha = 0.4, label = 'Min/Max')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    # plt.show()\n",
    "    plt.savefig(str(name)+'_performance.png',bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(algo):\n",
    "    N_floors = 5\n",
    "    N_elevators = 1\n",
    "    N_people = 1\n",
    "    # Load the ENV\n",
    "    env = Hotel(N_floors,N_elevators,N_people)\n",
    "\n",
    "    # size of each action\n",
    "    action_space = env.action_space\n",
    "\n",
    "    # examine the state space \n",
    "    state_space = env.state_space\n",
    "    print('Size of each action: {}, Size of the state space {}'.format(action_space,state_space))\n",
    "    \n",
    "    ddpg_config = Config(algo)\n",
    "\n",
    "    agent = DDPG(state_space, action_space,Actor,Critic,ddpg_config)\n",
    "    # Fill buffer with random actions up to min buffer size\n",
    "    seed_replay_buffer(env, agent, ddpg_config.min_buffer_size)\n",
    "    # Train agent\n",
    "    train_ddpg(env, agent, ddpg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: (1, 5), Size of the state space (5, 5)\n",
      "Priority Tree: Batch Size 256 Buffer size 10000 Number of intermediate Nodes 40\n",
      "finished replay warm up\n",
      "Episode: 3 out of 100, Steps 110, Mean steps 36.67, Noise 0.95, Rewards: mean 36.67, min 22.00, max 52.00, std 12.26, Elapsed 2.66\n"
     ]
    }
   ],
   "source": [
    "main('ddpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
