{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import json\n",
    "from os import path, walk, makedirs\n",
    "from sys import exit, stderr\n",
    "\n",
    "from cv2 import fillPoly, imwrite\n",
    "from shapely import wkt\n",
    "from shapely.geometry import mapping, Polygon, box\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "import imantics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/shuza/Code/xview2-baseline/data'\n",
    "\n",
    "masks_path = '/home/shuza/Code/xview2-baseline/data/guatemala-volcano/masks'\n",
    "imgs_path = '/home/shuza/Code/xview2-baseline/data/guatemala-volcano/images'\n",
    "\n",
    "ind_mask = '/home/shuza/Code/xview2-baseline/data/guatemala-volcano/masks/guatemala-volcano_00000000_pre_disaster.png'\n",
    "ind_pre = '/home/shuza/Code/xview2-baseline/data/guatemala-volcano/images/guatemala-volcano_00000000_pre_disaster.png'\n",
    "ind_post = '/home/shuza/Code/xview2-baseline/data/guatemala-volcano/images/guatemala-volcano_00000000_post_disaster.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals\n",
    "\n",
    "load an image, load the corresponding mask, mask out the image.\n",
    "\n",
    "- *Semantic map*\n",
    "- Train GAN on mask -> image pre\n",
    "- *Image Repair*\n",
    "- Train GAN on masked image -> image pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = Image.open(ind_pre)\n",
    "test_mask = Image.open(ind_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_arr = np.asarray(test_mask)\n",
    "img_arr = np.asarray(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask_arr.shape)\n",
    "print(img_arr.shape)\n",
    "print(test_img.mode)\n",
    "print(test_mask.mode)\n",
    "print(test_img.format)\n",
    "print(test_img.size)\n",
    "print(test_img.palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img.thumbnail((400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = test_img.convert('L')\n",
    "mask = test_mask.convert('L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "PIL.Image.composite(mask,img,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paste one image onto another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_img = test_img.copy()\n",
    "# position = ((image_copy.width - logo.width), (image_copy.height - logo.height))\n",
    "copy_img.paste(test_mask,(1024,1024),test_mask)\n",
    "imshow(copy_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_90 = test_img.rotate(90)\n",
    "imshow(image_90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_flip = test_img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "imshow(image_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = (150,200,600,600)\n",
    "cropped_image = test_img.crop(box)\n",
    "imshow(cropped_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto mask image from data\n",
    "\n",
    "## Create dataset for image repair\n",
    "\n",
    "Goes from masked image -> repairs buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path\n",
    "imgs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = os.listdir(imgs_path)\n",
    "mask_list = os.listdir(masks_path)\n",
    "mask_dict = {k:v for v,k in enumerate(mask_list)}\n",
    "img_dict = {k:v for v,k in enumerate(img_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_imgs = []\n",
    "src_imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = os.listdir('/home/shuza/Code/xview2-baseline/data/hurricane-florence')\n",
    "test.index('masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes parent folder with subfolder names images and masks,\n",
    "# Takes target folder, makes the src and target image folders\n",
    "# Put images through all 3 rotations and all 4 flipped rotations\n",
    "\n",
    "def create_dataset(dir_params):\n",
    "    # Go through images and create a dictionary to index all the files.\n",
    "#     parent_list = os.listdir(dir_params['src_folder'])\n",
    "    print(dir_params)\n",
    "    \n",
    "    if not path.isdir(dir_params['targets_folder']):\n",
    "        makedirs(dir_params['targets_folder'])\n",
    "        \n",
    "    if not path.isdir(dir_params['target_inputs']):\n",
    "        makedirs(dir_params['target_inputs'])\n",
    "    folder = 'mexico-earthquake'\n",
    "#     for folder in parent_list:\n",
    "    folder_path = os.path.join(dir_params['src_folder'],folder)\n",
    "    folder_items = os.listdir(folder_path)\n",
    "    mask_folder = os.path.join(folder_path,'masks')\n",
    "    images_folder = os.path.join(folder_path,'images')\n",
    "    img_list = os.listdir(images_folder)\n",
    "    mask_list = os.listdir(mask_folder)\n",
    "    img_dict = {k:v for v,k in enumerate(img_list)}\n",
    "    for mask_file in mask_list:\n",
    "#         print('mask_file',mask_file)\n",
    "#         corrected_mask_file = mask_file.split('pre_disaster')[0] + 'pre_disaster.png'\n",
    "#         index = img_dict[mask_file]\n",
    "        mask_path = os.path.join(mask_folder,mask_file)\n",
    "#         img_path = os.path.join(images_folder,img_list[index])\n",
    "        mask_img = Image.open(mask_path)\n",
    "#         pre_img = Image.open(img_path)\n",
    "#         L_img = pre_img.convert('L')\n",
    "        L_mask = mask_img.convert('L')\n",
    "#         masked_img = PIL.Image.composite(L_mask,L_img,L_mask)\n",
    "#         save_img = os.path.join(dir_params['targets_folder'],mask_file)\n",
    "#         save_masked = os.path.join(dir_params['target_inputs'],mask_file)\n",
    "        save_mask = os.path.join(dir_params['mask_folder'],mask_file)\n",
    "#         masked_img.save(save_masked)\n",
    "#         L_img.save(save_img)\n",
    "        L_mask.save(save_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_params = {\n",
    "    'targets_folder':'/home/shuza/Code/xview2-baseline/GAN_data/targets',\n",
    "    'target_inputs':'/home/shuza/Code/xview2-baseline/GAN_data/inputs',\n",
    "    'src_folder':'/home/shuza/Code/xview2-baseline/data',\n",
    "    'mask_folder':'/home/shuza/Code/xview2-baseline/GAN_data/masks'\n",
    "}\n",
    "\n",
    "create_dataset(dir_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test combining building masks from tier 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the concept\n",
    "\n",
    "file_1 = '/home/shuza/Code/xview2-baseline/data/woolsey-fire/masks/woolsey-fire_00000000_pre_disaster_22b37215-685d-450d-9fc8-b2eab7a5ebc1.png' \n",
    "file_2 = '/home/shuza/Code/xview2-baseline/data/woolsey-fire/masks/woolsey-fire_00000000_pre_disaster_40d4fa85-2b2f-488e-b3c2-0fee718cac82.png' \n",
    "file_3 = '/home/shuza/Code/xview2-baseline/data/woolsey-fire/masks/woolsey-fire_00000000_pre_disaster_fed80ca6-0dba-4bee-9a7c-67d2f218c204.png' \n",
    "\n",
    "img_1 = Image.open(file_1)\n",
    "img_2 = Image.open(file_2)\n",
    "img_3 = Image.open(file_3)\n",
    "\n",
    "print(img_1.size,img_1.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1.split('pre_disaster')[0] + 'pre_disaster'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(img_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_img = PIL.Image.composite(img_1.convert('L'),img_2.convert('L'),img_1.convert('L'))\n",
    "composite_img = PIL.Image.composite(img_3.convert('L'),composite_img,img_3.convert('L'))\n",
    "# Image.alpha_composite(img_2.convert('RGBA'),img_3.convert('RGBA'))\n",
    "                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(composite_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(composite_img.size)\n",
    "print(img_1.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "copy_img = img_1.copy()\n",
    "# position = ((image_copy.width - logo.width), (image_copy.height - logo.height))\n",
    "copy_img.paste(img_1,(1024,1024),img_2)\n",
    "imshow(copy_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine all the individual building masks in the tier 3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = ['/home/shuza/Code/xview2-baseline/data/joplin-tornado',\n",
    " '/home/shuza/Code/xview2-baseline/data/lower-puna-volcano' ,\n",
    "    '/home/shuza/Code/xview2-baseline/data/moore-tornado' ,\n",
    "'/home/shuza/Code/xview2-baseline/data/nepal-flooding' ,\n",
    "'/home/shuza/Code/xview2-baseline/data/pinery-bushfire' ,\n",
    "'/home/shuza/Code/xview2-baseline/data/portugal-wildfire',\n",
    " '/home/shuza/Code/xview2-baseline/data/sunda-tsunami' ,\n",
    "'/home/shuza/Code/xview2-baseline/data/tuscaloosa-tornado', \n",
    "'/home/shuza/Code/xview2-baseline/data/woolsey-fire']\n",
    "\n",
    "test_folder = '/home/shuza/Code/xview2-baseline/GAN_data/combined' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make progress bar for this.\n",
    "for folder in folder_list:\n",
    "    # Gather all the distinct names and all the single masks per name\n",
    "    mask_folder = os.path.join(folder,'masks')\n",
    "    print('current folder',mask_folder)\n",
    "    names = set()\n",
    "    duplicate_dict = {}\n",
    "    for mask_file in os.listdir(mask_folder):\n",
    "#         print(mask_file)\n",
    "        mask_path = os.path.join(mask_folder,mask_file)\n",
    "#         print(mask_path)\n",
    "        generic_name = mask_file.split('pre_disaster')[0] + 'pre_disaster'\n",
    "#         print('generic_name',generic_name)\n",
    "        names.add(generic_name)\n",
    "        if generic_name not in duplicate_dict:\n",
    "            duplicate_dict[generic_name] = [mask_path]\n",
    "        else:\n",
    "            duplicate_dict[generic_name].append(mask_path)\n",
    "    # Combine all the single building masks into one\n",
    "    for name in names:\n",
    "        mask_list = duplicate_dict[name]\n",
    "        if len(mask_list) > 1:\n",
    "            src_img = Image.open(mask_list[0]).convert('L')\n",
    "            for i in range(len(mask_list) - 1):\n",
    "                dup_img = Image.open(mask_list[i+1]).convert('L')\n",
    "                src_img = PIL.Image.composite(src_img,dup_img,src_img)\n",
    "            # save composite\n",
    "            src_img.save(test_folder + '/' + name + '.png')\n",
    "        else:\n",
    "            # don't change anything\n",
    "            src_img = Image.open(mask_list[0]).convert('L')\n",
    "            src_img.save(test_folder + '/' + name + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GAN on data\n",
    "\n",
    "Two GAN types:\n",
    "- Image repair - Context encoder\n",
    "- Generating img from a mask - Pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def downsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            return layers\n",
    "\n",
    "        def upsample(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.ConvTranspose2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat, 0.8))\n",
    "            layers.append(nn.ReLU())\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *downsample(channels, 32, normalize=False),\n",
    "            *downsample(32, 64),\n",
    "            *downsample(64, 128),\n",
    "            *downsample(128, 256),\n",
    "            *downsample(256, 512),\n",
    "            nn.Conv2d(512, 4000, 1),\n",
    "            *upsample(4000, 512),\n",
    "            *upsample(512, 256),\n",
    "            *upsample(256, 128),\n",
    "            *upsample(128, 64),\n",
    "            *upsample(64, 32),\n",
    "            nn.Conv2d(32, channels, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, stride, normalize):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 3, stride, 1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        layers = []\n",
    "        in_filters = channels\n",
    "        for out_filters, stride, normalize in [(64, 2, False), (128, 2, True), (256, 2, True), (512, 2, True),(1024, 1, True)]:\n",
    "            layers.extend(discriminator_block(in_filters, out_filters, stride, normalize))\n",
    "            in_filters = out_filters\n",
    "\n",
    "        layers.append(nn.Conv2d(out_filters, 1, 3, 1, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters(object):\n",
    "    def __init__(self):\n",
    "        self.mask_size = 64\n",
    "        self.img_size = 1024\n",
    "        self.channels = 1\n",
    "        self.lr = 0.0002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.batch_size = 8\n",
    "        self.n_cpu = 1\n",
    "        self.n_epochs = 1\n",
    "        self.sample_interval = 500\n",
    "        self.latent_dim = 100\n",
    "        self.inputs = '/home/shuza/Code/xview2-baseline/GAN_data/inputs'\n",
    "        self.targets = '/home/shuza/Code/xview2-baseline/GAN_data/targets'\n",
    "        self.masks = '/home/shuza/Code/xview2-baseline/GAN_data/masks'\n",
    "        \n",
    "opt = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test networks\n",
    "\n",
    "I need to make the masks boolean masks. So set all values > 0 to 1 and else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_path = '/home/shuza/Code/xview2-baseline/GAN_data/inputs/mexico-earthquake_00000000_pre_disaster.png'\n",
    "test_img = Image.open(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what patch does\n",
    "patch_h, patch_w = int(opt.mask_size / 2 ** 3), int(opt.mask_size / 2 ** 3)\n",
    "patch = (1, patch_h, patch_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(patch,patch_h,patch_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor =torch.FloatTensor\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch_h, patch_w = int(opt.mask_size / 2 ** 3), int(opt.mask_size / 2 ** 3)\n",
    "patch = (1, patch_h, patch_w)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = Variable(Tensor(1024, *patch).fill_(1.0), requires_grad=False)\n",
    "fake = Variable(Tensor(1024, *patch).fill_(0.0), requires_grad=False)\n",
    "\n",
    "# Configure input\n",
    "# test_img = Variable(test_img.type(Tensor))\n",
    "# masked_imgs = Variable(masked_imgs.type(Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(valid.size())\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test image dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, src_root,tgt_root,mask_root, transforms_=None, img_size=128, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.img_size = img_size\n",
    "        self.mode = mode\n",
    "        self.src_files = sorted(glob.glob(\"%s/*.png\" % src_root))\n",
    "        self.src_files = self.src_files[:-40] if mode == \"train\" else self.src_files[-40:]\n",
    "        self.tgt_files = sorted(glob.glob(\"%s/*.png\" % tgt_root))\n",
    "        self.tgt_files = self.tgt_files[:-40] if mode == \"train\" else self.tgt_files[-40:]\n",
    "        self.mask_files = sorted(glob.glob(\"%s/*.png\" % mask_root))\n",
    "        self.mask_files = self.mask_files[:-40] if mode == \"train\" else self.mask_files[-40:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = Image.open(self.src_files[index % len(self.src_files)])\n",
    "        masked_img = Image.open(self.tgt_files[index % len(self.tgt_files)])\n",
    "        mask_img = Image.open(self.mask_files[index % len(self.mask_files)])\n",
    "#         print(type(img),type(masked_img))\n",
    "        img = self.transform(img)\n",
    "        masked_img = self.transform(masked_img)\n",
    "        mask_img = self.transform(mask_img)\n",
    "\n",
    "#         print(type(img),type(masked_img))\n",
    "        return img, masked_img, mask_img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_files) + len(self.tgt_files) + len(self.mask_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_ = [\n",
    "    transforms.Resize((opt.img_size, opt.img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(opt.inputs,opt.targets,opt.masks,transforms_=transforms_,img_size=1024),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    ImageDataset(opt.inputs,opt.targets,opt.masks,transforms_=transforms_,img_size=1024,mode=\"test\"),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_cpu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataloader))\n",
    "# print(next(iter(dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(channels=opt.channels)\n",
    "discriminator = Discriminator(channels=opt.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test using mask to isolate masked parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src,tgt,mask = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_parts = generator(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 1024, 1024]) torch.Size([8, 1, 1024, 1024]) torch.Size([8, 1, 1024, 1024]) torch.Size([8, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(src.size(),tgt.size(),mask.size(),masked_parts.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = discriminator(masked_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(fv.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src,'\\n')\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times the generated image and src image by the mask and get the pixel loss\n",
    "tgt_pixels = tgt * mask\n",
    "gen_pixels = masked_parts * mask\n",
    "print(tgt_pixels.size(),gen_pixels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gen_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_img = gen_pixels.detach()[0,:,:,:].reshape(1024,1024)\n",
    "tgt_img = tgt_pixels.detach()[0,:,:,:].reshape(1024,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(tgt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(tgt[0,:,:,:].reshape(1024,1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train baby train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(batches_done):\n",
    "    masked_samples,samples,masks  = next(iter(test_dataloader))\n",
    "    samples = Variable(samples.type(Tensor))\n",
    "    masked_samples = Variable(masked_samples.type(Tensor))\n",
    "    masks = Variable(masks.type(Tensor))\n",
    "#     i = i[0].item()  # Upper-left coordinate of mask\n",
    "#     # Generate inpainted image\n",
    "    gen_img = generator(masked_samples)\n",
    "    generated_buildings = gen_img * masks\n",
    "#     filled_samples = masked_samples.clone()\n",
    "#     filled_samples[:, :, i : i + opt.mask_size, i : i + opt.mask_size] = gen_mask\n",
    "#     # Save sample\n",
    "#     sample = torch.cat((masked_samples.data, filled_samples.data, samples.data), -2)\n",
    "    save_image(generated_buildings, \"test_images/%d.png\" % batches_done, nrow=6, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = torch.nn.MSELoss()\n",
    "pixelwise_loss = torch.nn.L1Loss()\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator(channels=opt.channels)\n",
    "discriminator = Discriminator(channels=opt.channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize((opt.img_size, opt.img_size), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    ImageDataset(opt.inputs,opt.targets,opt.masks,transforms_=transforms_,img_size=1024),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    ImageDataset(opt.inputs,opt.targets,opt.masks,transforms_=transforms_,img_size=1024,mode=\"test\"),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.n_cpu,\n",
    ")\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "Tensor =  torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# src,tgt,mask = next(iter(dataloader))\n",
    "# print(type(src),type(tgt))\n",
    "# print(src.size(),tgt.size())\n",
    "# valid = Variable(Tensor(12, 1024,1024).fill_(1.0), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2883e8d22df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmasked_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Adversarial ground truths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/xview/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Masked parts: src image * mask = only src buildings\n",
    "masks: black and white mask of buildings and background\n",
    "masked imgs: src image with buildings masked out\n",
    "imgs: src image\n",
    "masked gen: generated image with background masked out\n",
    "\"\"\"\n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (masked_imgs,imgs, masks) in enumerate(dataloader):\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(Tensor(imgs.shape[0], 1,64,64).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(Tensor(imgs.shape[0], 1,64,64).fill_(0.0), requires_grad=False)\n",
    "        \n",
    "        masked_valid = valid * mask\n",
    "        masked_fake = fake * mask\n",
    "\n",
    "        buildings = imgs * masks\n",
    "        # Configure input\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        masked_imgs = Variable(masked_imgs.type(Tensor))\n",
    "        masks = Variable(masks.type(Tensor))\n",
    "        buildings = Variable(buildings.type(Tensor))\n",
    "        \n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_img = generator(masked_imgs)\n",
    "        \n",
    "        generated_buildings = gen_img * masks\n",
    "\n",
    "        # Adversarial and pixelwise loss\n",
    "        g_adv = adversarial_loss(discriminator(generated_buildings), masked_valid)\n",
    "        g_pixel = pixelwise_loss(generated_buildings, buildings)\n",
    "        # Total loss\n",
    "        g_loss = 0.001 * g_adv + 0.999 * g_pixel\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = adversarial_loss(discriminator(buildings), masked_valid)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_buildings.detach()), masked_fake)\n",
    "        d_loss = 0.5 * (real_loss + fake_loss)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G adv: %f, pixel: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_adv.item(), g_pixel.item())\n",
    "        )\n",
    "\n",
    "        # Generate sample at sample interval\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_sample(batches_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate img from mask\n",
    "\n",
    "Data already exists in the folders. Just a matter of linking up the src_img and the mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "##############################\n",
    "#           U-NET\n",
    "##############################\n",
    "\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
    "        super(UNetDown, self).__init__()\n",
    "        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n",
    "        if normalize:\n",
    "            layers.append(nn.InstanceNorm2d(out_size))\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dropout=0.0):\n",
    "        super(UNetUp, self).__init__()\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "\n",
    "        self.down1 = UNetDown(in_channels, 64, normalize=False)\n",
    "        self.down2 = UNetDown(64, 128)\n",
    "        self.down3 = UNetDown(128, 256)\n",
    "        self.down4 = UNetDown(256, 512, dropout=0.5)\n",
    "        self.down5 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down6 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down7 = UNetDown(512, 512, dropout=0.5)\n",
    "        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n",
    "\n",
    "        self.up1 = UNetUp(512, 512, dropout=0.5)\n",
    "        self.up2 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up3 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up4 = UNetUp(1024, 512, dropout=0.5)\n",
    "        self.up5 = UNetUp(1024, 256)\n",
    "        self.up6 = UNetUp(512, 128)\n",
    "        self.up7 = UNetUp(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, 4, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # U-Net generator with skip connections from encoder to decoder\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "\n",
    "        return self.final(u7)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
