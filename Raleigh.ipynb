{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the data looks like\n",
    "\n",
    "3 columns:\n",
    "1. Equation 1\n",
    "2. Equation 2\n",
    "3. Label\n",
    "\n",
    "Need to split the data by character and then join cols 1 and 2 with each other, with a seperator in between.\n",
    "\n",
    "Need to create a character dictionary that maps values to integars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/morgan/Downloads/balancedTrainingSet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0i1)</th>\n",
       "      <th>(0i1).1</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;0i2)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;1i0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;1i2)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;2i0)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;2i1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (0i1)    (0i1).1  1\n",
       "0  (0i1)  (0i1&0i2)  1\n",
       "1  (0i1)  (0i1&1i0)  1\n",
       "2  (0i1)  (0i1&1i2)  1\n",
       "3  (0i1)  (0i1&2i0)  1\n",
       "4  (0i1)  (0i1&2i1)  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999999\n"
     ]
    }
   ],
   "source": [
    "N = df.shape[0]\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shape the csv values\n",
    "\n",
    "Add divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "divider = np.full(N,'@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['divider'] = divider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input'] = df['(0i1)'] + df['divider'] + df['(0i1).1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (0i1)@(0i1&0i2)\n",
       "1    (0i1)@(0i1&1i0)\n",
       "2    (0i1)@(0i1&1i2)\n",
       "3    (0i1)@(0i1&2i0)\n",
       "4    (0i1)@(0i1&2i1)\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(0i1)</th>\n",
       "      <th>(0i1).1</th>\n",
       "      <th>1</th>\n",
       "      <th>divider</th>\n",
       "      <th>input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;0i2)</td>\n",
       "      <td>1</td>\n",
       "      <td>@</td>\n",
       "      <td>(0i1)@(0i1&amp;0i2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;1i0)</td>\n",
       "      <td>1</td>\n",
       "      <td>@</td>\n",
       "      <td>(0i1)@(0i1&amp;1i0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;1i2)</td>\n",
       "      <td>1</td>\n",
       "      <td>@</td>\n",
       "      <td>(0i1)@(0i1&amp;1i2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;2i0)</td>\n",
       "      <td>1</td>\n",
       "      <td>@</td>\n",
       "      <td>(0i1)@(0i1&amp;2i0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0i1)</td>\n",
       "      <td>(0i1&amp;2i1)</td>\n",
       "      <td>1</td>\n",
       "      <td>@</td>\n",
       "      <td>(0i1)@(0i1&amp;2i1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   (0i1)    (0i1).1  1 divider            input\n",
       "0  (0i1)  (0i1&0i2)  1       @  (0i1)@(0i1&0i2)\n",
       "1  (0i1)  (0i1&1i0)  1       @  (0i1)@(0i1&1i0)\n",
       "2  (0i1)  (0i1&1i2)  1       @  (0i1)@(0i1&1i2)\n",
       "3  (0i1)  (0i1&2i0)  1       @  (0i1)@(0i1&2i0)\n",
       "4  (0i1)  (0i1&2i1)  1       @  (0i1)@(0i1&2i1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the input and expand into new df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = df['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    (0i1)@(0i1&0i2)\n",
       "1    (0i1)@(0i1&1i0)\n",
       "2    (0i1)@(0i1&1i2)\n",
       "3    (0i1)@(0i1&2i0)\n",
       "4    (0i1)@(0i1&2i1)\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.str.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.apply(lambda x: pd.Series(list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = df.iloc[:,4].str.split().str.get(0).str.split('',expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean resulting df\n",
    "\n",
    "Replace '' and None with zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs.drop([0], axis=1,inplace=True)\n",
    "# inputs.drop([30], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999999, 39)\n",
      "  0  1  2  3  4  5  6  7  8  9   ...   29   30   31   32   33   34   35   36  \\\n",
      "0  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "3  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "4  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    37   38  \n",
      "0  NaN  NaN  \n",
      "1  NaN  NaN  \n",
      "2  NaN  NaN  \n",
      "3  NaN  NaN  \n",
      "4  NaN  NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(inputs.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn None into empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = inputs.applymap(lambda x: x is None)\n",
    "# cols = inputs.columns[(mask).any()]\n",
    "# for col in inputs[cols]:\n",
    "#     inputs.loc[mask[col], col] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  1  2  3  4  5  6  7  8  9   ...   29   30   31   32   33   34   35   36  \\\n",
      "0  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "1  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "2  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "3  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "4  (  0  i  1  )  @  (  0  i  1  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN   \n",
      "\n",
      "    37   38  \n",
      "0  NaN  NaN  \n",
      "1  NaN  NaN  \n",
      "2  NaN  NaN  \n",
      "3  NaN  NaN  \n",
      "4  NaN  NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# print(inputs.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turn empty string into character denoting white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = inputs.applymap(lambda x: x is np.nan)\n",
    "cols = inputs.columns[(mask).any()]\n",
    "for col in inputs[cols]:\n",
    "    inputs.loc[mask[col], col] = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  1  2  3  4  5  6  7  8  9   ...     29     30     31     32     33  \\\n",
      "0  (  0  i  1  )  @  (  0  i  1  ...  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
      "1  (  0  i  1  )  @  (  0  i  1  ...  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
      "2  (  0  i  1  )  @  (  0  i  1  ...  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
      "3  (  0  i  1  )  @  (  0  i  1  ...  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
      "4  (  0  i  1  )  @  (  0  i  1  ...  <pad>  <pad>  <pad>  <pad>  <pad>   \n",
      "\n",
      "      34     35     36     37     38  \n",
      "0  <pad>  <pad>  <pad>  <pad>  <pad>  \n",
      "1  <pad>  <pad>  <pad>  <pad>  <pad>  \n",
      "2  <pad>  <pad>  <pad>  <pad>  <pad>  \n",
      "3  <pad>  <pad>  <pad>  <pad>  <pad>  \n",
      "4  <pad>  <pad>  <pad>  <pad>  <pad>  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1999999, 39)\n",
      "RangeIndex(start=0, stop=39, step=1)\n"
     ]
    }
   ],
   "source": [
    "# print(inputs.iloc[0,13])\n",
    "# print(type(inputs[0]))\n",
    "print(inputs.head(5))\n",
    "print(type(inputs))\n",
    "print(inputs.shape)\n",
    "print(inputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.iloc[:,2].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.to_csv('cleanedRaleighData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Data\n",
    "\n",
    "### For transformer\n",
    "\n",
    "_ will be the end of line character.\n",
    "\n",
    "\n",
    "### For MLP\n",
    "\n",
    "Turn the dataset into 1 hot encoded vectors, with zero padding for end of line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dictionary of labels\n",
    "\n",
    "break up the inputs into single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all unique occurances in the dataset to feed into the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_uniques(values):\n",
    "    uniques, count = np.unique(values, return_counts=True)\n",
    "    print(uniques, count)\n",
    "    return uniques, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&' '(' ')' '0' '1' '2' '<pad>' '@' 'i' 'x' '|'] [ 5267649  6944727  6944727  8234252  8151150  8039350 17261002  1999999\n",
      "  6596280  5616096  2944729]\n"
     ]
    }
   ],
   "source": [
    "uniques,counts = return_uniques(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['&', '(', ')', '0', '1', '2', '<pad>', '@', 'i', 'x', '|']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(list(le.classes_))\n",
    "print('num_classes',num_classes)\n",
    "list(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [1000000  999999]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1000000,  999999]))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_uniques(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the inputs to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_inputs = inputs.apply(lambda x: le.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_inputs = ml_inputs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 8, 4, 2, 7, 1, 3, 8, 4, 0, 3, 8, 5, 2, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "       [1, 3, 8, 4, 2, 7, 1, 3, 8, 4, 0, 4, 8, 3, 2, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "       [1, 3, 8, 4, 2, 7, 1, 3, 8, 4, 0, 4, 8, 5, 2, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "       [1, 3, 8, 4, 2, 7, 1, 3, 8, 4, 0, 5, 8, 3, 2, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\n",
       "       [1, 3, 8, 4, 2, 7, 1, 3, 8, 4, 0, 5, 8, 4, 2, 6, 6, 6, 6, 6, 6, 6,\n",
       "        6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data sets\n",
    "\n",
    "Because the data is heavily mismatched, We should try a limited but equal dataset. And after that try training more frequently on the 1 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999999, 39) (1999999,)\n"
     ]
    }
   ],
   "source": [
    "X = np_inputs\n",
    "y = labels\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_1s = y == 1\n",
    "# mask_0s = y == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1 = X[mask_1s]\n",
    "# y1 = y[mask_1s]\n",
    "\n",
    "# X0 = X[mask_0s]\n",
    "# y0 = y[mask_0s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes = np.arange(X0.shape[0])\n",
    "# restricted_mask = np.random.choice(indexes,X1.shape[0],replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrictedX0 = X0[restricted_mask]\n",
    "# restrictedy0 = y0[restricted_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5473, 29)\n",
      "(994526, 29)\n",
      "(5473, 29)\n"
     ]
    }
   ],
   "source": [
    "# print(X1.shape)\n",
    "# print(X0.shape)\n",
    "# print(restrictedX0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrictedX = np.concatenate([restrictedX0,X1],axis=0)\n",
    "# restrictedy = np.concatenate([restrictedy0,y1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mathDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X,y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        sample = {'item': X[idx], 'label': y[idx]}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = mathDataset(X_train,y_train)\n",
    "valset = mathDataset(X_val,y_val)\n",
    "testset = mathDataset(X_test,y_test)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=512,\n",
    "                        shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(valset, batch_size=512,\n",
    "                        shuffle=False, num_workers=4)\n",
    "testloader = DataLoader(testset, batch_size=512,\n",
    "                        shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'trainset':trainloader,\n",
    "    'valset':valloader,\n",
    "    'testset':testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "We only need a binary classification in the output. So in a very real sense we don't even need a decoder, just a feed forward binary classification NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size,d_model)\n",
    "    def forward(self,x):\n",
    "        return self.embed(x)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self,d_model, maxlen_seq = 39):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(maxlen_seq,d_model)\n",
    "        for pos in range(maxlen_seq):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos,i] = pos / math.sin(10000 ** ((2*i)/d_model))\n",
    "                pe[pos,i+1] = pos / math.cos(10000 ** ((2*(i+1))/d_model))\n",
    "        pe.unsqueeze(0)\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        seq_len = x.size(-1)\n",
    "        x = x + Variable(self.pe[:,:self.d_model],requires_grad=False)\n",
    "        return x\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // heads\n",
    "        self.h = heads\n",
    "        \n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        bs = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "        \n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "       \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous()\\\n",
    "        .view(bs, -1, self.d_model)\n",
    "        \n",
    "        output = self.out(concat)\n",
    "    \n",
    "        return output\n",
    "    \n",
    "def attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    \n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1).unsqueeze(3)\n",
    "        # print('scores,mask',scores.size(),mask.size())\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "    # print('scores, v',scores.size(), v.size())\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.size = d_model\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm\n",
    "\n",
    "# build an encoder layer with one multi-head attention layer and one # feed-forward layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "    \n",
    "# build a decoder layer with two multi-head attention layers and\n",
    "# one feed-forward layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model)\n",
    "def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "# We can then build a convenient cloning function that can generate multiple layers:\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, src, mask):\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = torch.concat([hour,day,month,target])\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "class ClassificationOut(nn.Module):\n",
    "    def __init__(self,d_model,N):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.layers = get_clones(FeedForward(d_model),N)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab,N_categories,maxlen, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = ClassificationOut(d_model,N)\n",
    "        self.out = nn.Linear(maxlen*d_model, N_categories)\n",
    "    def forward(self, src, src_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(e_outputs)\n",
    "        flat_d = d_output.view(d_output.size(0),-1)\n",
    "        output = self.out(flat_d)\n",
    "        return output\n",
    "# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masks\n",
    "\n",
    "I don't need any masks aside from the input padding attention mask, because the output is not a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 256\n",
    "N = 2\n",
    "N_categories = 2\n",
    "heads = 8\n",
    "vocab_size = num_classes\n",
    "maxlen = 39\n",
    "\n",
    "test_net = Transformer(vocab_size,N_categories,maxlen, d_model, N, heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 39) [[1 3 8 4 2 7 1 3 8 4 0 3 8 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "  6 6 6]\n",
      " [1 3 8 4 2 7 1 3 8 4 0 4 8 3 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "  6 6 6]\n",
      " [1 3 8 4 2 7 1 3 8 4 0 4 8 5 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "  6 6 6]\n",
      " [1 3 8 4 2 7 1 3 8 4 0 5 8 3 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "  6 6 6]\n",
      " [1 3 8 4 2 7 1 3 8 4 0 5 8 4 2 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      "  6 6 6]]\n"
     ]
    }
   ],
   "source": [
    "test_data = np_inputs[:5]#.reshape(1,29)\n",
    "print(test_data.shape,test_data)\n",
    "torch_test = torch.from_numpy(test_data)\n",
    "src_msk = torch_test != 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.ones((5,maxlen,256))\n",
    "# b = a.view(a.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 39])\n",
      "torch.Size([5, 39])\n"
     ]
    }
   ],
   "source": [
    "# print(b.size())\n",
    "print(src_msk.size())\n",
    "print(torch_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.ones((5,8,maxlen,maxlen))\n",
    "# b = torch.zeros((5,1,maxlen,1))\n",
    "# a = a.masked_fill(b == 0, -1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores,mask torch.Size([5, 8, 39, 39]) torch.Size([5, 1, 39, 1])\n",
      "scores, v torch.Size([5, 8, 39, 39]) torch.Size([5, 8, 39, 32])\n",
      "scores,mask torch.Size([5, 8, 39, 39]) torch.Size([5, 1, 39, 1])\n",
      "scores, v torch.Size([5, 8, 39, 39]) torch.Size([5, 8, 39, 32])\n"
     ]
    }
   ],
   "source": [
    "preds = test_net(torch_test,src_msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0281,  0.0152],\n",
       "        [-0.0116,  0.0257],\n",
       "        [-0.0206,  0.0086],\n",
       "        [-0.0329,  0.0161],\n",
       "        [-0.0099, -0.0063]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4892, 0.5108],\n",
       "        [0.4907, 0.5093],\n",
       "        [0.4927, 0.5073],\n",
       "        [0.4878, 0.5122],\n",
       "        [0.4991, 0.5009]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(preds,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3265, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(preds.view(1,10),torch.tensor([0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,data_dict,train_params):\n",
    "    start_time = time.time()\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for e in range(train_params['epochs']):\n",
    "        for i, data in enumerate(data_dict['trainset'], 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data.values()\n",
    "            src_mask = inputs != 6\n",
    "            preds = model(inputs,src_mask)\n",
    "#             print('preds',preds,'y',labels)\n",
    "            optim.zero_grad()\n",
    "            loss = train_params['criterion'](preds,labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            scores_window.append(loss.data)\n",
    "            scores.append(loss.data)\n",
    "            if (i + 1) % train_params['print_every'] == 0:\n",
    "                loss_avg = np.mean(scores_window)\n",
    "                print(\"time = %dm, epoch %d, iter = %d, loss = %.3f, %ds per %d iters\" % \n",
    "                      ((time.time() - start_time) // 60,e + 1, i + 1, loss_avg, time.time() - start_time,\n",
    "                train_params['print_every']))\n",
    "            if i == train_params['episodes']:\n",
    "                break\n",
    "    return model,scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Hyperparam search and LR stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "N = 2\n",
    "N_categories = 2\n",
    "heads = 8\n",
    "maxlen = 39\n",
    "vocab_size = num_classes\n",
    "\n",
    "model = Transformer(vocab_size,N_categories,maxlen, d_model, N, heads)\n",
    "\n",
    "criterion = F.cross_entropy\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "train_params = {\n",
    "    'epochs':10,\n",
    "    'episodes':50,\n",
    "    'optimizer':optim,\n",
    "    'criterion': criterion,\n",
    "    'print_every':10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 2m, epoch 1, iter = 10, loss = 0.627,166s per 10 iters\n",
      "time = 5m, epoch 1, iter = 20, loss = 0.535,340s per 10 iters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-86238790e5a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-05681bf4803e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, data_dict, train_params)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'criterion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mscores_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model,scores = train_model(model,data,train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 29]) torch.Size([512])\n",
      "tensor([[9.9679e-01, 3.2067e-03],\n",
      "        [9.9441e-01, 5.5931e-03],\n",
      "        [9.5624e-01, 4.3756e-02],\n",
      "        ...,\n",
      "        [9.9461e-01, 5.3859e-03],\n",
      "        [9.9883e-01, 1.1724e-03],\n",
      "        [9.9958e-01, 4.2087e-04]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) tensor([[0.9349, 0.0651],\n",
      "        [0.7700, 0.2300],\n",
      "        [0.4588, 0.5412],\n",
      "        [0.8108, 0.1892],\n",
      "        [0.9732, 0.0268],\n",
      "        [0.4862, 0.5138],\n",
      "        [0.4204, 0.5796],\n",
      "        [0.8902, 0.1098],\n",
      "        [0.8161, 0.1839],\n",
      "        [0.8129, 0.1871],\n",
      "        [0.7815, 0.2185],\n",
      "        [0.9193, 0.0807],\n",
      "        [0.1583, 0.8417],\n",
      "        [0.7052, 0.2948],\n",
      "        [0.4678, 0.5322],\n",
      "        [0.9621, 0.0379],\n",
      "        [0.4987, 0.5013]], grad_fn=<IndexBackward>)\n",
      "torch.Size([512, 29]) torch.Size([512])\n",
      "tensor([[9.8417e-01, 1.5834e-02],\n",
      "        [2.8159e-01, 7.1841e-01],\n",
      "        [9.7386e-01, 2.6136e-02],\n",
      "        ...,\n",
      "        [9.9993e-01, 7.4016e-05],\n",
      "        [9.9528e-01, 4.7158e-03],\n",
      "        [9.8944e-01, 1.0562e-02]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) tensor([[0.2816, 0.7184],\n",
      "        [0.4153, 0.5847],\n",
      "        [0.8515, 0.1485],\n",
      "        [0.7535, 0.2465],\n",
      "        [0.4401, 0.5599],\n",
      "        [0.8884, 0.1116],\n",
      "        [0.6384, 0.3616],\n",
      "        [0.6205, 0.3795],\n",
      "        [0.5179, 0.4821],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.3781, 0.6219],\n",
      "        [0.7758, 0.2242],\n",
      "        [0.4538, 0.5462],\n",
      "        [0.8581, 0.1419],\n",
      "        [0.2512, 0.7488]], grad_fn=<IndexBackward>)\n",
      "torch.Size([512, 29]) torch.Size([512])\n",
      "tensor([[0.9817, 0.0183],\n",
      "        [0.9774, 0.0226],\n",
      "        [0.9971, 0.0029],\n",
      "        ...,\n",
      "        [0.9810, 0.0190],\n",
      "        [0.9983, 0.0017],\n",
      "        [0.8725, 0.1275]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) tensor([[0.3311, 0.6689],\n",
      "        [0.8228, 0.1772],\n",
      "        [0.5117, 0.4883],\n",
      "        [0.4020, 0.5980],\n",
      "        [0.8535, 0.1465],\n",
      "        [0.4984, 0.5016],\n",
      "        [0.8661, 0.1339],\n",
      "        [0.7986, 0.2014],\n",
      "        [0.7034, 0.2966],\n",
      "        [0.8329, 0.1671],\n",
      "        [0.7984, 0.2016],\n",
      "        [0.8519, 0.1481],\n",
      "        [0.8436, 0.1564],\n",
      "        [0.4824, 0.5176],\n",
      "        [0.8725, 0.1275]], grad_fn=<IndexBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    d = next(iter(data['trainset']))\n",
    "    inputs,label = d.values()\n",
    "    print(inputs.size(),label.size())\n",
    "    src_mask = inputs != 6\n",
    "    preds = trained_model(inputs,src_mask)\n",
    "    print(F.softmax(preds,dim=1))\n",
    "    nonzero_mask = label == 1\n",
    "    print(label[nonzero_mask],F.softmax(preds,dim=1)[nonzero_mask])\n",
    "#     print(labels - F.softmax(preds,dim=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN for predicting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def hard_update(source,target):\n",
    "    for target_param,param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(128,64,64,32)):\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        \n",
    "        self.emb = nn.Embedding(11,40)\n",
    "        self.input_layer = nn.Linear(40,hidden_dims[0])\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(hidden_dims[0],hidden_dims[1]))\n",
    "        for i in range(1,len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        # self.fc1 = nn.Linear(hidden_dims[0]+nA,hidden_dims[1])\n",
    "        # self.fc1_bn = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1],nA)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.input_layer.weight.data.uniform_(*hidden_init(self.input_layer))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.weight.data.uniform_(*hidden_init(hidden_layer))\n",
    "        self.output_layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        # With batchnorm\n",
    "        # xs = self.input_bn(F.relu(self.input_layer(state)))\n",
    "        # x = torch.cat((xs,action),dim=1)\n",
    "        # x = self.fc1_bn(F.relu(self.fc1(x)))\n",
    "        embs = self.emb(state)#.view((1, -1))\n",
    "        x = F.relu(self.input_layer(embs))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return F.log_softmax(x,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate the NN.\n",
    "\n",
    "predict 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np_inputs\n",
    "y = labels\n",
    "print(X.shape,vanilla_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nS = X.shape[1]\n",
    "nA = 2\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Critic(seed,nS,nA)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train,val,test\n",
    "\n",
    "1 hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='error',\n",
       "       n_values=None, sparse=True)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = preprocessing.OneHotEncoder(categories='auto')\n",
    "one_hot.fit(labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(639999, 29)\n",
      "(639999,)\n",
      "(160000, 29)\n",
      "(160000,)\n",
      "(200000, 29)\n",
      "(200000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [X_train,y_train, X_val, y_val, X_test, y_test]\n",
    "[print(d.shape) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch tensor\n",
    "torch_data = []\n",
    "for d in data:\n",
    "    torch_data.append(torch.from_numpy(d).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_X_train,torch_y_train, torch_X_val, torch_y_val, torch_X_test, torch_y_test = torch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data.values()\n",
    "    one_hot_labels = one_hot.transform(labels.reshape(-1,1)).toarray()\n",
    "    if i == 0:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 29]) tensor([[1, 3, 8,  ..., 7, 7, 7],\n",
      "        [1, 3, 8,  ..., 9, 3, 2],\n",
      "        [1, 3, 8,  ..., 7, 7, 7],\n",
      "        ...,\n",
      "        [1, 4, 9,  ..., 7, 7, 7],\n",
      "        [1, 3, 8,  ..., 7, 7, 7],\n",
      "        [1, 3, 8,  ..., 8, 3, 2]])\n",
      "(2048, 2) [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape,inputs)\n",
    "print(one_hot_labels.shape,one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.624871730804443\n",
      "7.624739170074463\n",
      "7.624658584594727\n",
      "7.624699592590332\n",
      "7.624795436859131\n",
      "7.624780178070068\n",
      "7.624799728393555\n",
      "7.624707221984863\n",
      "7.624665260314941\n",
      "7.624787330627441\n"
     ]
    }
   ],
   "source": [
    "for e in range(1,episodes+1):\n",
    "#     score_window = deque(maxlen=100)\n",
    "    # running_loss = 0\n",
    "    scores_window = deque(maxlen=100)\n",
    "    # for i, data in enumerate(trainloader, 0):\n",
    "    #     # get the inputs; data is a list of [inputs, labels]\n",
    "    #     inputs, labels = data.values()\n",
    "    #     one_hot_labels = one_hot.transform(labels.reshape(-1,1)).toarray()\n",
    "\t\t# y = torch.from_numpy(one_hot_labels).long()\n",
    "\t\t# X = torch.from_numpy(X).long()\n",
    "    # print(inputs)\n",
    "    # print(labels)\n",
    "    # print(type(inputs[0][0]))\n",
    "    # print(type(inputs),type(labels))\n",
    "    # print('torch_hot_labels',torch_hot_labels)\n",
    "    # print(inputs.size(),torch_hot_labels.size())\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, torch_hot_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    scores_window.append(loss.item())\n",
    "    # running_loss += loss.item()\n",
    "    # if i == 0:\n",
    "    #     break;\n",
    "    print(np.mean(scores_window))\n",
    "        # if i % 25 == 0:    # print every 25 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (e, i, running_loss / 25))\n",
    "        #     running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-345-a5e2a5de2056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m ]\n\u001b[1;32m      8\u001b[0m \u001b[0mhistorical_routes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistorical_routes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstripped_routes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistorical_routes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/data-analysis/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "import random\n",
    "names = ['marmoset,tamarin,capuchin,lemur,crocodile,alligator,hippopotamus']\n",
    "N = 5\n",
    "N = len(names) if N > len(names) else N\n",
    "stripped_routes = [\n",
    "    \n",
    "]\n",
    "historical_routes = np.zeros(5)\n",
    "historical_routes = np.vstack((stripped_routes,historical_routes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['marmoset,tamarin,capuchin,lemur,crocodile,alligator,hippopotamus']\n",
      "['marmoset,tamarin,capuchin,lemur,crocodile,alligator,hippopotamus']\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(names)\n",
    "print(names)\n",
    "random.shuffle(names)\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try different architectures\n",
    "\n",
    "- Vanilla NN\n",
    "- Transformer\n",
    "- LSTM\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost with PBT and multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process,Pool,RawArray\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier,XGBRegressor, plot_importance\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBTV2(object):\n",
    "    def __init__(self,boost_params,attribute_dict):\n",
    "        \"\"\"\n",
    "        boost_params : dictionary of default xgboost params\n",
    "        attribute_dict : dictionary of hyperparameters for PBT + dictionaries for random/scaled param search\n",
    "        \n",
    "        mu: Total population\n",
    "        tau: offspring per step. Must be less than mu (typically 1/6ish)\n",
    "        \n",
    "        Sample x from the top performers\n",
    "        Sample x from the highest diversity of the population\n",
    "        \n",
    "        modifications:\n",
    "        mutation of one or more params\n",
    "        combination of two parent genes\n",
    "        \n",
    "        Random sample from initial distribution\n",
    "        Scale by a factor between 0.8 and 1.2 \n",
    "        \n",
    "        Scores: dict, round -> scores of all agents.\n",
    "        \n",
    "        Individuals: np.array - shape(mu,len(params.keys) + scores + diversity)\n",
    "        param_dict: dictionary of param keys for generating random values within a range\n",
    "        param_constraints: dictionary of param keys : tuple([low,high],dtype) for scaling params within a range\n",
    "        set_dict: dictionary of param keys and values that are unchanging\n",
    "        \"\"\"\n",
    "        self.mu = attribute_dict['mu']\n",
    "        self.tau = attribute_dict['tau']\n",
    "        self.seed = attribute_dict['seed']\n",
    "        self.N_islands = attribute_dict['islands']\n",
    "        self.metrics = attribute_dict['metrics']\n",
    "        self.model_type = attribute_dict['model_type']\n",
    "        self.population_dict = {}\n",
    "        self.scores = []\n",
    "        self.combined_metric = []\n",
    "        self.historical_params = []\n",
    "        self.params = boost_params\n",
    "        self.useable_mask = np.ones(len(self.params.keys()),dtype=bool)\n",
    "        mask = np.array([-4,-3,-2,-1])\n",
    "        self.useable_mask[mask] = 0\n",
    "        self.param_dict = attribute_dict['param_dict']\n",
    "        # For scaling params (some params don't scale)\n",
    "        self.param_constraints = attribute_dict['param_constraints']\n",
    "        self.param_index = {k:i for i,k in enumerate(self.param_dict.keys())}\n",
    "        self.set_dict = attribute_dict['set_dict']\n",
    "        self.set_keys = ['num_round','n_estimators','scale_pos_weight','objective']\n",
    "        self.set_values = [30,1000,1,'reg:squarederror']\n",
    "        self.diversity_discount = np.array([0.199,4,7,7,0.5,0.5,0.5,0.5])\n",
    "        self.diversity_subtract = np.array([0.001,1,3,3,0.5,0.5,0,0])\n",
    "        self.N_params = len(self.param_dict.keys())\n",
    "        self.N_cols = self.N_params + 3 #+ score and diversity and combined\n",
    "        self.score_col = self.N_params\n",
    "        self.diversity_col = self.N_params + 1\n",
    "        self.combined_col = self.N_params + 2\n",
    "        self.M_shape = (self.mu,self.N_cols)\n",
    "        for island in range(self.N_islands):\n",
    "            self.population_dict[island] = self.instantiate_params()\n",
    "            # Now add score and diversity\n",
    "    #         self.linear_return_fitness(self.individuals)\n",
    "            self.population_dict[island] = self.return_fitness(self.population_dict[island])\n",
    "            self.population_dict[island] = self.return_diversity(self.population_dict[island])\n",
    "            self.population_dict[island] = self.return_combined_score(self.population_dict[island])\n",
    "            # Record for prosperity\n",
    "            self.scores.append({0:copy.copy(self.population_dict[island][:,self.score_col])})\n",
    "            self.combined_metric.append({0:copy.copy(self.population_dict[island][:,self.combined_col])})\n",
    "            self.historical_params.append({0:copy.copy(self.population_dict[island])})\n",
    "        \n",
    "    def np_return_model(self,params):\n",
    "        learning_rate,max_depth,min_child_weight,gamma,subsample,colsample_bytree,L2,L1 = params\n",
    "        model = self.model_type(num_round = self.set_dict['num_round'],\n",
    "        n_estimators=self.set_dict['n_estimators'],\n",
    "        objective=self.set_dict['objective'],\n",
    "        scale_pos_weight=self.set_dict['scale_pos_weight'],\n",
    "        learning_rate = learning_rate,\n",
    "        max_depth=int(max_depth),\n",
    "        min_child_weight=int(min_child_weight),\n",
    "        gamma=gamma,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        reg_lambda=L2,\n",
    "        reg_alpha=L1,\n",
    "        seed=self.seed)\n",
    "        return model\n",
    "        \n",
    "    def instantiate_params(self):\n",
    "        population = np.zeros((self.mu,self.N_cols))\n",
    "        for i,key in enumerate(self.param_dict.keys()):\n",
    "            column = [self.param_dict[key]() for row in range(self.mu)]\n",
    "            population[:,i] = np.asarray(column)\n",
    "        return population\n",
    "    \n",
    "    def fitness(self,params):\n",
    "#         print('params',params)\n",
    "        model = self.np_return_model(params)\n",
    "        model = PBTV2.XGBoostTrain(self.X_train, self.X_valid, self.y_train, self.y_valid, model)\n",
    "        y_pred = model.predict(self.X_valid)\n",
    "        loss = np.mean(abs(self.y_valid - y_pred))\n",
    "        print('Mean Absolute Error: ',loss)\n",
    "        return loss\n",
    "    \n",
    "    def linear_return_fitness(self,group):\n",
    "        params = group[:,:self.N_params]\n",
    "        losses = []\n",
    "        for row in params:\n",
    "            loss = self.fitness(row)\n",
    "            losses.append(loss)\n",
    "            \n",
    "    def return_fitness(self,population):\n",
    "        N = population.shape[0]\n",
    "        params = population[:,:self.N_params]\n",
    "        param_list = list(params)\n",
    "        print('param_list',param_list)\n",
    "        with Pool(processes=4) as pool:\n",
    "            results = pool.map(outside_fitness,param_list)\n",
    "            print('results',results)\n",
    "        population[:,self.score_col] = np.array(results).reshape(1,N)\n",
    "        return population\n",
    "        \n",
    "    def return_diversity(self,population):\n",
    "        \"\"\"\n",
    "        Each category must be weighted appropriately\n",
    "        'learning_rate' : '',\n",
    "        'min_child_weight': /100,\n",
    "        'max_depth': /100,\n",
    "        'gamma': /100,\n",
    "        'subsample': '',\n",
    "        'colsample_bytree': '',\n",
    "        'reg_lambda': '',\n",
    "        'reg_alpha': ''\n",
    "        The rest are summed up as is\n",
    "        \"\"\"\n",
    "        median_params = np.median(population[:,:self.N_params],axis=0)\n",
    "        difference = population[:,:self.N_params] - median_params.reshape(1,self.N_params)\n",
    "        difference -= self.diversity_subtract\n",
    "        difference /= self.diversity_discount\n",
    "        diversity_scores = np.sum(difference,axis=1)\n",
    "        population[:,self.diversity_col] = np.abs(diversity_scores)\n",
    "        return population\n",
    "    \n",
    "    def return_combined_score(self,population):\n",
    "        \"\"\"\n",
    "        combine diversity and score to get a combined metric\n",
    "        \"\"\"\n",
    "        combined = population[:,self.score_col] - population[:,self.diversity_col]\n",
    "        population[:,self.combined_col] = combined\n",
    "        return population\n",
    "            \n",
    "    def train(self,num_rounds):\n",
    "        for i in range(1,num_rounds+1):\n",
    "            self.step(i)\n",
    "    \n",
    "    def step(self,index):\n",
    "        \"\"\"\n",
    "        could sample with probability of priority as given by the score\n",
    "        \n",
    "        Take the tau top performing candidates. There should also a term that values uniqueness in terms of params.\n",
    "        \"\"\"\n",
    "        print('Step: ',index)\n",
    "        for island in range(0,self.N_islands):\n",
    "            scores = self.population_dict[island][:,self.metrics[island]]\n",
    "            scores = np.sort(scores)\n",
    "            cutoff = scores[-self.tau]\n",
    "            elite_cutoff = scores[self.tau]\n",
    "            print(scores,cutoff)\n",
    "            print('first score',np.min(scores))\n",
    "            print('last score',np.max(scores))\n",
    "            diversity = self.population_dict[island][:,self.diversity_col]\n",
    "            diversity = np.sort(diversity)\n",
    "            replaced_mask = np.where(self.population_dict[island][:,self.metrics[island]] >= cutoff)[0]\n",
    "            elite_mask = np.where(self.population_dict[island][:,self.metrics[island]] < cutoff)[0] # can use elite cutoff to restrict who propogates into the next generation\n",
    "            for idx in replaced_mask:\n",
    "                self.population_dict[island][idx,:] = self.recombine(self.population_dict[island],self.population_dict[island][idx,:],elite_mask)\n",
    "    #             func = choice([self.mutate,self.scale_param])\n",
    "                self.population_dict[island][idx,:] = self.mutate(self.population_dict[island][idx,:])\n",
    "                self.population_dict[island][idx,:] = self.mutate(self.population_dict[island][idx,:])\n",
    "#                 self.population_dict[island][idx,:] = self.scale_param(self.population_dict[island][idx,:])\n",
    "            self.population_dict[island][replaced_mask] = self.return_fitness(self.population_dict[island][replaced_mask])\n",
    "            self.population_dict[island][replaced_mask] = self.return_diversity(self.population_dict[island][replaced_mask])\n",
    "            self.population_dict[island][replaced_mask] = self.return_combined_score(self.population_dict[island][replaced_mask])\n",
    "            \n",
    "            self.scores[island][index] = copy.copy(self.population_dict[island][:,self.score_col])\n",
    "            self.combined_metric[island][index] = copy.copy(self.population_dict[island][:,self.combined_col])\n",
    "            self.historical_params[island][index] = copy.copy(self.population_dict[island])\n",
    "        \n",
    "    ### Mutation Methods ###\n",
    "    \n",
    "    def mutate(self,target):\n",
    "        key = choice(list(self.param_dict.keys()))\n",
    "        value = self.param_dict[key]()\n",
    "        index = self.param_index[key]\n",
    "        target[index] = value\n",
    "        return target\n",
    "        \n",
    "    def scale_param(self,target):\n",
    "        \"\"\"\n",
    "        scale value to search locally. Must be constrained between the min and max values however.\n",
    "        \"\"\"\n",
    "        key = choice(list(self.param_dict.keys()))\n",
    "        index = self.param_index[key]\n",
    "        value = np.random.random() * 0.4 + 0.8 # value between 0.8 - 1.2\n",
    "        target[index] *= value\n",
    "        constraint = self.param_constraints[key][0]\n",
    "        low = constraint[0]\n",
    "        high = constraint[1]\n",
    "        dtype = self.param_constraints[key][1]\n",
    "        target[index] = dtype(min(max(target[index],low),high))\n",
    "        return target\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_parents(population,elite_mask):\n",
    "        \"\"\"Get two random parents from a population.\n",
    "        :return (Individual, Individual): Two random parents.\n",
    "        \"\"\"\n",
    "        subgroup = population[elite_mask]\n",
    "        split = choice(range(1, subgroup.shape[0] - 1))\n",
    "        return choice(subgroup[:split]), choice(subgroup[split:])\n",
    "        \n",
    "        \n",
    "    def recombine(self,population,target,elite_mask):\n",
    "        parent_one,parent_two = PBTV2.random_parents(population,elite_mask)\n",
    "        # randomly selects a gene from one\n",
    "        np.concatenate([parent_one[:self.N_params],parent_two[:self.N_params]])\n",
    "        for index in range(self.N_params):\n",
    "            gene_one,gene_two = parent_one[index],parent_two[index]\n",
    "            gene = np.random.choice([gene_one,gene_two])\n",
    "            target[index] = gene\n",
    "        # Splits the genes and copy\n",
    "#         divide = np.random.choice(list(range(self.N_params))) # for splitting the two.\n",
    "#         genes_one = parent_one[:divide]\n",
    "#         genes_two = parent_two[divide:self.N_params]\n",
    "#         target[:self.N_params] = np.concatenate([genes_one,genes_two])\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoostTrain(X_train, X_valid, y_train, y_valid,model):\n",
    "    eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "    eval_metric = [\"rmse\"]\n",
    "    model.fit(X_train, y_train,eval_metric=eval_metric, eval_set=eval_set, verbose=False)\n",
    "    print(model)\n",
    "    # return_accuracy(model,X_valid,y_valid)\n",
    "    return model\n",
    "\n",
    "def outside_fitness(params,load=True):\n",
    "    model_type = XGBClassifier\n",
    "    model = outside_return_model(params,model_type)\n",
    "    # Load data from trainer if applicable.\n",
    "    if load == True:\n",
    "        losses = []\n",
    "        trainset = data['trainset']\n",
    "        valset = data['valset']\n",
    "        # Sample N times from trainloader\n",
    "        N = len(trainset)\n",
    "        V = len(valset)\n",
    "        N_index = np.random.choice(N,50)\n",
    "        V_index = np.random.choice(V,10)\n",
    "        inputs, labels = trainset[N_index].values()\n",
    "        V_inputs, V_labels = valset[V_index].values()\n",
    "        model = XGBoostTrain(inputs, V_inputs, labels, V_labels, model)\n",
    "        y_pred = model.predict(V_labels)\n",
    "        batch_loss = np.mean(abs(V_labels - y_pred))\n",
    "        loss = np.mean(losses)\n",
    "    else:\n",
    "        model = XGBoostTrain(data['X_train'], data['X_valid'], data['y_train'], data['y_valid'], model)\n",
    "        y_pred = model.predict(data['X_valid'])\n",
    "        loss = np.mean(abs(data['y_valid'] - y_pred))\n",
    "    print('Mean Absolute Error: ',loss)\n",
    "    return loss\n",
    "\n",
    "def outside_return_model(params,model_type):\n",
    "    learning_rate,max_depth,min_child_weight,gamma,subsample,colsample_bytree,L2,L1 = params\n",
    "    model = model_type(num_round = 30,\n",
    "    n_estimators=1000,\n",
    "    objective='reg:squarederror',\n",
    "    scale_pos_weight=1,\n",
    "    learning_rate = learning_rate,\n",
    "    max_depth=int(max_depth),\n",
    "    min_child_weight=int(min_child_weight),\n",
    "    gamma=gamma,\n",
    "    subsample=subsample,\n",
    "    colsample_bytree=colsample_bytree,\n",
    "    reg_lambda=L2,\n",
    "    reg_alpha=L1,\n",
    "    seed=27)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_list [array([0.15088505, 3.        , 9.        , 7.0934881 , 0.97707955,\n",
      "       0.79813886, 0.10524555, 0.07279346]), array([0.0410704 , 1.        , 5.        , 6.70459189, 0.9586591 ,\n",
      "       0.75919257, 0.21721193, 0.09741529]), array([0.09798658, 4.        , 4.        , 7.26248433, 0.61444035,\n",
      "       0.95284021, 0.16355721, 0.33061735]), array([0.02369729, 2.        , 5.        , 6.9139313 , 0.92573661,\n",
      "       0.59141349, 0.24894362, 0.23613034]), array([0.12769532, 2.        , 5.        , 3.71736133, 0.61411493,\n",
      "       0.63430885, 0.01787642, 0.25270027])]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.7591925738185055,\n",
      "       gamma=6.704591893961149, learning_rate=0.041070404460732644,\n",
      "       max_delta_step=0, max_depth=1, min_child_weight=5, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None, num_round=30,\n",
      "       objective='reg:squarederror', random_state=0,\n",
      "       reg_alpha=0.09741529230153484, reg_lambda=0.21721192748319107,\n",
      "       scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.9586591044522006, verbosity=1)XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.5914134870628314,\n",
      "       gamma=6.9139312971285465, learning_rate=0.023697289380071974,\n",
      "       max_delta_step=0, max_depth=2, min_child_weight=5, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None, num_round=30,\n",
      "       objective='reg:squarederror', random_state=0,\n",
      "       reg_alpha=0.23613034280876477, reg_lambda=0.2489436194812737,\n",
      "       scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.9257366133345744, verbosity=1)XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.952840211614948,\n",
      "       gamma=7.26248432589282, learning_rate=0.09798658261467637,\n",
      "       max_delta_step=0, max_depth=4, min_child_weight=4, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None, num_round=30,\n",
      "       objective='reg:squarederror', random_state=0,\n",
      "       reg_alpha=0.3306173483447567, reg_lambda=0.1635572121634074,\n",
      "       scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.6144403537780891, verbosity=1)\n",
      "\n",
      "\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.7981388623603345,\n",
      "       gamma=7.093488100762436, learning_rate=0.15088505315951117,\n",
      "       max_delta_step=0, max_depth=3, min_child_weight=9, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None, num_round=30,\n",
      "       objective='reg:squarederror', random_state=0,\n",
      "       reg_alpha=0.07279345511085938, reg_lambda=0.10524554500006411,\n",
      "       scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.9770795469318961, verbosity=1)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.6343088457186776,\n",
      "       gamma=3.7173613258320617, learning_rate=0.1276953243122071,\n",
      "       max_delta_step=0, max_depth=2, min_child_weight=5, missing=None,\n",
      "       n_estimators=1000, n_jobs=1, nthread=None, num_round=30,\n",
      "       objective='reg:squarederror', random_state=0,\n",
      "       reg_alpha=0.2527002702592947, reg_lambda=0.01787641579116933,\n",
      "       scale_pos_weight=1, seed=27, silent=None,\n",
      "       subsample=0.614114933700929, verbosity=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input numpy.ndarray must be 2 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/anaconda3/envs/data-analysis/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/anaconda3/envs/data-analysis/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-331-f9e3f18eb07a>\", line 25, in outside_fitness\n    y_pred = model.predict(V_labels)\n  File \"/anaconda3/envs/data-analysis/lib/python3.7/site-packages/xgboost/sklearn.py\", line 785, in predict\n    test_dmatrix = DMatrix(data, missing=self.missing, nthread=self.n_jobs)\n  File \"/anaconda3/envs/data-analysis/lib/python3.7/site-packages/xgboost/core.py\", line 404, in __init__\n    self._init_from_npy2d(data, missing, nthread)\n  File \"/anaconda3/envs/data-analysis/lib/python3.7/site-packages/xgboost/core.py\", line 474, in _init_from_npy2d\n    raise ValueError('Input numpy.ndarray must be 2 dimensional')\nValueError: Input numpy.ndarray must be 2 dimensional\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-332-22fc6911e63c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mpopulationV2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPBTV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattribute_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-330-0a10eb33c0b6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, boost_params, attribute_dict)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Now add score and diversity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#         self.linear_return_fitness(self.individuals)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_diversity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_combined_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulation_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misland\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-330-0a10eb33c0b6>\u001b[0m in \u001b[0;36mreturn_fitness\u001b[0;34m(self, population)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'param_list'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutside_fitness\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/data-analysis/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         '''\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/data-analysis/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input numpy.ndarray must be 2 dimensional"
     ]
    }
   ],
   "source": [
    "mu = 5\n",
    "tau = 2\n",
    "seed = 28\n",
    "islands = 1\n",
    "N_params = 8\n",
    "target_index = 0 # 0 = spread, 1 = total\n",
    "attribute_dict = {'mu':mu,\n",
    "                 'tau':tau,\n",
    "                 'seed':seed,\n",
    "                 'metrics':[N_params,N_params + 2],\n",
    "                  'model_type':XGBClassifier,\n",
    "                  'islands':islands,\n",
    "                 'param_dict' :{\n",
    "                'learning_rate' : lambda: np.random.sample() * 0.199 + 0.001,\n",
    "                'min_child_weight': lambda:int(np.random.sample() * 4 + 1),\n",
    "                'max_depth': lambda:int(np.random.sample() * 7 + 3),\n",
    "                'gamma': lambda:np.random.sample() * 7 + 3,\n",
    "                'subsample': lambda:np.random.sample() * 0.5 + 0.5,\n",
    "                'colsample_bytree': lambda:np.random.sample() * 0.5 + 0.5,\n",
    "                'reg_lambda': lambda:np.random.sample() * 0.5,\n",
    "                'reg_alpha': lambda:np.random.sample() * 0.5,\n",
    "                },\n",
    "                # For scaling params (some params don't scale)\n",
    "                'param_constraints' :{\n",
    "                'learning_rate' : ([0,0.5],float),\n",
    "                'min_child_weight': ([0,5],int),\n",
    "                'max_depth': ([3,10],int),\n",
    "                'gamma': ([3,10],float),\n",
    "                'subsample': ([0.5,1],float),\n",
    "                'colsample_bytree': ([0.5,1],float),\n",
    "                'reg_lambda': ([0,0.5],float),\n",
    "                'reg_alpha': ([0,0.5],float),\n",
    "                },\n",
    "                'set_dict':{'num_round':30,'n_estimators':1000,'scale_pos_weight':1,'objective':'reg:squarederror'}}\n",
    "params = {\n",
    "     'learning_rate' :0.01,\n",
    "     'max_depth':10,\n",
    "     'min_child_weight':1,\n",
    "     'gamma':0,\n",
    "     'subsample':0.8,\n",
    "     'colsample_bytree':0.8,\n",
    "     'scale_pos_weight':1,\n",
    "      'reg_lambda':1,\n",
    "      'reg_alpha':0,\n",
    "     'objective':'reg:squarederror',\n",
    "    'num_round': 30,\n",
    "     'n_estimators':1000}\n",
    "\n",
    "data = {\n",
    "    'trainset':trainset,\n",
    "    'valset':valset,\n",
    "    'testset':testset\n",
    "}\n",
    "# data = {\n",
    "#     'X_train':X_train,\n",
    "#     'X_valid':X_valid,\n",
    "#     'y_train':y_train,\n",
    "#     'y_valid':y_valid,\n",
    "#     'X':X\n",
    "# }\n",
    "\n",
    "populationV2 = PBTV2(params,attribute_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_scatter(X,colors):\n",
    "    num_classes = len(np.unique(colors))\n",
    "    palette = np.array(sns.color_palette(\"hls\", num_classes))\n",
    "\n",
    "    # create a scatter plot.\n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "    ax = plt.subplot(aspect='equal')\n",
    "    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)])\n",
    "    plt.xlim(-25, 25)\n",
    "    plt.ylim(-25, 25)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "\n",
    "    # add the labels for each digit corresponding to the label\n",
    "    txts = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "\n",
    "        # Position of each label at median of data points.\n",
    "\n",
    "        xtext, ytext = np.median(x[colors == i, :], axis=0)\n",
    "        txt = ax.text(xtext, ytext, str(i), fontsize=24)\n",
    "        txt.set_path_effects([\n",
    "            PathEffects.Stroke(linewidth=5, foreground=\"w\"),\n",
    "            PathEffects.Normal()])\n",
    "        txts.append(txt)\n",
    "\n",
    "    return f, ax, sc, txts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
