{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D4PG\n",
    "\n",
    "DDPG with \n",
    "- dueling distributional critic network. \n",
    "- Prioritized replay "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import copy\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(object):\n",
    "    def __init__(self, dimension, num_epochs, mu=0.0, var=1):\n",
    "        self.mu = mu\n",
    "        self.var = var\n",
    "        self.dimension = dimension\n",
    "        self.epochs = 0\n",
    "        self.num_epochs = num_epochs\n",
    "        self.min_epsilon = 0.01 # minimum exploration probability\n",
    "        self.epsilon = 0.3\n",
    "        self.decay_rate = 5.0/num_epochs # exponential decay rate for exploration prob\n",
    "        self.iter = 0\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.epsilon * np.random.normal(self.mu, self.var, size=self.dimension)\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.epsilon = self.min_epsilon + (1.0 - self.min_epsilon)*np.exp(-self.decay_rate*self.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveParamNoise(object):\n",
    "    def __init__(self, initial_stddev=0.1,desired_action_stddev=0.2,adaptation_coefficient=1.01):\n",
    "\n",
    "        self.initial_stddev = initial_stddev\n",
    "        self.desired_action_stddev = desired_action_stddev\n",
    "        self.adaptation_coefficient = adaptation_coefficient\n",
    "\n",
    "        self.current_stddev = initial_stddev\n",
    "\n",
    "    def adapt(self,distance):\n",
    "        if distance > self.desired_action_stddev:\n",
    "            # Decrease stddev\n",
    "            self.current_stddev /= self.adaptation_coefficient\n",
    "        else:\n",
    "            # Increase stddev\n",
    "            self.current_stddev *= self.adaptation_coefficient\n",
    "\n",
    "    def get_stats(self):\n",
    "        stats = {\n",
    "            'param_noise_stddev':self.current_stddev,\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    def __repr__(self):\n",
    "        fmt = \"AdaptiveNoiseParam(initial_stddev={},desired_action_stddev={},adaptation_coefficient={})\"\n",
    "        return fmt.format(self.initial_stddev,self.desired_action_stddev,self.adaptation_coefficient)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "def hard_update(source,target):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(256,128)):\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        \n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.hidden_layers.append(nn.Linear(hidden_dims[0]+nA,hidden_dims[1]))\n",
    "        for i in range(1,len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        # self.fc1 = nn.Linear(hidden_dims[0]+nA,hidden_dims[1])\n",
    "        # self.fc1_bn = nn.BatchNorm1d(hidden_dims[1])\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1],1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.input_layer.weight.data.uniform_(*hidden_init(self.input_layer))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.weight.data.uniform_(*hidden_init(hidden_layer))\n",
    "        self.output_layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,obs,action):\n",
    "        # With batchnorm\n",
    "        # xs = self.input_bn(F.relu(self.input_layer(state)))\n",
    "        # x = torch.cat((xs,action),dim=1)\n",
    "        # x = self.fc1_bn(F.relu(self.fc1(x)))\n",
    "        assert isinstance(obs,torch.Tensor)\n",
    "        xs = F.relu(self.input_layer(obs))\n",
    "        x = torch.cat((xs, action), dim=-1)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self,seed,nS,nA,hidden_dims=(256,128)):\n",
    "        super(Actor,self).__init__()\n",
    "        \n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.std = nn.Parameter(torch.zeros(1, nA))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.input_layer = nn.Linear(nS,hidden_dims[0])\n",
    "        self.fc1 = nn.Linear(hidden_dims[0],hidden_dims[1])\n",
    "        self.output_layer = nn.Linear(hidden_dims[1],nA)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.input_layer.weight.data.uniform_(*hidden_init(self.input_layer))\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.output_layer.weight.data.uniform_(-3e-3,3e-3)\n",
    "        \n",
    "    def forward(self,state):\n",
    "        assert isinstance(state,torch.Tensor)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.tanh(self.output_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priority Experience Replay (PER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Tree.\n",
    "3 tiered tree structure containing\n",
    "Root node (Object. sum of all lower values)\n",
    "Intermediate Node (Object. Root as parent, sums a given slice of the priority array)\n",
    "Priority Array (Array of priorities, length buffer_size)\n",
    "\n",
    "The number of Intermediate nodes is calculated by the buffer_size / batch_size.\n",
    "\n",
    "I_episode: current episode of training\n",
    "\n",
    "Index: is calculated by i_episode % buffer_size. This loops the index after exceeding the buffer_size.\n",
    "\n",
    "Indicies: (List) of memory/priority entries\n",
    "\n",
    "intermediate_dict: maps index to intermediate node. Since each Intermediate node is responsible \n",
    "for a given slice of the priority array, given a particular index, it will return the Intermediate node\n",
    "'responsible' for that index.\n",
    "\n",
    "## Functions:\n",
    "\n",
    "Add:\n",
    "Calculates the priority of each TD error -> (abs(TD_error)+epsilon)**alpha\n",
    "Stores the priority in the Priority_array.\n",
    "Updates the sum_tree with the new priority\n",
    "\n",
    "Update_Priorities:\n",
    "Updates the index with the latest priority of that sample. As priorities can change over training\n",
    "for a particular experience\n",
    "\n",
    "Sample:\n",
    "Splits the current priority_array based on the number of entries, by the batch_size.\n",
    "Returns the indicies of those samples and the priorities.\n",
    "\n",
    "Propogate:\n",
    "Propogates the new priority value up through the tree\n",
    "\"\"\"\n",
    "\n",
    "class PriorityTree(object):\n",
    "    def __init__(self,buffer_size,batch_size,alpha,epsilon):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_indicies = np.arange(0,self.batch_size)\n",
    "\n",
    "        self.num_intermediate_nodes = math.ceil(buffer_size / batch_size)\n",
    "        self.current_intermediate_node = 0\n",
    "        self.root = Node(None)\n",
    "        self.intermediate_nodes = [Intermediate(self.root,batch_size*x,batch_size*(x+1)) for x in range(self.num_intermediate_nodes)]\n",
    "        self.priority_array = np.zeros(buffer_size)\n",
    "        self.intermediate_dict = {}\n",
    "        for index,node in enumerate(self.intermediate_nodes):\n",
    "            for key in range((batch_size*(index+1))-batch_size,batch_size*(index+1)):\n",
    "                self.intermediate_dict[key] = node\n",
    "        print('Priority Tree: Batch Size {} Buffer size {} Number of intermediate Nodes {}'.format(batch_size,buffer_size,self.num_intermediate_nodes))\n",
    "        \n",
    "    def add(self,TD_error,index):\n",
    "        priority = (abs(TD_error)+self.epsilon)**self.alpha\n",
    "        self.priority_array[index] = priority\n",
    "        # Update sum\n",
    "        propogate(self.intermediate_dict[index],self.priority_array)\n",
    "    \n",
    "    def sample(self,index,limit):\n",
    "        # Sample one experience uniformly from each slice of the priorities\n",
    "        # if index >= self.buffer_size:\n",
    "        #     indicies = [random.sample(list(range(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes)),1)[0] for sample in range(self.batch_size)]\n",
    "        #     # indicies = np.random.sample(np.arange(sample*self.num_intermediate_nodes,(sample+1)*self.num_intermediate_nodes))\n",
    "        # else:\n",
    "        spacing = np.linspace(0,limit-self.batch_size,self.batch_size,dtype=np.int)\n",
    "        random_indicies = np.random.choice(self.batch_indicies,size=self.batch_size)\n",
    "        indicies = random_indicies + spacing\n",
    "\n",
    "\n",
    "        # interval = int(index / self.batch_size)\n",
    "        # indicies = [random.sample(list(range(sample*interval,(sample+1)*interval)),1)[0] for sample in range(self.batch_size)]\n",
    "#         print('indicies',indicies)\n",
    "        priorities = self.priority_array[indicies]\n",
    "        return priorities,indicies\n",
    "    \n",
    "    def update_priorities(self,TD_errors,indicies):\n",
    "#         print('TD_errors',TD_errors)\n",
    "#         print('TD_errors shape',TD_errors.shape)\n",
    "        priorities = (np.abs(TD_errors)+self.epsilon)**self.alpha\n",
    "#         print('priorities shape',priorities.shape)\n",
    "#         print('indicies shape',len(indicies))\n",
    "#         print('self.priority_array shape',self.priority_array.shape)\n",
    "        self.priority_array[indicies] = priorities\n",
    "        # Update sum\n",
    "        nodes = [self.intermediate_dict[index] for index in indicies] \n",
    "        intermediate_nodes = set(nodes)\n",
    "        [propogate(node,self.priority_array) for node in intermediate_nodes]\n",
    "    \n",
    "class Node(object):\n",
    "    def __init__(self,parent):\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.value = 0\n",
    "            \n",
    "    def add_child(self,child):\n",
    "        self.children.append(child)\n",
    "    \n",
    "    def set_value(self,value):\n",
    "        self.value = value\n",
    "    \n",
    "    def sum_children(self):\n",
    "        return sum([child.value for child in self.children])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.children)\n",
    "\n",
    "class Intermediate(Node):\n",
    "    def __init__(self,parent,start,end):\n",
    "        self.parent = parent\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.value = 0\n",
    "        parent.add_child(self)\n",
    "    \n",
    "    def sum_leafs(self,arr):\n",
    "        return np.sum(arr[self.start:self.end])\n",
    "\n",
    "def propogate(node,arr):\n",
    "    if node.parent != None:\n",
    "        node.value = node.sum_leafs(arr)\n",
    "        propogate(node.parent,arr)\n",
    "    else:\n",
    "        node.value = node.sum_children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Priority Buffer HyperParameters\n",
    "alpha(priority or w) dictates how biased the sampling should be towards the TD error. 0 < a < 1\n",
    "beta(IS) informs the importance of the sample update\n",
    "\n",
    "The paper uses a sum tree to calculate the priority sum in O(log n) time. As such, i've implemented my own version\n",
    "of the sum_tree which i call priority tree.\n",
    "\n",
    "We're increasing beta(IS) from 0.5 to 1 over time\n",
    "alpha(priority) we're holding constant at 0.5\n",
    "\"\"\"\n",
    "\n",
    "class PriorityReplayBuffer(object):\n",
    "    def __init__(self,buffer_size,batch_size,seed,alpha=0.5,beta=0.5,beta_end=1,beta_duration=1e+5,epsilon=7e-5,device=None):\n",
    "        \n",
    "        self.seed = random.seed(seed)\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_end = beta_end\n",
    "        self.beta_duration = beta_duration\n",
    "        self.beta_increment = (beta_end - beta) / beta_duration\n",
    "        self.max_w = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.TD_sum = 0\n",
    "        self.index = 0\n",
    "        if device == None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        self.experience = namedtuple('experience',field_names=['state','action','reward','next_state','done'])\n",
    "        self.sum_tree = PriorityTree(buffer_size,batch_size,alpha,epsilon)\n",
    "        self.memory = {}\n",
    "    \n",
    "    def add(self,state,action,reward,next_state,done,TD_error):\n",
    "        e = self.experience(state,action,reward,next_state,done)\n",
    "        # add memory to memory and add corresponding priority to the priority tree\n",
    "        self.memory[self.index] = e\n",
    "        self.sum_tree.add(TD_error,self.index)\n",
    "        self.index = (self.index + 1) % self.buffer_size \n",
    "\n",
    "    def sample(self):\n",
    "        # We times the error by these weights for the updates\n",
    "        # Super inefficient to sum everytime. We could implement the tree sum structure. \n",
    "        # Or we could sum once on the first sample and then keep track of what we add and lose from the buffer.\n",
    "        # priority^a over the sum of the priorities^a = likelyhood of the given choice\n",
    "        # Anneal beta\n",
    "        self.update_beta()\n",
    "        # Get the samples and indicies\n",
    "        priorities,indicies = self.sum_tree.sample(self.index,len(self))\n",
    "        # Normalize with the sum\n",
    "        norm_priorities = priorities / self.sum_tree.root.value\n",
    "        samples = [self.memory[index] for index in indicies]\n",
    "        # Importance weights\n",
    "        importances = [(priority * self.buffer_size)**-self.beta for priority in norm_priorities]\n",
    "        self.max_w = max(self.max_w,max(importances))\n",
    "        # Normalize importance weights\n",
    "#         print('importances',importances)\n",
    "#         print('self.max_w',self.max_w)\n",
    "        norm_importances = [importance / self.max_w for importance in importances]\n",
    "#         print('norm_importances',norm_importances)\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        states = torch.stack(states).float().to(self.device)\n",
    "        actions = torch.stack(actions).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n",
    "        next_states = torch.stack(next_states).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack(dones)).float().to(self.device)\n",
    "\n",
    "        # states = torch.from_numpy(np.vstack([e.state for e in samples if e is not None])).float().to(self.device)\n",
    "        # actions = torch.from_numpy(np.vstack([e.action for e in samples if e is not None])).float().to(self.device)\n",
    "        # rewards = torch.from_numpy(np.vstack([e.reward for e in samples if e is not None])).float().to(self.device)\n",
    "        # next_states = torch.from_numpy(np.vstack([e.next_state for e in samples if e is not None])).float().to(self.device)\n",
    "        # dones = torch.from_numpy(np.vstack([e.done for e in samples if e is not None]).astype(int)).float().to(self.device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones),indicies,norm_importances\n",
    "\n",
    "    def update_beta(self):\n",
    "        self.beta += self.beta_increment\n",
    "        self.beta = min(self.beta,self.beta_end)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, device, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size: maximum size of buffer\n",
    "            batch_size: size of each training batch\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\n",
    "            \"Experience\",\n",
    "            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "            )\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        samples = random.sample(self.memory, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*samples)\n",
    "\n",
    "        states = torch.tensor(states).float().to(self.device)\n",
    "        actions = torch.tensor(actions).float().to(self.device)\n",
    "        rewards = torch.tensor(rewards).float().to(self.device)\n",
    "        next_states = torch.tensor(next_states).float().to(self.device)\n",
    "        dones = torch.tensor(dones).float().to(self.device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Config file for loading hyperparams\n",
    "\"\"\"\n",
    "\n",
    "class Config(object):\n",
    "    def __init__(self,agent):\n",
    "        if agent == \"d4pg\":\n",
    "            self.seed = 99\n",
    "            self.name = agent\n",
    "            self.num_agents = 2\n",
    "            self.QLR = 0.001\n",
    "            self.ALR = 0.0001\n",
    "            self.gamma = 0.99\n",
    "            self.L2 = 0 # 0.1\n",
    "            self.tau=0.01 # 0.001\n",
    "            self.noise_decay=0.995\n",
    "            self.gae_lambda = 0.97\n",
    "            self.clip_norm = 10\n",
    "            # Buffer\n",
    "            self.buffer_size = int(1e4)\n",
    "            self.min_buffer_size = int(1e3)\n",
    "            self.batch_size = 256\n",
    "            # Priority Replay\n",
    "            self.ALPHA = 0.6 # 0.7 or 0.6\n",
    "            self.START_BETA = 0.5 # from 0.5-1\n",
    "            self.END_BETA = 1\n",
    "            # distributional\n",
    "            self.N_atoms = 51\n",
    "            self.v_min = -100\n",
    "            self.v_max = 100\n",
    "            self.delta_z = (self.v_min - self.v_max) / (self.N_atoms - 1)\n",
    "            # pendulum\n",
    "            self.action_low=-1.0 \n",
    "            self.action_high=1.0\n",
    "            self.winning_condition = -200\n",
    "            # Training\n",
    "            self.episodes = 4000\n",
    "            self.tmax = 2000\n",
    "            self.print_every = 4\n",
    "            self.SGD_epoch = 1\n",
    "            self.checkpoint_path = 'model_weights/ddpg.ckpt'\n",
    "        else:\n",
    "            raise ValueError('Agent not implemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4PG(object):\n",
    "    def __init__(self, nS, nA,config):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.action_low = config.action_low\n",
    "        self.action_high = config.action_high\n",
    "        self.seed = config.seed\n",
    "\n",
    "        self.clip_norm = config.clip_norm\n",
    "        self.tau = config.tau\n",
    "        self.gamma = config.gamma\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.L2 = config.L2\n",
    "        self.SGD_epoch = config.SGD_epoch\n",
    "        \n",
    "        # For distributional\n",
    "        self.N_atoms = config.N_atoms # 51 for C51\n",
    "        self.v_max = config.v_max # Max possible score\n",
    "        self.v_min = config.v_min # Min possible score\n",
    "        self.delta_z = (self.v_max - self.v_min) / float(self.N_atoms - 1)\n",
    "        self.atoms = np.linspace(self.v_min,self.v_max,self.N_atoms)\n",
    "        \n",
    "        # noise\n",
    "        self.noise = OUNoise(nA,config.seed)\n",
    "        self.noise_scale = 1.0\n",
    "        self.noise_decay = config.noise_decay\n",
    "\n",
    "        # Priority Replay Buffer\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer_size = config.buffer_size\n",
    "        self.alpha = config.ALPHA\n",
    "        self.beta = self.start_beta = config.START_BETA\n",
    "        self.end_beta = config.END_BETA\n",
    "\n",
    "        # actors networks\n",
    "        self.actor = Actor(self.seed,nS, nA).to(self.device)\n",
    "        self.actor_target = Actor(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # Param noise\n",
    "        self.param_noise = AdaptiveParamNoise()\n",
    "        self.actor_perturbed = Actor(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # critic networks\n",
    "        self.critic = Critic(self.seed,nS, nA).to(self.device)\n",
    "        self.critic_target = Critic(self.seed,nS, nA).to(self.device)\n",
    "\n",
    "        # Copy the weights from local to target\n",
    "        hard_update(self.critic,self.critic_target)\n",
    "        hard_update(self.actor,self.actor_target)\n",
    "\n",
    "        # optimizer\n",
    "        self.actor_opt = optim.Adam(self.actor.parameters(), lr=1e-4, weight_decay=self.L2)\n",
    "        self.critic_opt = optim.Adam(self.critic.parameters(), lr=1e-3, weight_decay=self.L2)\n",
    "\n",
    "        # replay buffer\n",
    "        self.PER = PriorityReplayBuffer(self.buffer_size, self.batch_size,self.seed,alpha=self.alpha,device=self.device)\n",
    "\n",
    "        # reset agent for training\n",
    "        self.reset_episode()\n",
    "        self.it = 0\n",
    "\n",
    "    def save_weights(self,path):\n",
    "        params = {}\n",
    "        params['actor'] = self.actor.state_dict()\n",
    "        params['critic'] = self.critic.state_dict()\n",
    "        torch.save(params, path)\n",
    "\n",
    "    def load_weights(self,path):\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor.load_state_dict(checkpoint['actor'])\n",
    "        self.actor_target.load_state_dict(checkpoint['actor'])\n",
    "        self.critic.load_state_dict(checkpoint['critic'])\n",
    "        self.critic_target.load_state_dict(checkpoint['critic'])\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def ddpg_distance_metric(actions1,actions2):\n",
    "        \"\"\"\n",
    "        Computes distance between actions taken by two different policies\n",
    "        Expects numpy arrays\n",
    "        \"\"\"\n",
    "        diff = actions1-actions2\n",
    "        mean_diff = np.mean(np.square(diff),axis=0)\n",
    "        dist = sqrt(np.mean(mean_diff))\n",
    "        return dist\n",
    "\n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(self.tensor(state)).cpu().numpy()\n",
    "        action += self.noise.sample() * self.noise_scale\n",
    "        self.actor.train()\n",
    "        return np.clip(action, self.action_low, self.action_high)\n",
    "\n",
    "    def act_perturbed(self,state):\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_perturbed(self.tensor(state)).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def perturbed_update(self):\n",
    "        hard_update(self.actor,self.actor_perturbed)\n",
    "        params = self.actor_perturbed.state_dict()\n",
    "        for name in params:\n",
    "            if 'ln' in name:\n",
    "                pass\n",
    "            param = params[name]\n",
    "            random = torch.randn(param.shape).to(self.device)\n",
    "            param += random * self.param_noise.current_stddev\n",
    "            \n",
    "\n",
    "    def evaluate(self,state):\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(self.tensor(state)).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    def step(self, obs, actions, rewards, next_obs, dones):\n",
    "        # Step noise\n",
    "        self.noise_scale = max(self.noise_scale * self.noise_decay, 0.01)\n",
    "        # cast as torch tensors\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(self.device)\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "        # Calc TD error\n",
    "        next_action = self.actor(next_obs)\n",
    "        next_value = self.critic_target(next_obs,next_action)\n",
    "        target = rewards + self.gamma * next_value * dones\n",
    "        local = self.critic(obs,actions)\n",
    "        TD_error = (target - local).squeeze(0)\n",
    "        self.PER.add(obs, actions, rewards, next_obs, dones, TD_error)\n",
    "        for _ in range(self.SGD_epoch):\n",
    "            samples,indicies,importances = self.PER.sample()\n",
    "            self.learn(samples,indicies,importances)\n",
    "\n",
    "    def add_replay_warmup(self,obs,actions,rewards,next_obs,dones):\n",
    "        next_obs = torch.from_numpy(next_obs).float().to(self.device)\n",
    "        obs = torch.from_numpy(obs).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).float().to(self.device)\n",
    "#         dones = torch.from_numpy(dones).float().to(self.device)\n",
    "#         rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        # Calculate TD_error\n",
    "        next_action = self.actor(next_obs)\n",
    "        next_value = self.critic_target(next_obs,next_action)\n",
    "        target = rewards + self.gamma * next_value * dones\n",
    "        local = self.critic(obs,actions)\n",
    "        TD_error = (target - local).squeeze(0)\n",
    "        self.PER.add(obs,actions,np.max(rewards),next_obs,np.max(dones),TD_error)\n",
    "\n",
    "    def learn(self,samples,indicies,importances):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = samples\n",
    "\n",
    "        with torch.no_grad():\n",
    "              target_actions = self.actor_target(next_states)\n",
    "        next_values = self.critic_target(next_states,target_actions)\n",
    "        y_target = rewards + self.gamma * next_values * (1-dones)\n",
    "        y_current = self.critic(states, actions)\n",
    "        TD_error = y_current - y_target\n",
    "        # update critic\n",
    "        critic_loss = ((torch.tensor(importances).to(self.device)*TD_error)**2).mean()\n",
    "        self.critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(self.critic.parameters(),self.clip_norm)\n",
    "        self.critic_opt.step()\n",
    "\n",
    "        # update actor\n",
    "        local_actions = self.actor(states)\n",
    "        actor_loss = -self.critic(states, local_actions).mean()\n",
    "        self.actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(),self.clip_norm)\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # Update PER\n",
    "        TD_errors = TD_error.squeeze(1).detach().cpu().numpy()\n",
    "        self.PER.sum_tree.update_priorities(TD_errors,indicies)\n",
    "\n",
    "        # soft update networks\n",
    "        self.soft_update()\n",
    "\n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target network\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        \"\"\"\n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data+(1-self.tau)*target_param.data)\n",
    "\n",
    "    def tensor(self, x):\n",
    "        return torch.from_numpy(x).float().to(self.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_replay_buffer(agent,env,min_buffer_size):\n",
    "    state = env.reset()\n",
    "    while len(agent.PER) < min_buffer_size:\n",
    "        # Random actions between 1 and -1\n",
    "        action = ((np.random.rand(1)*2)-1)\n",
    "        next_state,reward,done,_ = env.step(action)\n",
    "        # reshape\n",
    "        agent.add_replay_warmup(state,action,reward,next_state,done)\n",
    "        # Store experience\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        state = next_state\n",
    "    print('finished replay warm up')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,epsilon=1,noise_decay=70,n_episodes=100, tmax=200):\n",
    "    \"\"\"Deep Deterministic Policy Gradients.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        Instead of updating target every (int) steps, using 'soft' updating of .1 to gradually merge the networks\n",
    "        Index: current index for replacing memories in the priority replay buffer\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    N = OUNoise(agent.nA,123)\n",
    "    for e in range(1,n_episodes):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(1,tmax):\n",
    "            action = agent.act(state)\n",
    "            next_state,reward,done,_ = env.step(action)\n",
    "            # store memory and learn\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            # Bookkeeping\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)),end=\"\")\n",
    "        if e % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) == 100.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)))\n",
    "#             torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "MIN_BUFFER_SIZE = 200\n",
    "BATCH_SIZE = 50\n",
    "ALPHA = 0.6 # 0.7 or 0.6\n",
    "START_BETA = 0.5 # from 0.5-1\n",
    "END_BETA = 1\n",
    "QLR = 0.001\n",
    "ALR = 0.0001\n",
    "EPSILON = 1\n",
    "MIN_EPSILON = 0.01\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "L2 = 0.01\n",
    "UPDATE_EVERY = 4\n",
    "CLIP_NORM = 10\n",
    "V_MAX = 100\n",
    "V_MIN = -100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "def main():\n",
    "    seed = 7\n",
    "    config = Config('d4pg')\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    env.seed(seed)\n",
    "    nA = env.action_space.shape[0]\n",
    "    nS = env.observation_space.shape[0]\n",
    "    print('Observation Space {}, Action Space {}'.format(nS,nA))\n",
    "    agent = D4PG(nS,nA,config)\n",
    "    seed_replay_buffer(agent,env,config.min_buffer_size)\n",
    "    scores = train(agent,env)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space 3, Action Space 1\n",
      "Priority Tree: Batch Size 256 Buffer size 10000 Number of intermediate Nodes 40\n",
      "finished replay warm up\n",
      "Episode 44\tAverage Score: -1170.34"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-858475b42018>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-197-2448237ef2d7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD4PG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mseed_replay_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_buffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-196-30bd43e79314>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, epsilon, noise_decay, n_episodes, tmax)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# store memory and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Bookkeeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-452add87961c>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs, actions, rewards, next_obs, dones)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTD_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-191-360c1c6bef60>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = main()\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
